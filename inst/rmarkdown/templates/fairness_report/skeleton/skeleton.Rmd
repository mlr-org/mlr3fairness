---
title: Fairness Report
author:
  - name: First Author
output: html_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library("mlr3")
library("mlr3viz")
library("mlr3fairness")
library("ggplot2")
library("kableExtra")
```
<!-- This reads in `objects` provided to report_fairness: resample_result, task, ...-->

```{r, child = 'read_data.Rmd'}
```

Jump to section:

- [Task details](#task-details)
- [Model details](#model-details)
- [Interpretability](#Interpretability)

## Task details

In this fairness report, we investigate the fairness of the following task:

```{r}
task
```

### The General Task Documentation for the task:

```{r}
# Create the report attributes
report_attr = list(
  "Audit Date: ",
  "Task Name: ",
  "Number of observations: ",
  "Number of features: ",
  "Target Name: ",
  "Feature Names: ",
  "The Protected Attribute: "
)

# Create the skeleton of summary table
summary_table <- data.frame(Value=rep(NA, length(report_attr)))
rownames(summary_table) <- report_attr

#Assign the value to the table
summary_table$Value = c(
  as.character(Sys.Date()),
  task$id,
  task$nrow,
  task$ncol,
  task$target_names,
  paste(task$feature_names, collapse = ", "),
  task$col_roles$pta
)

summary_table %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```
### Audit Results: Summary
TBD

### Exploratory Data Analysis: 
The number of missing values, type and the levels for each feature:

```{r}
df_summary = data.frame(task$col_info)[task$col_info$id != "..row_id",
                                       !(names(task$col_info) %in% c("id", "label"))]
data.frame("Count_of_Missing_Value" = task$missings()) %>%
  cbind(df_summary) %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

We first look at the label distribution:

```{r}
autoplot(task) + facet_wrap(task$col_roles$pta)
```

## Model details

### Fairness Metrics

```{r}
fair_metrics = msrs(c("fairness.acc","fairness.eod","fairness.fnr",
  "fairness.fpr","fairness.npv","fairness.ppv","fairness.tnr",
  "fairness.tpr"))
```

```{r}
knitr::kable(resample_result$aggregate(fair_metrics))
```

```{r}
fairness_accuracy_tradeoff(resample_result)
```

```{r, eval = (resample_result$learner$predict_type == "prob")}
fairness_prediction_density(resample_result)
```

## Interpretability

# References
