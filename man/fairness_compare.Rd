% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/visualize.R
\name{fairness_compare}
\alias{fairness_compare}
\title{Fairness Comparison}
\usage{
fairness_compare(object, ...)
}
\arguments{
\item{object}{(\link{PredictionClassif})\cr (\link{BenchmarkResult})\cr (\link{ResampleResult})\cr
The binary class prediction object that will be evaluated. Only one data task is allowed for BenchmarkResult or ResampleResult
\itemize{
\item If provided a (\link{PredictionClassif}). Then the visualization will compare the fairness metrics among the binary level from protected field through bar plots.
\item If provided a (\link{ResampleResult}). Then the visualization will generate the boxplots for fairness metrics, and compare them among the binary level from protected field.
\item If provided a (\link{BenchmarkResult}). Then the visualization will generate the boxplots for fairness metrics, and compare them among both the binary level from protected field and the models implemented.
}}

\item{...}{The arguments to be passed to methods, such as:
\itemize{
\item \code{fairness_measure} (\link{Measure})\cr
The fairness measures that will evaluated on object, could be single measure \link{msr} or measures \link{msrs}.  Default measure set to be \code{msr("fairness.acc")}
\item \code{task} (\link{TaskClassif})\cr
The data task that contains the protected column, only required when the class of object is (\link{PredictionClassif})
}}
}
\description{
These functions specialize in comparing the Fairness metrics between learners and levels in protected columns through visualizations.
From the visualization, users could see the fairness metrics difference between learners and levels in protected fields.
Those visualizations could help the users to detect the fairness problems and choose the optimum model.
It could take multiple type of inputs, like (\link{PredictionClassif}), (\link{BenchmarkResult}) and (\link{ResampleResult}).
}
\examples{
library(mlr3fairness)
library(mlr3learners)
library(mlr3)
library(ggplot2)
library(data.table)

# Setup the Fairness Measures and tasks
task = tsk("adult_train")
learner = lrn("classif.ranger", predict_type = "prob")
learner$train(task)
predictions = learner$predict(task)
design = benchmark_grid(
  tasks = tsk("adult_train"),
  learners = lrns(c("classif.ranger", "classif.rpart"),
                 predict_type = "prob", predict_sets = c("train", "test")),
  resamplings = rsmps("cv", folds = 3)
)

bmr = benchmark(design)
fairness_measure = msr("fairness.tpr")
fairness_measures = msrs(c("fairness.tpr", "fairness.fnr"))

fairness_compare(predictions, fairness_measure, task)
fairness_compare(predictions, fairness_measures, task)
fairness_compare(bmr, fairness_measure)
fairness_compare(bmr, fairness_measures)
}
