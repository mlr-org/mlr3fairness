<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">

<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1"/>
  <meta name="generator" content="distill" />

  <style type="text/css">
  /* Hide doc at startup (prevent jankiness while JS renders/transforms) */
  body {
    visibility: hidden;
  }
  </style>

 <!--radix_placeholder_import_source-->
 <!--/radix_placeholder_import_source-->

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  { color: #00769e; background-color: #f1f3f5; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span { color: #00769e; } /* Normal */
code span.al { color: #ad0000; } /* Alert */
code span.an { color: #5e5e5e; } /* Annotation */
code span.at { color: #657422; } /* Attribute */
code span.bn { color: #ad0000; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #00769e; } /* ControlFlow */
code span.ch { color: #20794d; } /* Char */
code span.cn { color: #8f5902; } /* Constant */
code span.co { color: #5e5e5e; } /* Comment */
code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
code span.dt { color: #ad0000; } /* DataType */
code span.dv { color: #ad0000; } /* DecVal */
code span.er { color: #ad0000; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #ad0000; } /* Float */
code span.fu { color: #4758ab; } /* Function */
code span.im { } /* Import */
code span.in { color: #5e5e5e; } /* Information */
code span.kw { color: #00769e; } /* Keyword */
code span.op { color: #5e5e5e; } /* Operator */
code span.ot { color: #00769e; } /* Other */
code span.pp { color: #ad0000; } /* Preprocessor */
code span.sc { color: #5e5e5e; } /* SpecialChar */
code span.ss { color: #20794d; } /* SpecialString */
code span.st { color: #20794d; } /* String */
code span.va { color: #111111; } /* Variable */
code span.vs { color: #20794d; } /* VerbatimString */
code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
</style>


  <!--radix_placeholder_meta_tags-->
  <title>Fairness Audits and Debiasing Using mlr3fairness</title>

  <meta property="description" itemprop="description" content="Given an increase in data-driven automated decision-making based on machine learning models, it is imperative that along with tools to develop and improve such models there are sufficient capabilities to analyze and assess models with respect to potential biases. We present the package mlr3fairness, a collection of metrics and methods that allow for the assessment of bias in machine learning models. Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases along with a set of visualizations that can help to provide additional insights into such biases. mlr3fairness furthermore integrates bias mitigation methods that can help alleviate biases in Machine Learning (ML) models through data preprocessing or post-processing of predictions. These allow practitioners to trade off performance and fairness metric that are appropriate for their use case."/>

  <link rel="license" href="https://creativecommons.org/licenses/by/4.0/"/>

  <!--  https://schema.org/Article -->
  <meta property="article:published" itemprop="datePublished" content="2022-11-09"/>
  <meta property="article:created" itemprop="dateCreated" content="2022-11-09"/>
  <meta name="article:author" content="Florian Pfisterer"/>
  <meta name="article:author" content="Siyi Wei"/>
  <meta name="article:author" content="Sebastian Vollmer"/>
  <meta name="article:author" content="Michel Lang"/>
  <meta name="article:author" content="Bernd Bischl"/>

  <!--  https://developers.facebook.com/docs/sharing/webmasters#markup -->
  <meta property="og:title" content="Fairness Audits and Debiasing Using mlr3fairness"/>
  <meta property="og:type" content="article"/>
  <meta property="og:description" content="Given an increase in data-driven automated decision-making based on machine learning models, it is imperative that along with tools to develop and improve such models there are sufficient capabilities to analyze and assess models with respect to potential biases. We present the package mlr3fairness, a collection of metrics and methods that allow for the assessment of bias in machine learning models. Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases along with a set of visualizations that can help to provide additional insights into such biases. mlr3fairness furthermore integrates bias mitigation methods that can help alleviate biases in Machine Learning (ML) models through data preprocessing or post-processing of predictions. These allow practitioners to trade off performance and fairness metric that are appropriate for their use case."/>
  <meta property="og:locale" content="en_US"/>

  <!--  https://dev.twitter.com/cards/types/summary -->
  <meta property="twitter:card" content="summary"/>
  <meta property="twitter:title" content="Fairness Audits and Debiasing Using mlr3fairness"/>
  <meta property="twitter:description" content="Given an increase in data-driven automated decision-making based on machine learning models, it is imperative that along with tools to develop and improve such models there are sufficient capabilities to analyze and assess models with respect to potential biases. We present the package mlr3fairness, a collection of metrics and methods that allow for the assessment of bias in machine learning models. Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases along with a set of visualizations that can help to provide additional insights into such biases. mlr3fairness furthermore integrates bias mitigation methods that can help alleviate biases in Machine Learning (ML) models through data preprocessing or post-processing of predictions. These allow practitioners to trade off performance and fairness metric that are appropriate for their use case."/>

  <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->
  <meta name="citation_title" content="Fairness Audits and Debiasing Using mlr3fairness"/>
  <meta name="citation_pdf_url" content="mlr3fairness.pdf"/>
  <meta name="citation_journal_title" content="The R Journal"/>
  <meta name="citation_issn" content="2073-4859"/>
  <meta name="citation_firstpage" content="1"/>
  <meta name="citation_fulltext_world_readable" content=""/>
  <meta name="citation_online_date" content="2022/11/09"/>
  <meta name="citation_publication_date" content="2022/11/09"/>
  <meta name="citation_author" content="Florian Pfisterer"/>
  <meta name="citation_author_institution" content="Ludwig-Maximilians-Universität München"/>
  <meta name="citation_author" content="Siyi Wei"/>
  <meta name="citation_author_institution" content="University of Toronto"/>
  <meta name="citation_author" content="Sebastian Vollmer"/>
  <meta name="citation_author_institution" content="Deutsches Forschungszentrum für Künstliche Intelligenz"/>
  <meta name="citation_author" content="Michel Lang"/>
  <meta name="citation_author_institution" content="Research Center Trustworthy Data Science and Security"/>
  <meta name="citation_author" content="Bernd Bischl"/>
  <meta name="citation_author_institution" content="Ludwig-Maximilians-Universität München"/>
  <!--/radix_placeholder_meta_tags-->
  
  <meta name="citation_reference" content="citation_title=Iml: An r package for interpretable machine learning;citation_publisher=Journal of Open Source Software;citation_volume=3;citation_doi=10.21105/joss.00786;citation_author=Christoph Molnar;citation_author=Bernd Bischl;citation_author=Giuseppe Casalicchio"/>
  <meta name="citation_reference" content="citation_title=Equality of opportunity in supervised learning;citation_volume=29;citation_author=Moritz Hardt;citation_author=Eric Price;citation_author=Nati Srebro"/>
  <meta name="citation_reference" content="citation_title=Data preprocessing techniques for classification without discrimination;citation_publisher=Springer;citation_volume=33;citation_author=Faisal Kamiran;citation_author=Toon Calders"/>
  <meta name="citation_reference" content="citation_title=Model cards for model reporting;citation_publisher=PMLR; Association for Computing Machinery;citation_doi=10.1145/3287560.3287596;citation_author=Margaret Mitchell;citation_author=Simone Wu;citation_author=Andrew Zaldivar;citation_author=Parker Barnes;citation_author=Lucy Vasserman;citation_author=Ben Hutchinson;citation_author=Elena Spitzer;citation_author=Inioluwa Deborah Raji;citation_author=Timnit Gebru"/>
  <meta name="citation_reference" content="citation_title=Datasheets for datasets;citation_publisher=ACM New York, NY, USA;citation_volume=64;citation_author=Timnit Gebru;citation_author=Jamie Morgenstern;citation_author=Briana Vecchione;citation_author=Jennifer Wortman Vaughan;citation_author=Hanna Wallach;citation_author=Hal Daumé Iii;citation_author=Kate Crawford"/>
  <meta name="citation_reference" content="citation_title=&lt;span class=&quot;nocase&quot;&gt;Fairness beyond Disparate treatment &amp; Disparate Impact&lt;/span&gt;;citation_publisher=International World Wide Web Conferences Steering Committee;citation_doi=10.1145/3038912.3052660;citation_author=Muhammad Bilal Zafar;citation_author=Isabel Valera;citation_author=Manuel Gomez Rodriguez;citation_author=Krishna P. Gummadi"/>
  <meta name="citation_reference" content="citation_title=Bias preservation in machine learning: The legality of fairness metrics under EU non-discrimination law;citation_publisher=West Virginia University College of Law;citation_volume=123;citation_author=S. Wachter;citation_author=B. Mittelstadt;citation_author=C. Russell"/>
  <meta name="citation_reference" content="citation_title=Resampling methods for meta-model validation with recommendations for evolutionary computation;citation_publisher=MIT Press;citation_volume=20;citation_author=Bernd Bischl;citation_author=Olaf Mersmann;citation_author=Heike Trautmann;citation_author=Claus Weihs"/>
  <meta name="citation_reference" content="citation_title=Fairness through awareness;citation_author=Cynthia Dwork;citation_author=Moritz Hardt;citation_author=Toniann Pitassi;citation_author=Omer Reingold;citation_author=Richard Zemel"/>
  <meta name="citation_reference" content="citation_title=Nonconvex optimization for regression with fairness constraints;citation_author=Junpei Komiyama;citation_author=Akiko Takeda;citation_author=Junya Honda;citation_author=Hajime Shimao"/>
  <meta name="citation_reference" content="citation_title=Debiasing classifiers: Is reality at variance with expectation?;citation_author=Ashrya Agrawal;citation_author=Florian Pfisterer;citation_author=Bernd Bischl;citation_author=Jiahao Chen;citation_author=Srijan Sood;citation_author=Sameena Shah;citation_author=Francois Buet-Golfouse;citation_author=Bilal A Mateen;citation_author=Sebastian Vollmer"/>
  <meta name="citation_reference" content="citation_title=Achieving fairness with a simple ridge penalty;citation_author=Marco Scutari;citation_author=Francesca Panero;citation_author=Manuel Proissl"/>
  <meta name="citation_reference" content="citation_title=Aequitas: A bias and fairness audit toolkit;citation_author=Pedro Saleiro;citation_author=Benedict Kuester;citation_author=Loren Hinkson;citation_author=Jesse London;citation_author=Abby Stevens;citation_author=Ari Anisfeld;citation_author=Kit T Rodolfa;citation_author=Rayid Ghani"/>
  <meta name="citation_reference" content="citation_title=UCI machine learning repository;citation_publisher=University of California, Irvine, School of Information; Computer Sciences;citation_author=Dheeru Dua;citation_author=Casey Graff"/>
  <meta name="citation_reference" content="citation_title=R markdown cookbook;citation_publisher=Chapman; Hall/CRC;citation_author=Yihui Xie;citation_author=Christophe Dervieux;citation_author=Emily Riederer"/>
  <meta name="citation_reference" content="citation_title=Flexible and robust machine learning using mlr3 in r"/>
  <meta name="citation_reference" content="citation_title=Mind the gap: Measuring generalization performance across multiple objectives;citation_publisher=Springer Nature Switzerland;citation_author=Matthias Feurer;citation_author=Katharina Eggensperger;citation_author=Edward Bergman;citation_author=Florian Pfisterer;citation_author=Bernd Bischl;citation_author=Frank Hutter"/>
  <meta name="citation_reference" content="citation_title=A unifying framework for parallel and distributed processing in r using futures;citation_volume=13;citation_author=Henrik Bengtsson"/>
  <!--radix_placeholder_rmarkdown_metadata-->

  <script type="text/json" id="radix-rmarkdown-metadata">
  {"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","description","date","draft","author","preamble","bibliography","output","journal","slug","pdf_url","creative_commons","packages","CTV","csl"]}},"value":[{"type":"character","attributes":{},"value":["Fairness Audits and Debiasing Using mlr3fairness"]},{"type":"character","attributes":{},"value":["Given an increase in data-driven automated decision-making based on machine learning models, it is imperative that along with tools to develop and improve such models there are sufficient capabilities to analyze and assess models with respect to potential biases. We present the package mlr3fairness, a collection of metrics and methods that allow for the assessment of bias in machine learning models. Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases along with a set of visualizations that can help to provide additional insights into such biases. mlr3fairness furthermore integrates bias mitigation methods that can help alleviate biases in Machine Learning (ML) models through data preprocessing or post-processing of predictions. These allow practitioners to trade off performance and fairness metric that are appropriate for their use case.\n"]},{"type":"character","attributes":{},"value":["2022-11-09"]},{"type":"logical","attributes":{},"value":[true]},{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","orcid_id","email","affiliation","address","affiliation2","address2"]}},"value":[{"type":"character","attributes":{},"value":["Florian Pfisterer"]},{"type":"character","attributes":{},"value":["0000-0001-8867-762X"]},{"type":"character","attributes":{},"value":["florian.pfisterer@stat.uni-muenchen.de"]},{"type":"character","attributes":{},"value":["Ludwig-Maximilians-Universität München"]},{"type":"character","attributes":{},"value":["Munich, Germany"]},{"type":"character","attributes":{},"value":["Munich Center for Machine Learning"]},{"type":"character","attributes":{},"value":["Munich, Germany"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","email","affiliation","address"]}},"value":[{"type":"character","attributes":{},"value":["Siyi Wei"]},{"type":"character","attributes":{},"value":["weisiyi2@gmail.com"]},{"type":"character","attributes":{},"value":["University of Toronto"]},{"type":"character","attributes":{},"value":["Toronto, Canada"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","email","affiliation","address","affiliation2","address2"]}},"value":[{"type":"character","attributes":{},"value":["Sebastian Vollmer"]},{"type":"character","attributes":{},"value":["svollmer@stat.uni-muenchen.de"]},{"type":"character","attributes":{},"value":["Deutsches Forschungszentrum für Künstliche Intelligenz"]},{"type":"character","attributes":{},"value":["Kaiserslautern, Germany"]},{"type":"character","attributes":{},"value":["University of Kaiserslautern"]},{"type":"character","attributes":{},"value":["Kaiserslautern, Germany"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","email","orcid_id","affiliation","address","affiliation2","address2"]}},"value":[{"type":"character","attributes":{},"value":["Michel Lang"]},{"type":"character","attributes":{},"value":["michel.lang@stat.uni-muenchen.de"]},{"type":"character","attributes":{},"value":["0000-0001-9754-0393"]},{"type":"character","attributes":{},"value":["Research Center Trustworthy Data Science and Security"]},{"type":"character","attributes":{},"value":["Dortmund, Germany"]},{"type":"character","attributes":{},"value":["TU Dortmund University"]},{"type":"character","attributes":{},"value":["Dortmund, Germany"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","email","orcid_id","affiliation","address","affiliation2","address2"]}},"value":[{"type":"character","attributes":{},"value":["Bernd Bischl"]},{"type":"character","attributes":{},"value":["bernd.bischl@stat.uni-muenchen.de"]},{"type":"character","attributes":{},"value":["0000-0001-6002-6980"]},{"type":"character","attributes":{},"value":["Ludwig-Maximilians-Universität München"]},{"type":"character","attributes":{},"value":["Munich, Germany"]},{"type":"character","attributes":{},"value":["Munich Center for Machine Learning"]},{"type":"character","attributes":{},"value":["Munich, Germany"]}]}]},{"type":"character","attributes":{},"value":["\\usepackage{longtable}\n\\usepackage{bbm}\n"]},{"type":"character","attributes":{},"value":["mlr3fairness.bib"]},{"type":"character","attributes":{},"value":["rjtools::rjournal_web_article"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["title","issn","firstpage","lastpage"]}},"value":[{"type":"character","attributes":{},"value":["The R Journal"]},{"type":"character","attributes":{},"value":["2073-4859"]},{"type":"double","attributes":{},"value":[1]},{"type":"NULL"}]},{"type":"character","attributes":{},"value":["mlr3fairness"]},{"type":"character","attributes":{},"value":["mlr3fairness.pdf"]},{"type":"character","attributes":{},"value":["CC BY"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["cran","bioc"]}},"value":[{"type":"character","attributes":{},"value":["mlr3fairness","caret","tidymodels","SuperLearner","mlr","mlr3","mlr3tuning","fairness","aif360","fairmodels","DALEX","mlr3pipelines","mcboost","mlr3benchmark","mlr3oml","mlr3viz","ggplot2","rpart","glmnet","iml","future"]},{"type":"list","attributes":{},"value":[]}]},{"type":"character","attributes":{},"value":["Bayesian","Environmetrics","HighPerformanceComputing","MachineLearning","Phylogenetics","Spatial","Survival","TeachingStatistics"]},{"type":"character","attributes":{},"value":["/home/flo/R/x86_64-pc-linux-gnu-library/4.2/rjtools/rjournal.csl"]}]}
  </script>
  <!--/radix_placeholder_rmarkdown_metadata-->
  
  <script type="text/json" id="radix-resource-manifest">
  {"type":"character","attributes":{},"value":["author-guide.pdf","initial_checks.log","mlr3fairness_files/anchor-4.2.2/anchor.min.js","mlr3fairness_files/bowser-1.9.3/bowser.min.js","mlr3fairness_files/distill-2.2.21/template.v2.js","mlr3fairness_files/figure-html5/bmrbox-1.png","mlr3fairness_files/figure-html5/fat-1.png","mlr3fairness_files/figure-html5/predplots-1.png","mlr3fairness_files/figure-html5/unnamed-chunk-19-1.png","mlr3fairness_files/figure-html5/unnamed-chunk-7-1.png","mlr3fairness_files/header-attrs-2.18/header-attrs.js","mlr3fairness_files/jquery-3.6.0/jquery-3.6.0.js","mlr3fairness_files/jquery-3.6.0/jquery-3.6.0.min.js","mlr3fairness_files/jquery-3.6.0/jquery-3.6.0.min.map","mlr3fairness_files/popper-2.6.0/popper.min.js","mlr3fairness_files/tippy-6.2.7/tippy-bundle.umd.min.js","mlr3fairness_files/tippy-6.2.7/tippy-light-border.css","mlr3fairness_files/tippy-6.2.7/tippy.css","mlr3fairness_files/tippy-6.2.7/tippy.umd.min.js","mlr3fairness_files/webcomponents-2.0.0/webcomponents.js","mlr3fairness.bib","mlr3fairness.pdf","mlr3fairness.tex","pdfs/datasheet_template.pdf","pdfs/fairness_report.pdf","pdfs/modelcard_template.pdf","presentations/dagstat_2022.html","presentations/dagstat_2022.Rmd","presentations/images/center.png","presentations/images/center.Rmd","presentations/images/center.svg","presentations/images/qr.png","presentations/references.bib","presentations/style.css","RJournal.sty","RJwrapper.bbl","RJwrapper.log","RJwrapper.tex","Rlogo.pdf"]}
  </script>
  <!--radix_placeholder_navigation_in_header-->
  <!--/radix_placeholder_navigation_in_header-->
  <!--radix_placeholder_distill-->

  <style type="text/css">

  body {
    background-color: white;
  }

  .pandoc-table {
    width: 100%;
  }

  .pandoc-table>caption {
    margin-bottom: 10px;
  }

  .pandoc-table th:not([align]) {
    text-align: left;
  }

  .pagedtable-footer {
    font-size: 15px;
  }

  d-byline .byline {
    grid-template-columns: 2fr 2fr;
  }

  d-byline .byline h3 {
    margin-block-start: 1.5em;
  }

  d-byline .byline .authors-affiliations h3 {
    margin-block-start: 0.5em;
  }

  .authors-affiliations .orcid-id {
    width: 16px;
    height:16px;
    margin-left: 4px;
    margin-right: 4px;
    vertical-align: middle;
    padding-bottom: 2px;
  }

  d-title .dt-tags {
    margin-top: 1em;
    grid-column: text;
  }

  .dt-tags .dt-tag {
    text-decoration: none;
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0em 0.4em;
    margin-right: 0.5em;
    margin-bottom: 0.4em;
    font-size: 70%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  d-article table.gt_table td,
  d-article table.gt_table th {
    border-bottom: none;
    font-size: 100%;
  }

  .html-widget {
    margin-bottom: 2.0em;
  }

  .l-screen-inset {
    padding-right: 16px;
  }

  .l-screen .caption {
    margin-left: 10px;
  }

  .shaded {
    background: rgb(247, 247, 247);
    padding-top: 20px;
    padding-bottom: 20px;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .html-widget {
    margin-bottom: 0;
    border: 1px solid rgba(0, 0, 0, 0.1);
  }

  .shaded .shaded-content {
    background: white;
  }

  .text-output {
    margin-top: 0;
    line-height: 1.5em;
  }

  .hidden {
    display: none !important;
  }

  d-article {
    padding-top: 2.5rem;
    padding-bottom: 30px;
  }

  d-appendix {
    padding-top: 30px;
  }

  d-article>p>img {
    width: 100%;
  }

  d-article h2 {
    margin: 1rem 0 1.5rem 0;
  }

  d-article h3 {
    margin-top: 1.5rem;
  }

  d-article iframe {
    border: 1px solid rgba(0, 0, 0, 0.1);
    margin-bottom: 2.0em;
    width: 100%;
  }

  /* Tweak code blocks */

  d-article div.sourceCode code,
  d-article pre code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
  }

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: auto;
  }

  d-article div.sourceCode {
    background-color: white;
  }

  d-article div.sourceCode pre {
    padding-left: 10px;
    font-size: 12px;
    border-left: 2px solid rgba(0,0,0,0.1);
  }

  d-article pre {
    font-size: 12px;
    color: black;
    background: none;
    margin-top: 0;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  d-article pre a {
    border-bottom: none;
  }

  d-article pre a:hover {
    border-bottom: none;
    text-decoration: underline;
  }

  d-article details {
    grid-column: text;
    margin-bottom: 0.8em;
  }

  @media(min-width: 768px) {

  d-article pre,
  d-article div.sourceCode,
  d-article div.sourceCode pre {
    overflow: visible !important;
  }

  d-article div.sourceCode pre {
    padding-left: 18px;
    font-size: 14px;
  }

  d-article pre {
    font-size: 14px;
  }

  }

  figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  /* CSS for d-contents */

  .d-contents {
    grid-column: text;
    color: rgba(0,0,0,0.8);
    font-size: 0.9em;
    padding-bottom: 1em;
    margin-bottom: 1em;
    padding-bottom: 0.5em;
    margin-bottom: 1em;
    padding-left: 0.25em;
    justify-self: start;
  }

  @media(min-width: 1000px) {
    .d-contents.d-contents-float {
      height: 0;
      grid-column-start: 1;
      grid-column-end: 4;
      justify-self: center;
      padding-right: 3em;
      padding-left: 2em;
    }
  }

  .d-contents nav h3 {
    font-size: 18px;
    margin-top: 0;
    margin-bottom: 1em;
  }

  .d-contents li {
    list-style-type: none
  }

  .d-contents nav > ul {
    padding-left: 0;
  }

  .d-contents ul {
    padding-left: 1em
  }

  .d-contents nav ul li {
    margin-top: 0.6em;
    margin-bottom: 0.2em;
  }

  .d-contents nav a {
    font-size: 13px;
    border-bottom: none;
    text-decoration: none
    color: rgba(0, 0, 0, 0.8);
  }

  .d-contents nav a:hover {
    text-decoration: underline solid rgba(0, 0, 0, 0.6)
  }

  .d-contents nav > ul > li > a {
    font-weight: 600;
  }

  .d-contents nav > ul > li > ul {
    font-weight: inherit;
  }

  .d-contents nav > ul > li > ul > li {
    margin-top: 0.2em;
  }


  .d-contents nav ul {
    margin-top: 0;
    margin-bottom: 0.25em;
  }

  .d-article-with-toc h2:nth-child(2) {
    margin-top: 0;
  }


  /* Figure */

  .figure {
    position: relative;
    margin-bottom: 2.5em;
    margin-top: 1.5em;
  }

  .figure .caption {
    color: rgba(0, 0, 0, 0.6);
    font-size: 12px;
    line-height: 1.5em;
  }

  .figure img.external {
    background: white;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 1px 8px rgba(0, 0, 0, 0.1);
    padding: 18px;
    box-sizing: border-box;
  }

  .figure .caption a {
    color: rgba(0, 0, 0, 0.6);
  }

  .figure .caption b,
  .figure .caption strong, {
    font-weight: 600;
    color: rgba(0, 0, 0, 1.0);
  }

  /* Citations */

  d-article .citation {
    color: inherit;
    cursor: inherit;
  }

  div.hanging-indent{
    margin-left: 1em; text-indent: -1em;
  }

  /* Citation hover box */

  .tippy-box[data-theme~=light-border] {
    background-color: rgba(250, 250, 250, 0.95);
  }

  .tippy-content > p {
    margin-bottom: 0;
    padding: 2px;
  }


  /* Tweak 1000px media break to show more text */

  @media(min-width: 1000px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 80px [middle-start] 50px [text-start kicker-end] 65px 65px 65px 65px 65px 65px 65px 65px [text-end gutter-start] 65px [middle-end] 65px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 16px;
    }

    .grid {
      grid-column-gap: 16px;
    }

    d-article {
      font-size: 1.06rem;
      line-height: 1.7em;
    }
    figure .caption, .figure .caption, figure figcaption {
      font-size: 13px;
    }
  }

  @media(min-width: 1180px) {
    .base-grid,
    distill-header,
    d-title,
    d-abstract,
    d-article,
    d-appendix,
    distill-appendix,
    d-byline,
    d-footnote-list,
    d-citation-list,
    distill-footer {
      grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
      grid-column-gap: 32px;
    }

    .grid {
      grid-column-gap: 32px;
    }
  }


  /* Get the citation styles for the appendix (not auto-injected on render since
     we do our own rendering of the citation appendix) */

  d-appendix .citation-appendix,
  .d-appendix .citation-appendix {
    font-size: 11px;
    line-height: 15px;
    border-left: 1px solid rgba(0, 0, 0, 0.1);
    padding-left: 18px;
    border: 1px solid rgba(0,0,0,0.1);
    background: rgba(0, 0, 0, 0.02);
    padding: 10px 18px;
    border-radius: 3px;
    color: rgba(150, 150, 150, 1);
    overflow: hidden;
    margin-top: -12px;
    white-space: pre-wrap;
    word-wrap: break-word;
  }

  /* Include appendix styles here so they can be overridden */

  d-appendix {
    contain: layout style;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-top: 60px;
    margin-bottom: 0;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
    color: rgba(0,0,0,0.5);
    padding-top: 60px;
    padding-bottom: 48px;
  }

  d-appendix h3 {
    grid-column: page-start / text-start;
    font-size: 15px;
    font-weight: 500;
    margin-top: 1em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.65);
  }

  d-appendix h3 + * {
    margin-top: 1em;
  }

  d-appendix ol {
    padding: 0 0 0 15px;
  }

  @media (min-width: 768px) {
    d-appendix ol {
      padding: 0 0 0 30px;
      margin-left: -30px;
    }
  }

  d-appendix li {
    margin-bottom: 1em;
  }

  d-appendix a {
    color: rgba(0, 0, 0, 0.6);
  }

  d-appendix > * {
    grid-column: text;
  }

  d-appendix > d-footnote-list,
  d-appendix > d-citation-list,
  d-appendix > distill-appendix {
    grid-column: screen;
  }

  /* Include footnote styles here so they can be overridden */

  d-footnote-list {
    contain: layout style;
  }

  d-footnote-list > * {
    grid-column: text;
  }

  d-footnote-list a.footnote-backlink {
    color: rgba(0,0,0,0.3);
    padding-left: 0.5em;
  }



  /* Anchor.js */

  .anchorjs-link {
    /*transition: all .25s linear; */
    text-decoration: none;
    border-bottom: none;
  }
  *:hover > .anchorjs-link {
    margin-left: -1.125em !important;
    text-decoration: none;
    border-bottom: none;
  }

  /* Social footer */

  .social_footer {
    margin-top: 30px;
    margin-bottom: 0;
    color: rgba(0,0,0,0.67);
  }

  .disqus-comments {
    margin-right: 30px;
  }

  .disqus-comment-count {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
    cursor: pointer;
  }

  #disqus_thread {
    margin-top: 30px;
  }

  .article-sharing a {
    border-bottom: none;
    margin-right: 8px;
  }

  .article-sharing a:hover {
    border-bottom: none;
  }

  .sidebar-section.subscribe {
    font-size: 12px;
    line-height: 1.6em;
  }

  .subscribe p {
    margin-bottom: 0.5em;
  }


  .article-footer .subscribe {
    font-size: 15px;
    margin-top: 45px;
  }


  .sidebar-section.custom {
    font-size: 12px;
    line-height: 1.6em;
  }

  .custom p {
    margin-bottom: 0.5em;
  }

  /* Styles for listing layout (hide title) */
  .layout-listing d-title, .layout-listing .d-title {
    display: none;
  }

  /* Styles for posts lists (not auto-injected) */


  .posts-with-sidebar {
    padding-left: 45px;
    padding-right: 45px;
  }

  .posts-list .description h2,
  .posts-list .description p {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
  }

  .posts-list .description h2 {
    font-weight: 700;
    border-bottom: none;
    padding-bottom: 0;
  }

  .posts-list h2.post-tag {
    border-bottom: 1px solid rgba(0, 0, 0, 0.2);
    padding-bottom: 12px;
  }
  .posts-list {
    margin-top: 60px;
    margin-bottom: 24px;
  }

  .posts-list .post-preview {
    text-decoration: none;
    overflow: hidden;
    display: block;
    border-bottom: 1px solid rgba(0, 0, 0, 0.1);
    padding: 24px 0;
  }

  .post-preview-last {
    border-bottom: none !important;
  }

  .posts-list .posts-list-caption {
    grid-column: screen;
    font-weight: 400;
  }

  .posts-list .post-preview h2 {
    margin: 0 0 6px 0;
    line-height: 1.2em;
    font-style: normal;
    font-size: 24px;
  }

  .posts-list .post-preview p {
    margin: 0 0 12px 0;
    line-height: 1.4em;
    font-size: 16px;
  }

  .posts-list .post-preview .thumbnail {
    box-sizing: border-box;
    margin-bottom: 24px;
    position: relative;
    max-width: 500px;
  }
  .posts-list .post-preview img {
    width: 100%;
    display: block;
  }

  .posts-list .metadata {
    font-size: 12px;
    line-height: 1.4em;
    margin-bottom: 18px;
  }

  .posts-list .metadata > * {
    display: inline-block;
  }

  .posts-list .metadata .publishedDate {
    margin-right: 2em;
  }

  .posts-list .metadata .dt-authors {
    display: block;
    margin-top: 0.3em;
    margin-right: 2em;
  }

  .posts-list .dt-tags {
    display: block;
    line-height: 1em;
  }

  .posts-list .dt-tags .dt-tag {
    display: inline-block;
    color: rgba(0,0,0,0.6);
    padding: 0.3em 0.4em;
    margin-right: 0.2em;
    margin-bottom: 0.4em;
    font-size: 60%;
    border: 1px solid rgba(0,0,0,0.2);
    border-radius: 3px;
    text-transform: uppercase;
    font-weight: 500;
  }

  .posts-list img {
    opacity: 1;
  }

  .posts-list img[data-src] {
    opacity: 0;
  }

  .posts-more {
    clear: both;
  }


  .posts-sidebar {
    font-size: 16px;
  }

  .posts-sidebar h3 {
    font-size: 16px;
    margin-top: 0;
    margin-bottom: 0.5em;
    font-weight: 400;
    text-transform: uppercase;
  }

  .sidebar-section {
    margin-bottom: 30px;
  }

  .categories ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }

  .categories li {
    color: rgba(0, 0, 0, 0.8);
    margin-bottom: 0;
  }

  .categories li>a {
    border-bottom: none;
  }

  .categories li>a:hover {
    border-bottom: 1px solid rgba(0, 0, 0, 0.4);
  }

  .categories .active {
    font-weight: 600;
  }

  .categories .category-count {
    color: rgba(0, 0, 0, 0.4);
  }


  @media(min-width: 768px) {
    .posts-list .post-preview h2 {
      font-size: 26px;
    }
    .posts-list .post-preview .thumbnail {
      float: right;
      width: 30%;
      margin-bottom: 0;
    }
    .posts-list .post-preview .description {
      float: left;
      width: 45%;
    }
    .posts-list .post-preview .metadata {
      float: left;
      width: 20%;
      margin-top: 8px;
    }
    .posts-list .post-preview p {
      margin: 0 0 12px 0;
      line-height: 1.5em;
      font-size: 16px;
    }
    .posts-with-sidebar .posts-list {
      float: left;
      width: 75%;
    }
    .posts-with-sidebar .posts-sidebar {
      float: right;
      width: 20%;
      margin-top: 60px;
      padding-top: 24px;
      padding-bottom: 24px;
    }
  }


  /* Improve display for browsers without grid (IE/Edge <= 15) */

  .downlevel {
    line-height: 1.6em;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
    margin: 0;
  }

  .downlevel .d-title {
    padding-top: 6rem;
    padding-bottom: 1.5rem;
  }

  .downlevel .d-title h1 {
    font-size: 50px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0.5rem;
  }

  .downlevel .d-title p {
    font-weight: 300;
    font-size: 1.2rem;
    line-height: 1.55em;
    margin-top: 0;
  }

  .downlevel .d-byline {
    padding-top: 0.8em;
    padding-bottom: 0.8em;
    font-size: 0.8rem;
    line-height: 1.8em;
  }

  .downlevel .section-separator {
    border: none;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
  }

  .downlevel .d-article {
    font-size: 1.06rem;
    line-height: 1.7em;
    padding-top: 1rem;
    padding-bottom: 2rem;
  }


  .downlevel .d-appendix {
    padding-left: 0;
    padding-right: 0;
    max-width: none;
    font-size: 0.8em;
    line-height: 1.7em;
    margin-bottom: 0;
    color: rgba(0,0,0,0.5);
    padding-top: 40px;
    padding-bottom: 48px;
  }

  .downlevel .footnotes ol {
    padding-left: 13px;
  }

  .downlevel .base-grid,
  .downlevel .distill-header,
  .downlevel .d-title,
  .downlevel .d-abstract,
  .downlevel .d-article,
  .downlevel .d-appendix,
  .downlevel .distill-appendix,
  .downlevel .d-byline,
  .downlevel .d-footnote-list,
  .downlevel .d-citation-list,
  .downlevel .distill-footer,
  .downlevel .appendix-bottom,
  .downlevel .posts-container {
    padding-left: 40px;
    padding-right: 40px;
  }

  @media(min-width: 768px) {
    .downlevel .base-grid,
    .downlevel .distill-header,
    .downlevel .d-title,
    .downlevel .d-abstract,
    .downlevel .d-article,
    .downlevel .d-appendix,
    .downlevel .distill-appendix,
    .downlevel .d-byline,
    .downlevel .d-footnote-list,
    .downlevel .d-citation-list,
    .downlevel .distill-footer,
    .downlevel .appendix-bottom,
    .downlevel .posts-container {
    padding-left: 150px;
    padding-right: 150px;
    max-width: 900px;
  }
  }

  .downlevel pre code {
    display: block;
    border-left: 2px solid rgba(0, 0, 0, .1);
    padding: 0 0 0 20px;
    font-size: 14px;
  }

  .downlevel code, .downlevel pre {
    color: black;
    background: none;
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    text-align: left;
    white-space: pre;
    word-spacing: normal;
    word-break: normal;
    word-wrap: normal;
    line-height: 1.5;

    -moz-tab-size: 4;
    -o-tab-size: 4;
    tab-size: 4;

    -webkit-hyphens: none;
    -moz-hyphens: none;
    -ms-hyphens: none;
    hyphens: none;
  }

  .downlevel .posts-list .post-preview {
    color: inherit;
  }



  </style>

  <script type="application/javascript">

  function is_downlevel_browser() {
    if (bowser.isUnsupportedBrowser({ msie: "12", msedge: "16"},
                                   window.navigator.userAgent)) {
      return true;
    } else {
      return window.load_distill_framework === undefined;
    }
  }

  // show body when load is complete
  function on_load_complete() {

    // add anchors
    if (window.anchors) {
      window.anchors.options.placement = 'left';
      window.anchors.add('d-article > h2, d-article > h3, d-article > h4, d-article > h5');
    }


    // set body to visible
    document.body.style.visibility = 'visible';

    // force redraw for leaflet widgets
    if (window.HTMLWidgets) {
      var maps = window.HTMLWidgets.findAll(".leaflet");
      $.each(maps, function(i, el) {
        var map = this.getMap();
        map.invalidateSize();
        map.eachLayer(function(layer) {
          if (layer instanceof L.TileLayer)
            layer.redraw();
        });
      });
    }

    // trigger 'shown' so htmlwidgets resize
    $('d-article').trigger('shown');
  }

  function init_distill() {

    init_common();

    // create front matter
    var front_matter = $('<d-front-matter></d-front-matter>');
    $('#distill-front-matter').wrap(front_matter);

    // create d-title
    $('.d-title').changeElementType('d-title');

    // create d-byline
    var byline = $('<d-byline></d-byline>');
    $('.d-byline').replaceWith(byline);

    // create d-article
    var article = $('<d-article></d-article>');
    $('.d-article').wrap(article).children().unwrap();

    // move posts container into article
    $('.posts-container').appendTo($('d-article'));

    // create d-appendix
    $('.d-appendix').changeElementType('d-appendix');

    // flag indicating that we have appendix items
    var appendix = $('.appendix-bottom').children('h3').length > 0;

    // replace footnotes with <d-footnote>
    $('.footnote-ref').each(function(i, val) {
      appendix = true;
      var href = $(this).attr('href');
      var id = href.replace('#', '');
      var fn = $('#' + id);
      var fn_p = $('#' + id + '>p');
      fn_p.find('.footnote-back').remove();
      var text = fn_p.html();
      var dtfn = $('<d-footnote></d-footnote>');
      dtfn.html(text);
      $(this).replaceWith(dtfn);
    });
    // remove footnotes
    $('.footnotes').remove();

    // move refs into #references-listing
    $('#references-listing').replaceWith($('#refs'));

    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      var id = $(this).attr('id');
      $('.d-contents a[href="#' + id + '"]').parent().remove();
      appendix = true;
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('d-appendix'));
    });

    // show d-appendix if we have appendix content
    $("d-appendix").css('display', appendix ? 'grid' : 'none');

    // localize layout chunks to just output
    $('.layout-chunk').each(function(i, val) {

      // capture layout
      var layout = $(this).attr('data-layout');

      // apply layout to markdown level block elements
      var elements = $(this).children().not('details, div.sourceCode, pre, script');
      elements.each(function(i, el) {
        var layout_div = $('<div class="' + layout + '"></div>');
        if (layout_div.hasClass('shaded')) {
          var shaded_content = $('<div class="shaded-content"></div>');
          $(this).wrap(shaded_content);
          $(this).parent().wrap(layout_div);
        } else {
          $(this).wrap(layout_div);
        }
      });


      // unwrap the layout-chunk div
      $(this).children().unwrap();
    });

    // remove code block used to force  highlighting css
    $('.distill-force-highlighting-css').parent().remove();

    // remove empty line numbers inserted by pandoc when using a
    // custom syntax highlighting theme
    $('code.sourceCode a:empty').remove();

    // load distill framework
    load_distill_framework();

    // wait for window.distillRunlevel == 4 to do post processing
    function distill_post_process() {

      if (!window.distillRunlevel || window.distillRunlevel < 4)
        return;

      // hide author/affiliations entirely if we have no authors
      var front_matter = JSON.parse($("#distill-front-matter").html());
      var have_authors = front_matter.authors && front_matter.authors.length > 0;
      if (!have_authors)
        $('d-byline').addClass('hidden');

      // article with toc class
      $('.d-contents').parent().addClass('d-article-with-toc');

      // strip links that point to #
      $('.authors-affiliations').find('a[href="#"]').removeAttr('href');

      // add orcid ids
      $('.authors-affiliations').find('.author').each(function(i, el) {
        var orcid_id = front_matter.authors[i].orcidID;
        if (orcid_id) {
          var a = $('<a></a>');
          a.attr('href', 'https://orcid.org/' + orcid_id);
          var img = $('<img></img>');
          img.addClass('orcid-id');
          img.attr('alt', 'ORCID ID');
          img.attr('src','data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==');
          a.append(img);
          $(this).append(a);
        }
      });

      // hide elements of author/affiliations grid that have no value
      function hide_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'hidden');
      }

      // affiliations
      var have_affiliations = false;
      for (var i = 0; i<front_matter.authors.length; ++i) {
        var author = front_matter.authors[i];
        if (author.affiliation !== "&nbsp;") {
          have_affiliations = true;
          break;
        }
      }
      if (!have_affiliations)
        $('d-byline').find('h3:contains("Affiliations")').css('visibility', 'hidden');

      // published date
      if (!front_matter.publishedDate)
        hide_byline_column("Published");

      // document object identifier
      var doi = $('d-byline').find('h3:contains("DOI")');
      var doi_p = doi.next().empty();
      if (!front_matter.doi) {
        // if we have a citation and valid citationText then link to that
        if ($('#citation').length > 0 && front_matter.citationText) {
          doi.html('Citation');
          $('<a href="#citation"></a>')
            .text(front_matter.citationText)
            .appendTo(doi_p);
        } else {
          hide_byline_column("DOI");
        }
      } else {
        $('<a></a>')
           .attr('href', "https://doi.org/" + front_matter.doi)
           .html(front_matter.doi)
           .appendTo(doi_p);
      }

       // change plural form of authors/affiliations
      if (front_matter.authors.length === 1) {
        var grid = $('.authors-affiliations');
        grid.children('h3:contains("Authors")').text('Author');
        grid.children('h3:contains("Affiliations")').text('Affiliation');
      }

      // remove d-appendix and d-footnote-list local styles
      $('d-appendix > style:first-child').remove();
      $('d-footnote-list > style:first-child').remove();

      // move appendix-bottom entries to the bottom
      $('.appendix-bottom').appendTo('d-appendix').children().unwrap();
      $('.appendix-bottom').remove();

      // hoverable references
      $('span.citation[data-cites]').each(function() {
        const citeChild = $(this).children()[0]
        // Do not process if @xyz has been used without escaping and without bibliography activated
        // https://github.com/rstudio/distill/issues/466
        if (citeChild === undefined) return true

        if (citeChild.nodeName == "D-FOOTNOTE") {
          var fn = citeChild
          $(this).html(fn.shadowRoot.querySelector("sup"))
          $(this).id = fn.id
          fn.remove()
        }
        var refs = $(this).attr('data-cites').split(" ");
        var refHtml = refs.map(function(ref) {
          // Could use CSS.escape too here, we insure backward compatibility in navigator
          return "<p>" + $('div[id="ref-' + ref + '"]').html() + "</p>";
        }).join("\n");
        window.tippy(this, {
          allowHTML: true,
          content: refHtml,
          maxWidth: 500,
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        });
      });

      // fix footnotes in tables (#411)
      // replacing broken distill.pub feature
      $('table d-footnote').each(function() {
        // we replace internal showAtNode methode which is triggered when hovering a footnote
        this.hoverBox.showAtNode = function(node) {
          // ported from https://github.com/distillpub/template/pull/105/files
          calcOffset = function(elem) {
              let x = elem.offsetLeft;
              let y = elem.offsetTop;
              // Traverse upwards until an `absolute` element is found or `elem`
              // becomes null.
              while (elem = elem.offsetParent && elem.style.position != 'absolute') {
                  x += elem.offsetLeft;
                  y += elem.offsetTop;
              }

              return { left: x, top: y };
          }
          // https://developer.mozilla.org/en-US/docs/Web/API/HTMLElement/offsetTop
          const bbox = node.getBoundingClientRect();
          const offset = calcOffset(node);
          this.show([offset.left + bbox.width, offset.top + bbox.height]);
        }
      })

      // clear polling timer
      clearInterval(tid);

      // show body now that everything is ready
      on_load_complete();
    }

    var tid = setInterval(distill_post_process, 50);
    distill_post_process();

  }

  function init_downlevel() {

    init_common();

     // insert hr after d-title
    $('.d-title').after($('<hr class="section-separator"/>'));

    // check if we have authors
    var front_matter = JSON.parse($("#distill-front-matter").html());
    var have_authors = front_matter.authors && front_matter.authors.length > 0;

    // manage byline/border
    if (!have_authors)
      $('.d-byline').remove();
    $('.d-byline').after($('<hr class="section-separator"/>'));
    $('.d-byline a').remove();

    // remove toc
    $('.d-contents').remove();

    // move appendix elements
    $('h1.appendix, h2.appendix').each(function(i, val) {
      $(this).changeElementType('h3');
    });
    $('h3.appendix').each(function(i, val) {
      $(this).nextUntil($('h1, h2, h3')).addBack().appendTo($('.d-appendix'));
    });


    // inject headers into references and footnotes
    var refs_header = $('<h3></h3>');
    refs_header.text('References');
    $('#refs').prepend(refs_header);

    var footnotes_header = $('<h3></h3');
    footnotes_header.text('Footnotes');
    $('.footnotes').children('hr').first().replaceWith(footnotes_header);

    // move appendix-bottom entries to the bottom
    $('.appendix-bottom').appendTo('.d-appendix').children().unwrap();
    $('.appendix-bottom').remove();

    // remove appendix if it's empty
    if ($('.d-appendix').children().length === 0)
      $('.d-appendix').remove();

    // prepend separator above appendix
    $('.d-appendix').before($('<hr class="section-separator" style="clear: both"/>'));

    // trim code
    $('pre>code').each(function(i, val) {
      $(this).html($.trim($(this).html()));
    });

    // move posts-container right before article
    $('.posts-container').insertBefore($('.d-article'));

    $('body').addClass('downlevel');

    on_load_complete();
  }


  function init_common() {

    // jquery plugin to change element types
    (function($) {
      $.fn.changeElementType = function(newType) {
        var attrs = {};

        $.each(this[0].attributes, function(idx, attr) {
          attrs[attr.nodeName] = attr.nodeValue;
        });

        this.replaceWith(function() {
          return $("<" + newType + "/>", attrs).append($(this).contents());
        });
      };
    })(jQuery);

    // prevent underline for linked images
    $('a > img').parent().css({'border-bottom' : 'none'});

    // mark non-body figures created by knitr chunks as 100% width
    $('.layout-chunk').each(function(i, val) {
      var figures = $(this).find('img, .html-widget');
      // ignore leaflet img layers (#106)
      figures = figures.filter(':not(img[class*="leaflet"])')
      if ($(this).attr('data-layout') !== "l-body") {
        figures.css('width', '100%');
      } else {
        figures.css('max-width', '100%');
        figures.filter("[width]").each(function(i, val) {
          var fig = $(this);
          fig.css('width', fig.attr('width') + 'px');
        });

      }
    });

    // auto-append index.html to post-preview links in file: protocol
    // and in rstudio ide preview
    $('.post-preview').each(function(i, val) {
      if (window.location.protocol === "file:")
        $(this).attr('href', $(this).attr('href') + "index.html");
    });

    // get rid of index.html references in header
    if (window.location.protocol !== "file:") {
      $('.distill-site-header a[href]').each(function(i,val) {
        $(this).attr('href', $(this).attr('href').replace(/^index[.]html/, "./"));
      });
    }

    // add class to pandoc style tables
    $('tr.header').parent('thead').parent('table').addClass('pandoc-table');
    $('.kable-table').children('table').addClass('pandoc-table');

    // add figcaption style to table captions
    $('caption').parent('table').addClass("figcaption");

    // initialize posts list
    if (window.init_posts_list)
      window.init_posts_list();

    // implmement disqus comment link
    $('.disqus-comment-count').click(function() {
      window.headroom_prevent_pin = true;
      $('#disqus_thread').toggleClass('hidden');
      if (!$('#disqus_thread').hasClass('hidden')) {
        var offset = $(this).offset();
        $(window).resize();
        $('html, body').animate({
          scrollTop: offset.top - 35
        });
      }
    });
  }

  document.addEventListener('DOMContentLoaded', function() {
    if (is_downlevel_browser())
      init_downlevel();
    else
      window.addEventListener('WebComponentsReady', init_distill);
  });

  </script>

  <!--/radix_placeholder_distill-->
  <script src="mlr3fairness_files/header-attrs-2.18/header-attrs.js"></script>
  <script src="mlr3fairness_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="mlr3fairness_files/popper-2.6.0/popper.min.js"></script>
  <link href="mlr3fairness_files/tippy-6.2.7/tippy.css" rel="stylesheet" />
  <link href="mlr3fairness_files/tippy-6.2.7/tippy-light-border.css" rel="stylesheet" />
  <script src="mlr3fairness_files/tippy-6.2.7/tippy.umd.min.js"></script>
  <script src="mlr3fairness_files/anchor-4.2.2/anchor.min.js"></script>
  <script src="mlr3fairness_files/bowser-1.9.3/bowser.min.js"></script>
  <script src="mlr3fairness_files/webcomponents-2.0.0/webcomponents.js"></script>
  <script src="mlr3fairness_files/distill-2.2.21/template.v2.js"></script>
  <!--radix_placeholder_site_in_header-->
  <!--/radix_placeholder_site_in_header-->
  <script>
    $(function() {
      console.log("Starting...")

      // Always show Published - distill hides it if not set
      function show_byline_column(caption) {
        $('d-byline').find('h3:contains("' + caption + '")').parent().css('visibility', 'visible');
      }

      show_byline_column('Published')

      // tweak function
      var rmd_meta = JSON.parse($("#radix-rmarkdown-metadata").html());
      function get_meta(name, meta) {
        var ind = meta.attributes.names.value.findIndex((e) => e == name)
        var val = meta.value[ind]
        if (val.type != 'list') {
          return val.value.toString()
        }
        return val
      }

      // tweak description
      // Add clickable tags
      const slug = get_meta('slug', rmd_meta)
      const cite_url = get_meta('citation_url', rmd_meta)

      var title = $("d-title").text

      const buttons = $('<div class="dt-tags" style="grid-column: page;">')
      buttons.append('<a href="#citation" class="dt-tag"><i class="fas fa-quote-left"></i> Cite</a>')
      buttons.append('<a href="' + slug + '.pdf" class="dt-tag"><i class="fas fa-file-pdf"></i> PDF</a>')
      buttons.append('<a href="https://twitter.com/intent/tweet?text='+title+'&url=' + cite_url + '" class="dt-tag"><i class="fab fa-twitter"></i> Tweet</a>')

      const abstract = $('<d-abstract>')
      abstract.append('<b>Abstract:</b><br>')
      abstract.append($("d-title p:not(:empty)").first()) // Move description to d-abstract
      $("d-title p:empty").remove() // Remove empty paragraphs after title
      abstract.append(buttons)
      abstract.insertAfter($('d-title')) // Add abstract section after title

      // tweak by-line
      var byline = $("d-byline div.byline")
      ind = rmd_meta.attributes.names.value.findIndex((e) => e == "journal")
      const journal = get_meta('journal', rmd_meta)
      const volume = get_meta('volume', rmd_meta)
      const issue = get_meta('issue', rmd_meta)
      const jrtitle = get_meta('title', journal)
      const year = ((jrtitle == "R News") ? 2000 : 2008) + parseInt(volume)
      const firstpage = get_meta('firstpage', journal)
      const lastpage = get_meta('lastpage', journal)
      byline.append('<div class="rjournal grid">')
      $('div.rjournal').append('<h3>Volume</h3>')
      $('div.rjournal').append('<h3>Pages</h3>')
      $('div.rjournal').append('<a class="volume" href="../../issues/'+year+'-'+issue+'">'+volume+'/'+issue+'</a>')
      $('div.rjournal').append('<p class="pages">'+firstpage+' - '+lastpage+'</p>')

      const received_date = new Date(get_meta('date_received', rmd_meta))
      byline.find('h3:contains("Published")').parent().append('<h3>Received</h3><p>'+received_date.toLocaleDateString('en-US', {month: 'short'})+' '+received_date.getDate()+', '+received_date.getFullYear()+'</p>')

    })
  </script>

  <style>
      /*
    .nav-dropdown-content .nav-dropdown-header {
      text-transform: lowercase;
    }
    */

    d-byline .byline {
      grid-template-columns: 2fr 2fr 2fr 2fr;
    }

    d-byline .rjournal {
      grid-column-end: span 2;
      grid-template-columns: 1fr 1fr;
      margin-bottom: 0;
    }

    d-title h1, d-title p, d-title figure,
    d-abstract p, d-abstract b {
      grid-column: page;
    }

    d-title .dt-tags {
      grid-column: page;
    }

    .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    d-article h1 {
      line-height: 1.1em;
    }

    d-abstract p, d-article p {
      text-align: justify;
    }

    @media(min-width: 1000px) {
      .d-contents.d-contents-float {
        justify-self: end;
      }

      nav.toc {
        border-right: 1px solid rgba(0, 0, 0, 0.1);
        border-right-width: 1px;
        border-right-style: solid;
        border-right-color: rgba(0, 0, 0, 0.1);
      }
    }

    .posts-list .dt-tags .dt-tag {
      text-transform: lowercase;
    }

    @keyframes highlight-target {
      0% {
        background-color: #ffa;
      }
      66% {
        background-color: #ffa;
      }
      100% {
        background-color: none;
      }
    }

    d-article :target, d-appendix :target {
       animation: highlight-target 3s;
    }

    .header-section-number {
      margin-right: 0.5em;
    }
  </style>


</head>

<body>

<!--radix_placeholder_front_matter-->

<script id="distill-front-matter" type="text/json">
{"title":"Fairness Audits and Debiasing Using mlr3fairness","description":"Given an increase in data-driven automated decision-making based on machine learning models, it is imperative that along with tools to develop and improve such models there are sufficient capabilities to analyze and assess models with respect to potential biases. We present the package mlr3fairness, a collection of metrics and methods that allow for the assessment of bias in machine learning models. Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases along with a set of visualizations that can help to provide additional insights into such biases. mlr3fairness furthermore integrates bias mitigation methods that can help alleviate biases in Machine Learning (ML) models through data preprocessing or post-processing of predictions. These allow practitioners to trade off performance and fairness metric that are appropriate for their use case.","authors":[{"author":"Florian Pfisterer","authorURL":"#","affiliation":"Ludwig-Maximilians-Universität München","affiliationURL":"#","orcidID":"0000-0001-8867-762X"},{"author":"Siyi Wei","authorURL":"#","affiliation":"University of Toronto","affiliationURL":"#","orcidID":""},{"author":"Sebastian Vollmer","authorURL":"#","affiliation":"Deutsches Forschungszentrum für Künstliche Intelligenz","affiliationURL":"#","orcidID":""},{"author":"Michel Lang","authorURL":"#","affiliation":"Research Center Trustworthy Data Science and Security","affiliationURL":"#","orcidID":"0000-0001-9754-0393"},{"author":"Bernd Bischl","authorURL":"#","affiliation":"Ludwig-Maximilians-Universität München","affiliationURL":"#","orcidID":"0000-0001-6002-6980"}],"publishedDate":"2022-11-09T00:00:00.000+01:00","citationText":"Pfisterer, et al., 2022"}
</script>

<!--/radix_placeholder_front_matter-->
<!--radix_placeholder_navigation_before_body-->
<!--/radix_placeholder_navigation_before_body-->
<!--radix_placeholder_site_before_body-->
<!--/radix_placeholder_site_before_body-->

<div class="d-title">
<h1>Fairness Audits and Debiasing Using mlr3fairness</h1>
<!--radix_placeholder_categories-->
<!--/radix_placeholder_categories-->
<p><p>Given an increase in data-driven automated decision-making based on machine learning models, it is imperative that along with tools to develop and improve such models there are sufficient capabilities to analyze and assess models with respect to potential biases. We present the package mlr3fairness, a collection of metrics and methods that allow for the assessment of bias in machine learning models. Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases along with a set of visualizations that can help to provide additional insights into such biases. mlr3fairness furthermore integrates bias mitigation methods that can help alleviate biases in Machine Learning (ML) models through data preprocessing or post-processing of predictions. These allow practitioners to trade off performance and fairness metric that are appropriate for their use case.</p></p>
</div>

<div class="d-byline">
  Florian Pfisterer  (Ludwig-Maximilians-Universität München)
  
,   Siyi Wei  (University of Toronto)
  
,   Sebastian Vollmer  (Deutsches Forschungszentrum für Künstliche Intelligenz)
  
,   Michel Lang  (Research Center Trustworthy Data Science and Security)
  
,   Bernd Bischl  (Ludwig-Maximilians-Universität München)
  
<br/>2022-11-09
</div>

<div class="d-article">
<h1 data-number="1" id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>Humans are increasingly subject to data-driven automated decision-making.
Those automated procedures such as credit risk assessments are often applied using predictive models <span class="citation" data-cites="kozodoi2022fairness galindo2000credit">(Galindo and Tamayo <a href="#ref-galindo2000credit" role="doc-biblioref">2000</a>; Kozodoi et al. <a href="#ref-kozodoi2022fairness" role="doc-biblioref">2022</a>)</span> often profoundly affecting individual’s lives.
It is therefore important that, along with tools to develop and improve such models, we also develop sufficient capabilities to analyze and assess models not only with respect to their robustness and predictive performance but also with respect to potential biases.
This is highlighted, e.g., by the European General Data Protection Regulation (GDPR) which requires data to be processed fairly.
Popular R modelling frameworks such as <a href="https://cran.r-project.org/package=caret">caret</a> <span class="citation" data-cites="caret">(Kuhn <a href="#ref-caret" role="doc-biblioref">2021</a>)</span>, <a href="https://cran.r-project.org/package=tidymodels">tidymodels</a> <span class="citation" data-cites="tidymodels">(Kuhn and Wickham <a href="#ref-tidymodels" role="doc-biblioref">2020</a>)</span>, <a href="https://cran.r-project.org/package=SuperLearner">SuperLearner</a> <span class="citation" data-cites="superlearner">(Polley et al. <a href="#ref-superlearner" role="doc-biblioref">2021</a>)</span>, or <a href="https://cran.r-project.org/package=mlr">mlr</a> <span class="citation" data-cites="mlr">(Bischl et al. <a href="#ref-mlr" role="doc-biblioref">2016</a>)</span> implement a plethora of metrics to measure performance, but fairness metrics are widely missing.
This lack of availability can be detrimental to obtaining fair and unbiased models if the result is to forgo bias audits due to the considerable complexity of implementing such metrics.
Consequently, there exists a considerable necessity for R packages to (a) implement such metrics, and (b) to connect these metrics to existing ML frameworks.
If biases are detected and need to be mitigated, we might furthermore want to employ bias mitigation techniques that tightly integrate with the fitting and evaluation of the resulting models in order to obtain trade-offs between a model’s fairness and utility (e.g., predictive accuracy).</p>
<p>In this article, we present the <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a> package which builds upon the ML framework <a href="https://cran.r-project.org/package=mlr3">mlr3</a> <span class="citation" data-cites="mlr3">(Lang et al. <a href="#ref-mlr3" role="doc-biblioref">2019</a>)</span>.
Our extension contains fairness metrics, fairness visualizations, and model-agnostic pre- and postprocessing operators that aim to reduce biases in ML models.
Additionally, <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a> comes with reporting functionality that assists the user in documenting data and ML models, as well as to perform fairness audits.</p>
<p>In the remainder of the article, we first provide an introduction to fairness in ML to raise awareness for biases that can arise due to the use of ML models.
Next, we introduce the <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a> package, followed by an extensive case study, showcasing the capabilities of <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a>.
We conclude with a summary.</p>
<h1 data-number="2" id="fairness-in-machine-learning"><span class="header-section-number">2</span> Fairness in Machine Learning</h1>
<p>Studies have found that data-driven automated decision-making systems often improve over human expertise (<span class="citation" data-cites="dawes1989clinical">Dawes et al. (<a href="#ref-dawes1989clinical" role="doc-biblioref">1989</a>)</span>) and high-stakes decisions can therefore be enhanced using data-driven systems.
This often does not only improve predictions but can also make decisions more efficient through automation.
Such systems, often without human oversight, are now ubiquitous in everyday life <span class="citation" data-cites="o2016weapons eubanks2018automating noble2018algorithms">(O’neil <a href="#ref-o2016weapons" role="doc-biblioref">2016</a>; Eubanks <a href="#ref-eubanks2018automating" role="doc-biblioref">2018</a>; Noble <a href="#ref-noble2018algorithms" role="doc-biblioref">2018</a>)</span>.
To provide further examples, ML-driven systems are used for highly influential decisions such as loan accommodations <span class="citation" data-cites="Chen2018 Turner2019">(Turner and McBurnett <a href="#ref-Turner2019" role="doc-biblioref">2019</a>; Chen <a href="#ref-Chen2018" role="doc-biblioref">2018</a>)</span>, job applications <span class="citation" data-cites="schumann">(Schumann et al. <a href="#ref-schumann" role="doc-biblioref">2020</a>)</span>, healthcare <span class="citation" data-cites="Topol2019">(Topol <a href="#ref-Topol2019" role="doc-biblioref">2019</a>)</span>, and criminal sentencing <span class="citation" data-cites="compas corbettcompas richardcompas">(Corbett-Davies et al. <a href="#ref-corbettcompas" role="doc-biblioref">2017</a>; Berk et al. <a href="#ref-richardcompas" role="doc-biblioref">2018</a>; Angwin et al. <a href="#ref-compas" role="doc-biblioref">2016</a>)</span>.
With this proliferation, such decisions have become subject to scrutiny as a result of prominent inadequacies or failures, for example in the case of the COMPAS recidivism prediction system <span class="citation" data-cites="compas">(Angwin et al. <a href="#ref-compas" role="doc-biblioref">2016</a>)</span>.</p>
<p>Without proper auditing, those models can unintentionally result in negative consequences for individuals, often from underprivileged groups <span class="citation" data-cites="fairmlbook">(Barocas et al. <a href="#ref-fairmlbook" role="doc-biblioref">2019</a>)</span>.
Several sources of such biases are worth mentioning in this context:
Data often contains <strong>historical biases</strong> such as gender or racial stereotypes, that – if picked up by the model – will be replicated into the future.
Similarly, unprivileged populations are often not represented in data due to <strong>sampling biases</strong> leading to models that perform well in groups sufficiently represented in the data but worse on others <span class="citation" data-cites="gendershades">(Buolamwini and Gebru <a href="#ref-gendershades" role="doc-biblioref">2018</a>)</span> – this includes a higher rate of missing data.
Other biases include biases in how <em>labels</em> and <em>data</em> are measured <span class="citation" data-cites="bao2021s">(Bao et al. <a href="#ref-bao2021s" role="doc-biblioref">2021</a>)</span> as well as <strong>feedback</strong> loops where repeated decisions affect the population subject to such decisions.
For an in-depth discussion and further sources of biases, the interested reader is referred to available surveys of the field <span class="citation" data-cites="fairmlbook mehrabi mitchell2021algorithmic">(Barocas et al. <a href="#ref-fairmlbook" role="doc-biblioref">2019</a>; Mehrabi et al. <a href="#ref-mehrabi" role="doc-biblioref">2021</a>; Mitchell et al. <a href="#ref-mitchell2021algorithmic" role="doc-biblioref">2021</a>)</span>.</p>
<h2 class="unnumbered" data-number="" id="quantifying-fairness">Quantifying fairness</h2>
<p>We now turn to the question of how we can detect whether disparities exist in a model and if so, how they can be quantified.
What constitutes a fair model depends on a society’s ethical values and which normative position we take, resulting in different metrics that are applied to a problem at hand.
In this article, we focus on a subgroup of these, so-called <em>statistical group fairness</em> metrics.
First, the observations are grouped by a sensitive attribute <span class="math inline">\(A\)</span> (<span class="math inline">\(A = 0\)</span> vs. <span class="math inline">\(A = 1\)</span>), which, e.g., is an identifier for a person’s race or a person’s gender.
For the sake of simplicity, we consider a <em>binary classification</em> scenario and a <em>binary sensitive attribute</em>.
Each observation has an associated label <span class="math inline">\(Y, Y \in \{0, 1\}\)</span>, and we aim to predict, e.g., whether a defendant was caught re-offending.
A system then makes a prediction <span class="math inline">\(\hat{Y}, \hat{Y} \in \{0,1\}\)</span>, with the goal to predict whether an individual might re-offend.
We assume that <span class="math inline">\(Y = 1\)</span> is the favored outcome in the following exposition.
While we do not describe them in detail, the concepts discussed in the following often extend naturally to more complex scenarios including multi-class classification, regression or survival analysis and similarly to settings with multiple sensitive attributes.
We now provide and discuss several metrics grouped into metrics that require <em>Separation</em> and <em>Independence</em> <span class="citation" data-cites="fairmlbook">(Barocas et al. <a href="#ref-fairmlbook" role="doc-biblioref">2019</a>)</span> to provide further intuition regarding core concepts and possible applications.</p>
<h3 class="unnumbered" data-number="" id="separation">Separation</h3>
<p>One group of widely used fairness notions requires <strong>Separation</strong>: <span class="math inline">\(\hat{Y} \perp A | Y\)</span>.
In order for separation to hold, the prediction <span class="math inline">\(\hat{Y}\)</span> has to be independent of <span class="math inline">\(A\)</span> given the true label <span class="math inline">\(Y\)</span>.
This essentially requires that some notion of model error, e.g., accuracy or false positive rate, is equal across groups <span class="math inline">\(A\)</span>.
From this notion, we can derive several metrics that come with different implications.
It is important to note that those metrics can only meaningfully identify biases under the assumption that no disparities exist in the data or that they are legally justified.
For example, if societal biases lead to disparate measurements of an observed quantity (e.g. SAT scores) for individuals with the same underlying ability, <em>separation</em> based metrics might not identify existing biases.
For this reason, <span class="citation" data-cites="wachter-vlr2020">Wachter et al. (<a href="#ref-wachter-vlr2020" role="doc-biblioref">2020</a>)</span> refer to those metrics as <em>bias-preserving</em> metrics since underlying disparities are not addressed.
We now provide and discuss several metrics to provide further intuition regarding core concepts and possible applications.</p>
<h4 class="unnumbered" data-number="" id="equalized-odds">Equalized Odds</h4>
<p>A predictor <span class="math inline">\(\hat{Y}\)</span> satisfies <em>equalized odds</em> with respect to a sensitive attribute <span class="math inline">\(A\)</span> and observed outcome <span class="math inline">\(Y\)</span>, if <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(A\)</span> are conditionally independent given <span class="math inline">\(Y\)</span>:
<span class="math display" id="eq:eod">\[\begin{equation}
\mathbb{P}\left(\hat{Y} = 1 \mid A = 0, Y = y\right) = \mathbb{P}\left(\hat{Y} = 1 \mid A = 1, Y = y\right), \quad y \in \{0,1\}.
\tag{1}
\end{equation}\]</span>
In short, we require that the true positive rates (TPR) and false positive rates (FPR) across both groups <span class="math inline">\(A = 0\)</span> and <span class="math inline">\(A = 1\)</span> are equal.
This intuitively requires, e.g., in the case of university admission, independent of the sensitive attribute, equal chances for qualified individuals to be accepted and unqualified individuals to be rejected.
Similar measures have been proposed based on equalized false positive rates <span class="citation" data-cites="chouldechova2017fair">(Chouldechova <a href="#ref-chouldechova2017fair" role="doc-biblioref">2017</a>)</span> and false omission rates <span class="citation" data-cites="richardcompas">(Berk et al. <a href="#ref-richardcompas" role="doc-biblioref">2018</a>)</span>, depending on the scenario and societal context.</p>
<h4 class="unnumbered" data-number="" id="equality-of-opportunity">Equality of Opportunity</h4>
<p>A predictor <span class="math inline">\(\hat{Y}\)</span> satisfies <em>equality of opportunity</em> with respect to a sensitive attribute <span class="math inline">\(A\)</span> and observed outcome <span class="math inline">\(Y\)</span>, if <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(A\)</span> are conditionally independent for <span class="math inline">\(Y = 1\)</span>.
This is a relaxation of the aforementioned <em>equalized odds</em> essentially only requiring equal TPRs:
<span class="math display" id="eq:eop">\[\begin{equation}
\mathbb{P}\left(\hat{Y} = 1 \mid A = 0, Y = 1\right) = \mathbb{P}\left(\hat{Y} = 1 \mid A = 1, Y = 1\right).
\tag{2}
\end{equation}\]</span>
Intuitively, this only requires that, independent of the sensitive attribute, qualified individuals have the same chance of being accepted.</p>
<h4 class="unnumbered" data-number="" id="performance-parity">Performance Parity</h4>
<p>A more general formulation can be applied when we require parity of some performance metric across groups.
To provide an example, <span class="citation" data-cites="gendershades">Buolamwini and Gebru (<a href="#ref-gendershades" role="doc-biblioref">2018</a>)</span> compare accuracy across intersectional subgroups, essentially arguing that model performance should be equal across groups:</p>
<p><span class="math display" id="eq:accp">\[\begin{equation}
\mathbb{P}\left(\hat{Y} = Y \mid A = 0\right) = \mathbb{P}\left(\hat{Y} = Y \mid A = 1\right).
 \tag{3}
\end{equation}\]</span>
This intuitively requires that the model should work equally well for all groups, i.e., individuals are correctly accepted or denied at the same rate, independent of the predicted attribute.
This notion can be extended across supervised learning settings and performance metrics, leading to considerations of equal mean squared error, e.g., in a regression setting.</p>
<h3 class="unnumbered" data-number="" id="independence">Independence</h3>
<p>The second group of fairness metrics is given by so-called <em>bias-transforming</em> metrics <span class="citation" data-cites="wachter-vlr2020">(Wachter et al. <a href="#ref-wachter-vlr2020" role="doc-biblioref">2020</a>)</span>.
They require that decision rates, such as the positive rate, are equal across groups.
This notion can identify biases, e.g., arising from societal biases that manifest in different base rates across groups.
At the same time, employing such notions poses a considerable risk, as blindly optimizing for demographic parity might result in predictors that, e.g., jail innocent people from an advantaged group in order to achieve parity across both groups <span class="citation" data-cites="dwork2012 richardcompas">(Dwork et al. <a href="#ref-dwork2012" role="doc-biblioref">2012</a>; Berk et al. <a href="#ref-richardcompas" role="doc-biblioref">2018</a>)</span>.
A predictor <span class="math inline">\(\hat{Y}\)</span> satisfies <em>demographic parity</em> <span class="citation" data-cites="Calders2010">(Calders and Verwer <a href="#ref-Calders2010" role="doc-biblioref">2010</a>)</span> with respect to a sensitive attribute <span class="math inline">\(A\)</span> and observed outcome <span class="math inline">\(Y\)</span>, if <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(A\)</span> are conditionally independent:
<span class="math display" id="eq:dp">\[\begin{equation}
\mathbb{P}\left(\hat{Y} = 1 \mid A = 0\right) = \mathbb{P}\left(\hat{Y} = 1 \mid A = 1\right).
 \tag{4}
\end{equation}\]</span>
In contrast to the previous definitions, this requires that the chance of being accepted is equal across groups.</p>
<h3 class="unnumbered" data-number="" id="fairness-metrics">Fairness metrics</h3>
<p>In order to encode the requirements in equations <a href="#eq:eod">(1)</a> - <a href="#eq:dp">(4)</a> into a fairness metric, we encode differences between measured quantities in two groups.
For a performance metric <span class="math inline">\(M\)</span>, e.g., the true positive rate (TPR), we calculate the difference in the metric across the two groups:</p>
<p><span class="math display">\[
\Delta_{\mathrm{M}} = \mathrm{M}_{A=0} - \mathrm{M}_{A=1}.
\]</span>
When <span class="math inline">\(\Delta_{\mathrm{M}}\)</span> now significantly deviates from <span class="math inline">\(0\)</span>, this indicates a fairness violation with respect to the fairness notion described in <span class="math inline">\(M\)</span>.
To provide an example, with <span class="math inline">\(\mathbb{P}\left(\hat{Y} = 1 \mid A = \star, Y = 1\right)\)</span> denoted with <span class="math inline">\(\mathrm{TPR}_{A=\star}\)</span>, we calculate the difference in TPR between the two groups:
<span class="math display">\[
\Delta_{\mathrm{TPR}} = \mathrm{TPR}_{A=0} - \mathrm{TPR}_{A=1}.
\]</span>
When <span class="math inline">\(\Delta_{\mathrm{TPR}}\)</span> now significantly deviates from <span class="math inline">\(0\)</span>, the prediction <span class="math inline">\(\hat{Y}\)</span> violates the requirement for <em>equality of opportunity</em> formulated above.</p>
<p>It is important to note that in practice, we might not be able to perfectly satisfy a given metric, e.g., due to stochasticity in data and labels.
Instead, to provide a binary conclusion regarding fairness, a model could be considered fair if <span class="math inline">\(|\Delta_{\mathrm{TPR}}| &lt; \epsilon\)</span> for a given threshold <span class="math inline">\(\epsilon &gt; 0\)</span>, e.g., <span class="math inline">\(\epsilon = 0.05\)</span>.
This allows for small deviations from perfect fairness due to variance in the estimation of <span class="math inline">\(\mathrm{TPR}_{A=\star}\)</span> or additional sources of bias.
However, choosing appropriate thresholds is difficult, and widely used values for <span class="math inline">\(\epsilon\)</span> such as <span class="math inline">\(0.05\)</span> are arbitrary and do not translate to legal doctrines, such as, e.g., disparate impact <span class="citation" data-cites="chen22">(Watkins et al. <a href="#ref-chen22" role="doc-biblioref">2022</a>)</span>.
A more in-depth treatment of metrics is given in <span class="citation" data-cites="fairmlbook saleiro2018aequitas kim2020fact mehrabi wachter-vlr2020">(Saleiro et al. <a href="#ref-saleiro2018aequitas" role="doc-biblioref">2018</a>; Barocas et al. <a href="#ref-fairmlbook" role="doc-biblioref">2019</a>; Kim et al. <a href="#ref-kim2020fact" role="doc-biblioref">2020</a>; Wachter et al. <a href="#ref-wachter-vlr2020" role="doc-biblioref">2020</a>; Mehrabi et al. <a href="#ref-mehrabi" role="doc-biblioref">2021</a>)</span>.</p>
<h4 class="unnumbered" data-number="" id="selecting-fairness-metrics">Selecting fairness metrics</h4>
<p>While the aforementioned metrics are conceptually similar, they encode different beliefs of what constitutes <em>fair</em> in a given scenario.
<span class="citation" data-cites="wachter-vlr2020">Wachter et al. (<a href="#ref-wachter-vlr2020" role="doc-biblioref">2020</a>)</span> differentiate between <em>bias-preserving</em> and <em>bias-transforming</em> metrics:
Bias-preserving metrics such as equalized odds and equality of opportunity require that errors made by a model are equal across groups.
This can help to detect biases stemming, e.g., from imbalances in the sampling or under- and overfitting in ML models, but might be problematic in cases where labels are biased.
To provide an example, police enforcement and subsequent arrests of violent re-offenders might be different across ZIP code areas, a proxy for race.
This might lead to situations where observed labels <span class="math inline">\(Y\)</span> suffer from differential measurement bias strongly correlated with race <span class="citation" data-cites="bao2021s">(Bao et al. <a href="#ref-bao2021s" role="doc-biblioref">2021</a>)</span>.
Bias-preserving metrics do not take such disparities into account and might, therefore (wrongly) lead to the conclusion, that a given model is fair.</p>
<p>Bias-transforming methods, in contrast, do not depend on labels and might therefore not suffer from this problem.
They can help detect biases arising from different base rates across populations, arising, e.g., from aforementioned biases in the labelling or as a consequence of structural discrimination.
Deciding which metrics to use constitutes a value judgement and requires careful assessment of the societal context a decision-making system is deployed in.
A discussion of different metrics and their applicability can be found in the Aequitas Fairness Toolkit <span class="citation" data-cites="saleiro2018aequitas">(Saleiro et al. <a href="#ref-saleiro2018aequitas" role="doc-biblioref">2018</a>)</span> which also provides guidance towards selecting a metric via the <a href="http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/">Aequitas Fairness Tree</a>.
<span class="citation" data-cites="wachter-vlr2020">Wachter et al. (<a href="#ref-wachter-vlr2020" role="doc-biblioref">2020</a>)</span> recommend using <em>bias-transforming</em> metrics and providing a checklist that can guide the choice of fairness metric.
<span class="citation" data-cites="corbett2018measure">Corbett-Davies and Goel (<a href="#ref-corbett2018measure" role="doc-biblioref">2018</a>)</span>, on the other hand, point out several limitations of available metrics and argue for grounding decisions in real-world quantities in addition to abstract fairness metrics.
Similarly, <span class="citation" data-cites="friedler16">Friedler et al. (<a href="#ref-friedler16" role="doc-biblioref">2016</a>)</span> emphasize the need to differentiate between constructs we aim to measure (e.g., job-related knowledge) and the observed quantity that can be measured in practice (e.g., years in a job) when trying to automate decision, since disparities in how constructs translate to observed quantities might suffer from bias.
To provide an example, individuals with similar abilities might exhibit different measured quantities (grades) due to structural bias, e.g., worse access to after-school tutoring programs.</p>
<h4 class="unnumbered" data-number="" id="the-dangers-of-fairness-metrics">The dangers of fairness metrics</h4>
<p>We want to stress that overly trusting in metrics can be dangerous and that fairness metrics can and should not be used to <em>prove</em> or <em>guarantee</em> fairness.
Whether a selected fairness notion (and a corresponding numerical value) is actually fair depends on the societal context in which a decision is made and which action should be derived from a given prediction.
Therefore, selecting the correct fairness metric requires a thorough understanding of the societal context of a decision, as well as the possible implications of such decisions.
To provide an example, in some cases discrepancies in positive predictions might be justified or even desired, as they, e.g., allow for a more nuanced, gender-specific diagnosis <span class="citation" data-cites="cirillo2020sex">(Cirillo et al. <a href="#ref-cirillo2020sex" role="doc-biblioref">2020</a>)</span>.
Furthermore, fairness metrics might not detect biases in more fine-grained subgroups, e.g., at the intersection of multiple sensitive attributes.
It is also important to note that fairness metrics merely provide a reduction of the aforementioned fairness notions into mathematical objectives.
As such, they require a variety of abstraction steps that might invalidate the metric <span class="citation" data-cites="chen22">(Watkins et al. <a href="#ref-chen22" role="doc-biblioref">2022</a>)</span>, as they, e.g., require that the data is a large enough and representative sample of exactly the population that we aim to investigate.
Furthermore, practitioners need to look beyond the model, and also at the data used for training and the process of data and label acquisition.
If the data for example exhibit disparate measurement errors in the features or labels, valid fairness assessments can become impossible.
Similarly, feedback loops might arise from a prediction leading to changes in the data collected in the future.
Even an <em>initially fair</em> model might then lead to adverse effects in the long term <span class="citation" data-cites="schwobel-facct22a">(Schwöbel and Remmers <a href="#ref-schwobel-facct22a" role="doc-biblioref">2022</a>)</span>.</p>
<p>Note that the fairness definitions presented above serve a dual purpose <span class="citation" data-cites="wachter-vlr2020">(Wachter et al. <a href="#ref-wachter-vlr2020" role="doc-biblioref">2020</a>)</span>:
First, as a <em>diagnostic tool</em> to detect disparities.
This allows for assessing whether a model has inherited biases, e.g., from historical disparities reflected in the data.
The second purpose is as a basis for <em>model selection</em> and making fair decisions in practice.
In this setting, fairness notions are employed to audit ML models or to select which model should be used in practice.
In this setting, it is important to note that fairness metrics should not be used as the sole basis for making decisions about whether to employ a given ML model or to assess whether a given system is fair.
We therefore explicitly encourage using the presented metrics for exploratory purposes.</p>
<h4 class="unnumbered" data-number="" id="other-notions-of-fairness">Other notions of fairness</h4>
<p>In addition to <em>statistical group fairness notions</em> introduced above, several additional fairness notions exist.
The notion of <em>individual fairness</em> was proposed by <span class="citation" data-cites="dwork2012">Dwork et al. (<a href="#ref-dwork2012" role="doc-biblioref">2012</a>)</span>.
Its core idea comes from the principle of <em>treating similar cases similarly and different cases differently</em>.
In contrast to statistical group fairness notions, this notion allows assessing <em>fairness</em> at an individual level and
would therefore allow determining, whether an individual is treated fairly.
A more in-depth treatment of individual fairness notions is, e.g., given in <span class="citation" data-cites="binns2020apparent">Binns (<a href="#ref-binns2020apparent" role="doc-biblioref">2020</a>)</span>.
Similarly, a variety of <em>causal</em> fairness notions exist (c.f. <span class="citation" data-cites="kilbertus2017avoiding">Kilbertus et al. (<a href="#ref-kilbertus2017avoiding" role="doc-biblioref">2017</a>)</span>).
They argue that assessing fairness requires incorporating causal relationships in the data and propose a variety of causal fairness metrics based on a <em>directed acyclic graph</em> describing relationships in the data.</p>
<h4 class="unnumbered" data-number="" id="fairness-constraints">Fairness constraints</h4>
<p>Statistical group fairness notions suffer from two further problems in practice:
First, it might be hard to exactly satisfy the required fairness notions, e.g., due to a limited amount of data available for evaluation.
Secondly, only requiring fairness might lead to degenerate solutions <span class="citation" data-cites="corbett2018measure">(Corbett-Davies and Goel <a href="#ref-corbett2018measure" role="doc-biblioref">2018</a>)</span> or models that have low utility, e.g., in separating <em>good</em> and <em>bad</em> credit risk.
One approach to take this into account is to employ models which maximize utility but satisfy some maximum constraint on potential unfairness.
This can be achieved via constraints on the employed fairness measure, e.g. <span class="math inline">\(|\Delta_M| \leq \epsilon\)</span> requiring that the absolute difference in a metric <span class="math inline">\(M\)</span> between groups
is smaller than a chosen value <span class="math inline">\(\epsilon\)</span>.
In the following, we denote the fairness metric we want to minimize with <span class="math inline">\(\Delta_M\)</span> and a performance metric with <span class="math inline">\(\rho\)</span>.</p>
<p><span class="math display">\[
  \rho_{|\Delta_M| \leq \epsilon} = \left\{
\begin{array}{ll}
\rho         &amp; |\Delta_M| \leq \epsilon      \\
- |\Delta_M| &amp;  \textrm{else.}                \\
\end{array}
\right.
\]</span></p>
<p>Note, that this assumes that the fairness metric <span class="math inline">\(\rho\)</span> is strictly positive and should be maximized.
This approach is similar in spirit to the approach of <span class="citation" data-cites="perrone2021fair">(Perrone et al. <a href="#ref-perrone2021fair" role="doc-biblioref">2021</a>)</span> who optimize the constrained expected improvement <span class="math inline">\(cEI = \mathbb{P}(|\Delta_M| \leq \epsilon) \cdot \rho\)</span>.</p>
<p>However, it is not immediately clear, how the constraint <span class="math inline">\(\epsilon\)</span> should be chosen.
An alternative, therefore, is to employ <em>multi-objective optimization</em> to investigate available trade-offs between performance and accuracy metrics.
This can be done via <a href="https://cran.r-project.org/package=mlr3tuning">mlr3tuning</a> which contains functionality to tune models for multiple metrics, e.g., described in more detail in the <a href="https://mlr3book.mlr-org.com/optimization.html#mult-measures-tuning">mlr3book</a> <span class="citation" data-cites="mlr3book">(Bernd et al. <a href="#ref-mlr3book" role="doc-biblioref">2023</a>)</span>.
The result of multi-objective optimization then is the <em>Pareto-set</em>: A list of models which optimally trade off the specified objectives.</p>
<h2 class="unnumbered" data-number="" id="bias-mitigation">Bias mitigation</h2>
<p>If biases are detected in a model, we might now be interested in improving models in order to potentially mitigate such biases.
Bias in models might arise from a variety of sources, so a careful understanding of the data, data quality and distribution might lead to approaches that can help in decreasing biases, e.g. through the collection of better or additional data or a better balancing of sensitive groups.
Similarly, biases might arise from the model, e.g., through under- or overfitting and more careful tuning of model hyperparameters might help with improving fairness.
Especially if the goal is to satisfy <em>bias-transforming</em> metrics, a better solution might often be to address fairness problems in the real world instead of relying on algorithmic interventions to solve fairness. This might lead to more robust, long-term solutions instead of temporarily addressing issues via algorithmic interventions.
In addition, a variety of algorithmic bias mitigation techniques, that might help with obtaining fairer models have been proposed.
Their goal is to reduce measured gaps in fairness, either via data pre-processing, employing models that incorporate fairness, or by applying post-processing techniques to a model’s predictions.
Popular examples of such techniques include computing instance weights before training <span class="citation" data-cites="kamiran2012data">(Kamiran and Calders <a href="#ref-kamiran2012data" role="doc-biblioref">2012</a>)</span>, where each observation is weighted proportional to the inverse frequency of its label and sensitive attribute.
Other methods work by directly learning fair models that incorporate fairness constraints into the fitting procedure <span class="citation" data-cites="Zafar2017">(Zafar et al. <a href="#ref-Zafar2017" role="doc-biblioref">2017</a>)</span> or by adapting model predictions, e.g., <span class="citation" data-cites="hardt2016equality">(Hardt et al. <a href="#ref-hardt2016equality" role="doc-biblioref">2016</a>)</span> propose to randomly flip a small fraction of predictions in each group given by <span class="math inline">\(\hat{Y}\)</span> and <span class="math inline">\(A\)</span>, such that fairness metrics are satisfied in expectation.
Since bias mitigation techniques are often tailored towards a particular fairness metric, the optimal choice is often not trivial and a combination of algorithms and bias mitigation techniques, e.g. determined via tuning might result in an optimal model.</p>
<p>Bias-mitigation techniques, as proposed above, have the goal of mitigating fairness issues, as measured by fairness metrics.
In practice, this usually comes with several drawbacks:
First, bias-mitigation strategies often lead to a decrease in a classifier’s predictive performance <span class="citation" data-cites="corbett2018measure">(Corbett-Davies and Goel <a href="#ref-corbett2018measure" role="doc-biblioref">2018</a>)</span>.
In addition, processing schemes can worsen interpretability or introduce stochasticity during prediction (see, e.g., <span class="citation" data-cites="hardt2016equality">Hardt et al. (<a href="#ref-hardt2016equality" role="doc-biblioref">2016</a>)</span>).
Furthermore, we want to caution against favouring bias-mitigation techniques over policy interventions that tackle biases at their root cause.
A different set of risks is posed by <em>fairwashing</em> <span class="citation" data-cites="fairwashing">(Aivodji et al. <a href="#ref-fairwashing" role="doc-biblioref">2019</a>)</span>, i.e., finding fair explanations or satisfying fairness metrics for otherwise unfair models.
If biases are only addressed at a given moment and without regard for downstream effects, they might simultaneously lead to a decrease in predictive performance in the near term and to negative consequences for the sensitive group in the long term <span class="citation" data-cites="schwobel-facct22a">(Schwöbel and Remmers <a href="#ref-schwobel-facct22a" role="doc-biblioref">2022</a>)</span>.</p>
<h1 data-number="3" id="main"><span class="header-section-number">3</span> The <a href="#">mlr3fairness</a> package</h1>
<p>In this section, we first give an overview of related software.
Next, we give a very briefly introduce to the <a href="https://cran.r-project.org/package=mlr3">mlr3</a> ecosystem of packages.
Finally, the implemented extensions for fairness are presented.</p>
<h2 class="unnumbered" data-number="" id="related">Related software</h2>
<p>Several R packages provide similar capabilities to our software, but mostly focus on fairness metrics and visualization.
The <a href="https://cran.r-project.org/package=fairness">fairness</a> package <span class="citation" data-cites="fairness">(Kozodoi and V. Varga <a href="#ref-fairness" role="doc-biblioref">2021</a>)</span> allows for the calculation of a variety of fairness metrics, while <a href="https://cran.r-project.org/package=aif360">aif360</a> <span class="citation" data-cites="aif360">(Bellamy et al. <a href="#ref-aif360" role="doc-biblioref">2019</a>)</span> wraps the Python <a href="#">aif360</a> module allowing for the computation of fairness metrics and several bias mitigation techniques but has only limited interoperability with R objects such as s.
The <a href="https://cran.r-project.org/package=fairmodels">fairmodels</a> <span class="citation" data-cites="fairmodels">(Wiśniewski and Biecek <a href="#ref-fairmodels" role="doc-biblioref">2022</a>)</span> package again allows for the computation of fairness metrics for classification and regression settings as well as several bias mitigation techniques.
It tightly integrates with <a href="https://cran.r-project.org/package=DALEX">DALEX</a> <span class="citation" data-cites="dalex">(Biecek <a href="#ref-dalex" role="doc-biblioref">2018</a>)</span> to gain further insight using interpretability techniques.</p>
<p>Outside R, in Python, the <a href="#">fairlearn</a> module <span class="citation" data-cites="fairlearn">(Bird et al. <a href="#ref-fairlearn" role="doc-biblioref">2020</a>)</span> provides ample functionality to study a wide variety of metrics, bias mitigation with respect to a variety of pre-, in- and postprocessing methods as well as to visualize differences.
It furthermore provides a <em>fairlearn dashboard</em> providing a comprehensive fairness report.
The <a href="#">aif360</a> <span class="citation" data-cites="aif360">(Bellamy et al. <a href="#ref-aif360" role="doc-biblioref">2019</a>)</span> module similarly provides metrics as well as bias mitigation techniques while the <a href="#">aequitas</a> fairness toolkit <span class="citation" data-cites="saleiro2018aequitas">(Saleiro et al. <a href="#ref-saleiro2018aequitas" role="doc-biblioref">2018</a>)</span> provides similar capabilities.
Interoperability with the <a href="#">scikit-learn</a> <span class="citation" data-cites="sklearn">(Pedregosa et al. <a href="#ref-sklearn" role="doc-biblioref">2011</a>)</span> ML framework allows for bias mitigation for a wide variety of ML models in all aforementioned systems.
Similar capabilities are also available in Julia’s <a href="#">Fairness.jl</a> <span class="citation" data-cites="fairnessjl">(Agrawal et al. <a href="#ref-fairnessjl" role="doc-biblioref">2020</a><a href="#ref-fairnessjl" role="doc-biblioref">a</a>)</span> library.</p>
<h2 class="unnumbered" data-number="" id="mlr3">The mlr3 ecosystem</h2>
<p><a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a> is tightly integrated into the ecosystem of packages around the ML framework <a href="https://cran.r-project.org/package=mlr3">mlr3</a> <span class="citation" data-cites="mlr3">(Lang et al. <a href="#ref-mlr3" role="doc-biblioref">2019</a>)</span>.
<a href="https://cran.r-project.org/package=mlr3">mlr3</a> provides the infrastructure to fit, resample, and evaluate over 100 ML algorithms using a unified API.
Multiple extension packages bring numerous additional advantages and extra functionality.
In the context of fairness, the following extension packages deserve special mention:</p>
<ul>
<li><a href="https://cran.r-project.org/package=mlr3pipelines">mlr3pipelines</a> <span class="citation" data-cites="mlr3pipelines">(Binder et al. <a href="#ref-mlr3pipelines" role="doc-biblioref">2021</a>)</span> for pre- and postprocessing via pipelining.
This allows composing bias mitigation techniques with arbitrary ML algorithms shipped with <a href="https://cran.r-project.org/package=mlr3">mlr3</a> as well as fusing ML algorithms with preprocessing steps such as imputation or class balancing.
It furthermore integrates with <a href="https://cran.r-project.org/package=mcboost">mcboost</a> <span class="citation" data-cites="mcboost">(Pfisterer et al. <a href="#ref-mcboost" role="doc-biblioref">2021</a>)</span>, which implements additional bias mitigation methods.
We present an example in the supplementary material.</li>
<li><a href="https://cran.r-project.org/package=mlr3tuning">mlr3tuning</a> for its extensive tuning capabilities.</li>
<li><a href="#">mlr3proba</a> <span class="citation" data-cites="mlr3proba">(Sonabend et al. <a href="#ref-mlr3proba" role="doc-biblioref">2021</a>)</span> for survival analysis.</li>
<li><a href="https://cran.r-project.org/package=mlr3benchmark">mlr3benchmark</a> for post-hoc analysis of benchmarked approaches.</li>
<li><a href="https://cran.r-project.org/package=mlr3oml">mlr3oml</a> as a connector to OpenML <span class="citation" data-cites="Vanschoren2014">(Vanschoren et al. <a href="#ref-Vanschoren2014" role="doc-biblioref">2014</a>)</span>, an online scientific platform for collaborative ML.</li>
</ul>
<p>In order to provide the required understanding for <a href="https://cran.r-project.org/package=mlr3">mlr3</a>, we briefly introduce some terminology and syntax.
A full introduction can be found in the mlr3 book <span class="citation" data-cites="mlr3book">(Bernd et al. <a href="#ref-mlr3book" role="doc-biblioref">2023</a>)</span>.</p>
<p>A <code>Task</code> in <a href="https://cran.r-project.org/package=mlr3">mlr3</a> is a basic building block holding the data, storing covariates and the target variable along with some meta-information.
The shorthand constructor function <code>tsk()</code> can be used to quickly access example tasks shipped with <a href="https://cran.r-project.org/package=mlr3">mlr3</a> or <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a>.
In the following chunk, we retrieve the binary classification task with id <code>"adult_train"</code> from the package.
It contains a part of the <code>Adult</code> data set <span class="citation" data-cites="uci">(Dua and Graff <a href="#ref-uci" role="doc-biblioref">2017</a>)</span>.
The task is to predict whether an individual earns more than $50.000 per year.
The column <code>"sex"</code> is set as a binary sensitive attribute with levels <code>"Female"</code> and <code>"Male"</code>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://mlr3verse.mlr-org.com'>"mlr3verse"</a></span><span class='op'>)</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://mlr3fairness.mlr-org.com'>"mlr3fairness"</a></span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>task</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsk</a></span><span class='op'>(</span><span class='st'>"adult_train"</span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://rdrr.io/r/base/print.html'>print</a></span><span class='op'>(</span><span class='va'>task</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>&lt;TaskClassif:adult_train&gt; (30718 x 13)
* Target: target
* Properties: twoclass
* Features (12):
  - fct (7): education, marital_status, occupation, race,
    relationship, sex, workclass
  - int (5): age, capital_gain, capital_loss, education_num,
    hours_per_week
* Protected attribute: sex</code></pre>
</div>
<p>The second building block is the <code>Learner</code>.
It is a wrapper around an ML algorithm, e.g., an implementation of logistic regression or a decision tree.
It can be trained on a <code>Task</code> and used for obtaining a <code>Prediction</code> on an independent test set which can subsequently be scored using a <code>Measure</code> to get an estimate for the predictive performance on new data.
The shorthand constructors <code>lrn()</code> and <code>msr()</code> allow for the instantiation of implemented <code>Learner</code>s and <code>Measure</code>s, respectively.
In the following example, we will first instantiate a learner, then split our data into a train and test set, afterwards train it on the train set of the dataset and finally evaluate predictions on held-out test data.
The train-test split in this case is given by row indices, here stored in the <code>idx</code> variable.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>learner</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.rpart"</span>, predict_type <span class='op'>=</span> <span class='st'>"prob"</span><span class='op'>)</span></span>
<span><span class='va'>idx</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/partition.html'>partition</a></span><span class='op'>(</span><span class='va'>task</span><span class='op'>)</span></span>
<span><span class='va'>learner</span><span class='op'>$</span><span class='fu'>train</span><span class='op'>(</span><span class='va'>task</span>, <span class='va'>idx</span><span class='op'>$</span><span class='va'>train</span><span class='op'>)</span></span>
<span><span class='va'>prediction</span> <span class='op'>=</span> <span class='va'>learner</span><span class='op'>$</span><span class='fu'>predict</span><span class='op'>(</span><span class='va'>task</span>, <span class='va'>idx</span><span class='op'>$</span><span class='va'>test</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>We then employ the <code>classif.acc</code> measure which measures the accuracy of a prediction compared to the true label:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>measure</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"classif.acc"</span><span class='op'>)</span></span>
<span><span class='va'>prediction</span><span class='op'>$</span><span class='fu'>score</span><span class='op'>(</span><span class='va'>measure</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>classif.acc 
     0.8382 </code></pre>
</div>
<p>In the example above, we obtain an accuracy score of 0.8382, meaning our ML model correctly classifies roughly 84 % of the samples in the test data.
As the split into training set and test set is stochastic, the procedure should be repeated multiple times for smaller datasets <span class="citation" data-cites="bischl2012resampling">(Bischl et al. <a href="#ref-bischl2012resampling" role="doc-biblioref">2012</a>)</span> and the resulting performance values should be aggregated.
This process is called resampling, and can easily be performed with the <code>resample()</code> function, yielding a <code>ResampleResult</code> object.
In the following, we employ 10-fold cross-validation as a resampling strategy:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>resampling</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>rsmp</a></span><span class='op'>(</span><span class='st'>"cv"</span>, folds <span class='op'>=</span> <span class='fl'>10</span><span class='op'>)</span></span>
<span><span class='va'>rr</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/resample.html'>resample</a></span><span class='op'>(</span><span class='va'>task</span>, <span class='va'>learner</span>, <span class='va'>resampling</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>We can call the <code>aggregate</code> method on the <code>ResampleResult</code> to obtain the accuracy aggregated across all <span class="math inline">\(10\)</span> replications.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>rr</span><span class='op'>$</span><span class='fu'>aggregate</span><span class='op'>(</span><span class='va'>measure</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>classif.acc 
     0.8408 </code></pre>
</div>
<p>Here, we obtain an accuracy of 0.8408, so slightly higher than previous scores, due to using a larger fraction of the data.
Furthermore, this estimate has a lower variance (as it is an aggregate) at the cost of additional computation time.
To properly compare competing modelling approaches, candidates can be benchmarked against each other using the <code>benchmark()</code> function (yielding a <code>BenchmarkResult</code>).
In the following, we compare the decision tree from above to a logistic regression model.
To do this, we use the <code>benchmark_grid</code> function to compare the two <code>Learners</code> across the same <code>Task</code> and resampling procedure.
Finally, we aggregate the measured scores each learner obtains on each cross-validation split using the <code>$aggregate()</code> function.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>learner2</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.log_reg"</span>, predict_type <span class='op'>=</span> <span class='st'>"prob"</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>grid</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/benchmark_grid.html'>benchmark_grid</a></span><span class='op'>(</span><span class='va'>task</span>, <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>learner</span>, <span class='va'>learner2</span><span class='op'>)</span>, <span class='va'>resampling</span><span class='op'>)</span></span>
<span><span class='va'>bmr</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/benchmark.html'>benchmark</a></span><span class='op'>(</span><span class='va'>grid</span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>bmr</span><span class='op'>$</span><span class='fu'>aggregate</span><span class='op'>(</span><span class='va'>measure</span><span class='op'>)</span><span class='op'>[</span>, <span class='fu'>.</span><span class='op'>(</span><span class='va'>learner_id</span>, <span class='va'>classif.acc</span><span class='op'>)</span><span class='op'>]</span></span></code></pre>
</div>
<pre><code>        learner_id classif.acc
1:   classif.rpart      0.8408
2: classif.log_reg      0.8467</code></pre>
</div>
<p>After running the benchmark, we can again call <code>.$aggregate</code> to obtain aggregated scores.
The <a href="https://cran.r-project.org/package=mlr3viz">mlr3viz</a> package comes with several ready-made visualizations for objects from <code>mlr3</code> via <a href="https://cran.r-project.org/package=ggplot2">ggplot2</a>’s <span class="citation" data-cites="ggplot2">(Wickham <a href="#ref-ggplot2" role="doc-biblioref">2016</a>)</span> <code>autoplot</code> function.
For a <code>BenchmarkResult</code>, the <code>autoplot</code> function provides a Box-plot comparison of performances across the cross-validation folds for each <code>Learner</code>.
Figure <a href="#fig:bmrbox">1</a> contains the box-plot comparison.
We can see that <code>log_reg</code> has higher accuracy and lower interquartile range across the 10 folds, and we might therefore want to prefer the <code>log_reg</code> model.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bmrbox"></span>
<img src="mlr3fairness_files/figure-html5/bmrbox-1.png" alt="Side-by-side comparison of twoboxplots for the rpart and log\_reg learners, both showing a classification accuracy of around .845 and IQR of .005 with log\_reg having slightly higher accuracy." width="336" />
<p class="caption">
Figure 1: Model comparison based on accuracy for decision trees (rpart) and logistic regression (log_reg) across resampling splits.
</p>
</div>
</div>
<h3 class="unnumbered" data-number="" id="selecting-the-sensitive-attribute">Selecting the sensitive attribute</h3>
<p>For a given task, we can select one or multiple sensitive attributes.
In <a href="https://cran.r-project.org/package=mlr3">mlr3</a>, the sensitive attribute is identified by the column role <code>pta</code> and can be set as follows:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>task</span><span class='op'>$</span><span class='fu'>set_col_roles</span><span class='op'>(</span><span class='st'>"marital_status"</span>, add_to <span class='op'>=</span> <span class='st'>"pta"</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>In the example above, we add the <code>"martial_status"</code> as an additional sensitive attribute.
This information is then automatically passed on when the task is used, e.g., when computing fairness metrics.
If more than one sensitive attribute is specified, metrics will be computed based on intersecting groups formed by the columns.</p>
<h3 class="unnumbered" data-number="" id="quantifying-fairness-1">Quantifying fairness</h3>
<p>With the <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a> package loaded, fairness measures can be constructed via <code>msr()</code> like any other measure in <a href="https://cran.r-project.org/package=mlr3">mlr3</a>.
They are listed with prefix <em>fairness</em>, and simply calling <code>msr()</code> without any arguments will return a list of all available measures.
Table <a href="#tab:metrics">1</a> provides an overview over some popular fairness measures which are readily available.</p>
<table>
<caption><span id="tab:metrics">Table 1: </span> Overview of fairness metrics available with mlr3fairness.</caption>
<colgroup>
<col style="width: 19%" />
<col style="width: 80%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">key</th>
<th style="text-align: left;">description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">fairness.acc</td>
<td style="text-align: left;">Accuracy equality </td>
</tr>
<tr class="even">
<td style="text-align: left;">fairness.mse</td>
<td style="text-align: left;">Mean squared error equality (Regression)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">fairness.eod</td>
<td style="text-align: left;">Equalized odds </td>
</tr>
<tr class="even">
<td style="text-align: left;">fairness.tpr</td>
<td style="text-align: left;">True positive rate equality / Equality of opportunity </td>
</tr>
<tr class="odd">
<td style="text-align: left;">fairness.fpr</td>
<td style="text-align: left;">False positive rate equality / Predictive equality </td>
</tr>
<tr class="even">
<td style="text-align: left;">fairness.tnr</td>
<td style="text-align: left;">True negative rate equality</td>
</tr>
<tr class="odd">
<td style="text-align: left;">fairness.fnr</td>
<td style="text-align: left;">False negative rate equality </td>
</tr>
<tr class="even">
<td style="text-align: left;">fairness.fomr</td>
<td style="text-align: left;">False omission rate equality </td>
</tr>
<tr class="odd">
<td style="text-align: left;">fairness.tnr</td>
<td style="text-align: left;">Negative predictive value equality</td>
</tr>
<tr class="even">
<td style="text-align: left;">fairness.tnr</td>
<td style="text-align: left;">Positive predictive value equality</td>
</tr>
<tr class="odd">
<td style="text-align: left;">fairness.cv</td>
<td style="text-align: left;">Demographic parity / Equalized positive rates </td>
</tr>
<tr class="even">
<td style="text-align: left;">fairness.pp</td>
<td style="text-align: left;">Predictive parity / Equalized precision </td>
</tr>
<tr class="odd">
<td style="text-align: left;">fairness.{tp, fp, tn, fn}</td>
<td style="text-align: left;">Equal true positives, false positives, true negatives, false negatives</td>
</tr>
<tr class="even">
<td style="text-align: left;">fairness.acc_eod=.05</td>
<td style="text-align: left;">Accuracy under equalized odds constraint </td>
</tr>
<tr class="odd">
<td style="text-align: left;">fairness.acc_ppv=.05</td>
<td style="text-align: left;">Accuracy under ppv constraint </td>
</tr>
</tbody>
</table>
<p>Furthermore, new custom fairness measures can be easily implemented, either by implementing them directly or by composing them from existing metrics.
This process is extensively documented in an accompanying <a href="https://mlr3fairness.mlr-org.com/articles/measures-vignette.html">measures vignette</a> available with the package.</p>
<p>Here we choose the binary accuracy measure <code>"classif.acc"</code> and the equalized odds metric from above using <code>"fairness.eod"</code>:
The constructed list of measures can then be used to score a <code>Prediction</code>, a <code>ResampleResult</code> or <code>BenchmarkResult</code>, e.g.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>measures</span> <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"classif.acc"</span><span class='op'>)</span>, <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"fairness.eod"</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>rr</span><span class='op'>$</span><span class='fu'>aggregate</span><span class='op'>(</span><span class='va'>measures</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>            classif.acc fairness.equalized_odds 
                0.84078                 0.07939 </code></pre>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>We can clearly see a comparatively large difference in equalized odds at around 0.08.
This means, that in total, the false positive rates (FPR) and true positive rates (TPR) on average differ by ~0.08, indicating that our model might exhibit a bias.
Looking at the individual components yields a clearer picture.
Here, we are looking at the confusion matrices of the combined predictions of the 10 folds, grouped by sensitive attribute:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://mlr3fairness.mlr-org.com/reference/fairness_tensor.html'>fairness_tensor</a></span><span class='op'>(</span><span class='va'>rr</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>$Male
        truth
response   &lt;=50K    &gt;50K
   &lt;=50K 0.43030 0.10033
   &gt;50K  0.03408 0.11202

$Female
        truth
response    &lt;=50K     &gt;50K
   &lt;=50K 0.282668 0.020900
   &gt;50K  0.003907 0.015789</code></pre>
</div>
<p>Plotting the prediction density or comparing measures graphically often provides additional insights:
For example, in Figure <a href="#fig:predplots">2</a>, we can see that Females are more often predicted to earn below $50.000.
Similarly, we can see that both equality in FPR and TPR differ considerably.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://mlr3fairness.mlr-org.com/reference/fairness_prediction_density.html'>fairness_prediction_density</a></span><span class='op'>(</span><span class='va'>prediction</span>, <span class='va'>task</span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://mlr3fairness.mlr-org.com/reference/fairness_compare_metrics.html'>compare_metrics</a></span><span class='op'>(</span><span class='va'>prediction</span>, <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msrs</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"fairness.fpr"</span>, <span class='st'>"fairness.tpr"</span>, <span class='st'>"fairness.eod"</span><span class='op'>)</span><span class='op'>)</span>, <span class='va'>task</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:predplots"></span>
<img src="mlr3fairness_files/figure-html5/predplots-1.png" alt="Two pannel plot. On the left hand side, a plot of rediction densities for the negative class for groups Female and Male with density concentrating towards the right.  Plots show a higher likelihood for the '&lt;50k' class for females resulting in large fairness metrics. Right: Fairness metrics comparison using bar plots for FPR, TPR, EOd metrics indicating disparities of aroung 0.1 for all metrics." width="576" />
<p class="caption">
Figure 2: Visualizing predictions of the decision tree model. Left: Prediction densities for the negative class for Female and Male. Right: Fairness metrics comparison for FPR, TPR, EOd metrics. Plots show a higher likelihood for the ‘&lt;50k’ class for females resulting in fairness metrics different from 0.
</p>
</div>
</div>
<h3 class="unnumbered" data-number="" id="bias-mitigation-1">Bias mitigation</h3>
<p>As mentioned above, several ways to improve a model’s fairness exist.
While non-technical interventions, such as e.g. collecting more data should be preferred,
<a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a> provides several bias mitigation techniques that can be used together with a <code>Learner</code> to obtain fairer models.
Table <a href="#tab:biasmitigation">2</a> provides an overview of implemented bias mitigation techniques.
They are implemented as <code>PipeOps</code> from the <a href="https://cran.r-project.org/package=mlr3pipelines">mlr3pipelines</a> package and can be
combined with arbitrary learners using the <code>%&gt;&gt;%</code> operator to build a pipeline that can later be trained.
In the following example, we show how to combine a learner with a reweighing scheme (<code>reweighing_wts</code>) or alternatively how to post-process predictions using the equalized odds debiasing (<code>EOd</code>) strategy.
An introduction to <a href="https://cran.r-project.org/package=mlr3pipelines">mlr3pipelines</a> is available in the corresponding <a href="https://mlr3book.mlr-org.com/pipelines.html">mlr3book chapter</a> <span class="citation" data-cites="mlr3book">(Bernd et al. <a href="#ref-mlr3book" role="doc-biblioref">2023</a>)</span>.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"reweighing_wts"</span><span class='op'>)</span> <span class='op'><a href='https://mlr3pipelines.mlr-org.com/reference/grapes-greater-than-greater-than-grapes.html'>%&gt;&gt;%</a></span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.glmnet"</span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"learner_cv"</span>, <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.glmnet"</span><span class='op'>)</span><span class='op'>)</span> <span class='op'><a href='https://mlr3pipelines.mlr-org.com/reference/grapes-greater-than-greater-than-grapes.html'>%&gt;&gt;%</a></span> <span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"EOd"</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<table>
<caption><span id="tab:biasmitigation">Table 2: </span> Overview of bias mitigation techniques available in mlr3fairness.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Key</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Reference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">EOd</td>
<td style="text-align: left;">Equalized-Odds Debiasing</td>
<td style="text-align: left;">Postprocessing</td>
<td style="text-align: left;"><span class="citation" data-cites="hardt2016equality">Hardt et al. (<a href="#ref-hardt2016equality" role="doc-biblioref">2016</a>)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">reweighing_os</td>
<td style="text-align: left;">Reweighing (Oversampling)</td>
<td style="text-align: left;">Preprocessing</td>
<td style="text-align: left;"><span class="citation" data-cites="kamiran2012data">Kamiran and Calders (<a href="#ref-kamiran2012data" role="doc-biblioref">2012</a>)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">reweighing_wts</td>
<td style="text-align: left;">Reweighing (Instance weights)</td>
<td style="text-align: left;">Preprocessing</td>
<td style="text-align: left;"><span class="citation" data-cites="kamiran2012data">Kamiran and Calders (<a href="#ref-kamiran2012data" role="doc-biblioref">2012</a>)</span></td>
</tr>
</tbody>
</table>
<p>It is simple for users or package developers to extend <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a> with additional
bias mitigation methods – as an example, the <a href="https://cran.r-project.org/package=mcboost">mcboost</a> package adds further postprocessing methods
that can improve fairness.<br>
Along with pipeline operators, <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a> contains several machine learning algorithms listed in table @ref{tab:fairlearns} that can directly incorporate
fairness constraints. They can similarly be constructed using the <code>lrn()</code> shorthand.</p>
<table>
<caption><span id="tab:fairlearns">Table 3: </span> Overview of fair ML algorithms available with mlr3fairness.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Key</th>
<th style="text-align: left;">Package</th>
<th style="text-align: left;">Reference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">regr.fairfrrm</td>
<td style="text-align: left;">fairml</td>
<td style="text-align: left;"><span class="citation" data-cites="scutari">Scutari et al. (<a href="#ref-scutari" role="doc-biblioref">2021</a>)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">classif.fairfgrrm</td>
<td style="text-align: left;">fairml</td>
<td style="text-align: left;"><span class="citation" data-cites="scutari">Scutari et al. (<a href="#ref-scutari" role="doc-biblioref">2021</a>)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">regr.fairzlm</td>
<td style="text-align: left;">fairml</td>
<td style="text-align: left;"><span class="citation" data-cites="Zafar2017">Zafar et al. (<a href="#ref-Zafar2017" role="doc-biblioref">2017</a>)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">classif.fairzlrm</td>
<td style="text-align: left;">fairml</td>
<td style="text-align: left;"><span class="citation" data-cites="Zafar2017">Zafar et al. (<a href="#ref-Zafar2017" role="doc-biblioref">2017</a>)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">regr.fairnclm</td>
<td style="text-align: left;">fairml</td>
<td style="text-align: left;"><span class="citation" data-cites="komiyama">Komiyama et al. (<a href="#ref-komiyama" role="doc-biblioref">2018</a>)</span></td>
</tr>
</tbody>
</table>
<h3 class="unnumbered" data-number="" id="reports">Reports</h3>
<p>Because fairness aspects can not always be investigated based on the fairness definitions above (e.g., due to biased sampling or labelling procedures), it is important to document data collection and the resulting data as well as the models resulting from this data.
Informing auditors about those aspects of a deployed model can lead to better assessments of a model’s fairness.
Questionnaires for ML models <span class="citation" data-cites="modelcards">(Mitchell et al. <a href="#ref-modelcards" role="doc-biblioref">2019</a>)</span> and data sets <span class="citation" data-cites="datasheets">(Gebru et al. <a href="#ref-datasheets" role="doc-biblioref">2021</a>)</span> have been proposed in literature.
We further add automated report templates using R markdown <span class="citation" data-cites="rmarkdown">(Xie et al. <a href="#ref-rmarkdown" role="doc-biblioref">2020</a>)</span> for data sets and ML models.
In addition, we provide a template for a <em>fairness report</em> which includes many fairness metrics and visualizations to provide a good starting point for generating a fairness report inspired by the <em>Aequitas Toolkit</em> <span class="citation" data-cites="saleiro2018aequitas">(Saleiro et al. <a href="#ref-saleiro2018aequitas" role="doc-biblioref">2018</a>)</span>.
A preview for the different reports can be obtained from the <a href="https://mlr3fairness.mlr-org.com/articles/reports-vignette.html">Reports vignette</a> in the package documentation.</p>
<table>
<caption><span id="tab:reports">Table 4: </span> Overview of reports generated by mlr3fairness.</caption>
<thead>
<tr class="header">
<th>Report</th>
<th>Description</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3fairness.mlr-org.com/articles/modelcard/modelcard.html"><code>report_modelcard()</code></a></td>
<td>Modelcard for ML models</td>
<td><span class="citation" data-cites="modelcards">Mitchell et al. (<a href="#ref-modelcards" role="doc-biblioref">2019</a>)</span></td>
</tr>
<tr class="even">
<td><a href="https://mlr3fairness.mlr-org.com/articles/datasheet/datasheet.html"><code>report_datasheet()</code></a></td>
<td>Datasheet for data sets</td>
<td><span class="citation" data-cites="datasheets">Gebru et al. (<a href="#ref-datasheets" role="doc-biblioref">2021</a>)</span></td>
</tr>
<tr class="odd">
<td><a href="https://mlr3fairness.mlr-org.com/articles/fairness/fairness.html"><code>report_fairness()</code></a></td>
<td>Fairness Report</td>
<td>–</td>
</tr>
</tbody>
</table>
<h1 data-number="4" id="case-study"><span class="header-section-number">4</span> Case study</h1>
<p>In order to demonstrate a full workflow, we conduct full bias assessment and bias mitigation on the popular adult data set <span class="citation" data-cites="uci">(Dua and Graff <a href="#ref-uci" role="doc-biblioref">2017</a>)</span>.
The goal is to predict whether an individual’s income is larger than $<span class="math inline">\(50.000\)</span> with the sensitive attribute being <em>gender</em>.
The data set ships with <a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a>, separated into a <em>train</em> and <em>test</em> task and can be instantiated using <code>tsk("adult_train")</code> and <code>tsk("adult_test")</code>, respectively.
As a fairness metric, we consider <em>true positive parity</em> which calls for equality in the true positive rates across groups, in this case the <code>sex</code> variable.
We furthermore are interested in the model’s utility, here measured as its classification accuracy.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://mlr3verse.mlr-org.com'>"mlr3verse"</a></span><span class='op'>)</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='st'><a href='https://mlr3fairness.mlr-org.com'>"mlr3fairness"</a></span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>task</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsk</a></span><span class='op'>(</span><span class='st'>"adult_train"</span><span class='op'>)</span></span>
<span><span class='fu'><a href='https://rdrr.io/r/base/print.html'>print</a></span><span class='op'>(</span><span class='va'>task</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>&lt;TaskClassif:adult_train&gt; (30718 x 13)
* Target: target
* Properties: twoclass
* Features (12):
  - fct (7): education, marital_status, occupation, race,
    relationship, sex, workclass
  - int (5): age, capital_gain, capital_loss, education_num,
    hours_per_week
* Protected attribute: sex</code></pre>
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>measures</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msrs</a></span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='st'>"fairness.tpr"</span>, <span class='st'>"classif.acc"</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>In order to get an initial perspective, we benchmark three models using 3-fold cross-validation each:</p>
<ul>
<li>a classification tree from the <a href="https://cran.r-project.org/package=rpart">rpart</a> package,</li>
<li>a penalized logistic regression from the <a href="https://cran.r-project.org/package=glmnet">glmnet</a> package and</li>
<li>a penalized logistic regression from the <a href="https://cran.r-project.org/package=glmnet">glmnet</a> package, but with reweighing preprocessing.</li>
</ul>
<p>The logistic regression in the latter two approaches do not support operating on factor features natively, therefore we pre-process the data with a feature encoder from <a href="https://cran.r-project.org/package=mlr3pipelines">mlr3pipelines</a>.
To achieve this, we connect the feature encoder <code>po("encode")</code> with the learner using the <code>%&gt;&gt;%</code> operator.
This encodes factor variables into integers using dummy encoding.
We then evaluate all three learners on the <code>adult_train</code> data using 3-fold cross-validation by building up a grid of experiments we want to run using <code>benchmark_grid</code>.
This grid is then executed using the <code>benchmark</code> function, and we can aggregate the performance and fairness metric scores via the <code>$aggregate()</code> function.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/r/base/Random.html'>set.seed</a></span><span class='op'>(</span><span class='fl'>4321</span><span class='op'>)</span></span>
<span><span class='va'>learners</span> <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span></span>
<span>    <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.rpart"</span><span class='op'>)</span>,</span>
<span>    <span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"encode"</span><span class='op'>)</span> <span class='op'><a href='https://mlr3pipelines.mlr-org.com/reference/grapes-greater-than-greater-than-grapes.html'>%&gt;&gt;%</a></span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.glmnet"</span><span class='op'>)</span>,</span>
<span>    <span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"encode"</span><span class='op'>)</span> <span class='op'><a href='https://mlr3pipelines.mlr-org.com/reference/grapes-greater-than-greater-than-grapes.html'>%&gt;&gt;%</a></span> <span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"reweighing_wts"</span><span class='op'>)</span> <span class='op'><a href='https://mlr3pipelines.mlr-org.com/reference/grapes-greater-than-greater-than-grapes.html'>%&gt;&gt;%</a></span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.glmnet"</span><span class='op'>)</span></span>
<span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>grid</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/benchmark_grid.html'>benchmark_grid</a></span><span class='op'>(</span></span>
<span>  tasks <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsks</a></span><span class='op'>(</span><span class='st'>"adult_train"</span><span class='op'>)</span>,</span>
<span>  learners <span class='op'>=</span> <span class='va'>learners</span>,</span>
<span>  resamplings <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>rsmp</a></span><span class='op'>(</span><span class='st'>"cv"</span>, folds <span class='op'>=</span> <span class='fl'>3</span><span class='op'>)</span></span>
<span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>bmr1</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/benchmark.html'>benchmark</a></span><span class='op'>(</span><span class='va'>grid</span><span class='op'>)</span></span>
<span><span class='va'>bmr1</span><span class='op'>$</span><span class='fu'>aggregate</span><span class='op'>(</span><span class='va'>measures</span><span class='op'>)</span><span class='op'>[</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>, <span class='fl'>7</span>, <span class='fl'>8</span><span class='op'>)</span><span class='op'>]</span></span></code></pre>
</div>
<pre><code>                             learner_id fairness.tpr classif.acc
1:                        classif.rpart     0.059767      0.8408
2:                encode.classif.glmnet     0.070781      0.8411
3: encode.reweighing_wts.classif.glmnet     0.004732      0.8351</code></pre>
</div>
<p>The preprocessing step of reweighing already improved the fairness while sacrificing only a tiny bit of performance.
To see if we can further improve, we use <a href="https://cran.r-project.org/package=mlr3tuning">mlr3tuning</a> to jointly tune all hyperparameters of the <em>glmnet</em> model as well as our reweighing hyperparameter.
In order to do this, we use an <code>AutoTuner</code> from <a href="https://cran.r-project.org/package=mlr3tuning">mlr3tuning</a>; a model that tunes its own hyperparameters during training.
The full code for setting up this model can be found in the appendix.
An <code>AutoTuner</code> requires a specific metric to tune for.
Here, we define a fairness-thresholded accuracy metric. We set <span class="math inline">\(\epsilon = 0.01\)</span> as a threshold:</p>
<p><span class="math display">\[
  if \; |\Delta_{EOd}| \leq \epsilon: \textrm{accuracy} \;\; else: \;  - |\Delta_{EOd}|.
\]</span></p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>metric</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"fairness.constraint"</span>,</span>
<span>    performance_measure <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"classif.acc"</span><span class='op'>)</span>,</span>
<span>    fairness_measure <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"fairness.eod"</span><span class='op'>)</span>,</span>
<span>    epsilon <span class='op'>=</span> <span class='fl'>0.01</span></span>
<span><span class='op'>)</span></span></code></pre>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>We then design the pipeline and the hyperparameters we want to tune over.
In the following example, we choose <code>tuning_iters = 3</code> and set a small range for the hyperparameters in <code>vals</code> to shorten the run time of the tuning procedure.
In real settings, this parameter would be set to a larger number, such as <span class="math inline">\(100\)</span>.
To construct a self-tuning learner, we construct an <code>AutoTuner</code> that takes as input a learner, the resampling procedure and metric used for tuning as well as the tuning strategy along with a termination criterion (here how many tuning iterations should be run).
In addition, we provide a new <code>id</code> for subsequent printing and visualization.
We can then use this self-tuning learner like any other learner and benchmark it using <code>benchmark</code> as described above.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>tuning_iters</span> <span class='op'>=</span> <span class='fl'>3</span></span>
<span><span class='va'>at</span> <span class='op'>=</span> <span class='va'><a href='https://mlr3tuning.mlr-org.com/reference/AutoTuner.html'>AutoTuner</a></span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span><span class='va'>lrn</span>, <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>rsmp</a></span><span class='op'>(</span><span class='st'>"holdout"</span><span class='op'>)</span>,</span>
<span>    <span class='va'>metric</span>,</span>
<span>    tuner <span class='op'>=</span> <span class='fu'>mlr3tuning</span><span class='fu'>::</span><span class='fu'><a href='https://mlr3tuning.mlr-org.com/reference/tnr.html'>tnr</a></span><span class='op'>(</span><span class='st'>"random_search"</span><span class='op'>)</span>,</span>
<span>    terminator <span class='op'>=</span> <span class='fu'><a href='https://bbotk.mlr-org.com/reference/trm.html'>trm</a></span><span class='op'>(</span><span class='st'>"evals"</span>, n_evals <span class='op'>=</span> <span class='va'>tuning_iters</span><span class='op'>)</span></span>
<span><span class='op'>)</span></span>
<span><span class='va'>at</span><span class='op'>$</span><span class='va'>id</span> <span class='op'>=</span> <span class='st'>"glmnet_weighted_tuned"</span></span>
<span></span>
<span><span class='va'>grd</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/benchmark_grid.html'>benchmark_grid</a></span><span class='op'>(</span></span>
<span>  tasks <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsks</a></span><span class='op'>(</span><span class='st'>"adult_train"</span><span class='op'>)</span>,</span>
<span>  learners <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span><span class='va'>at</span><span class='op'>)</span>,</span>
<span>  resamplings <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>rsmp</a></span><span class='op'>(</span><span class='st'>"cv"</span>, folds <span class='op'>=</span> <span class='fl'>3</span><span class='op'>)</span></span>
<span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>bmr2</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/benchmark.html'>benchmark</a></span><span class='op'>(</span><span class='va'>grd</span>, store_models <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span></span>
<span><span class='va'>bmr2</span><span class='op'>$</span><span class='fu'>aggregate</span><span class='op'>(</span><span class='va'>measures</span><span class='op'>)</span><span class='op'>[</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>, <span class='fl'>7</span>, <span class='fl'>8</span><span class='op'>)</span><span class='op'>]</span></span></code></pre>
</div>
<pre><code>              learner_id fairness.tpr classif.acc
1: glmnet_weighted_tuned     0.009486      0.8385</code></pre>
</div>
<p>The result improves w.r.t. accuracy while only slightly decreasing the measured fairness.
Note that the generalization error is estimated using a holdout strategy during training and slight violations of the desired threshold <span class="math inline">\(\epsilon\)</span> should therefore be considered <span class="citation" data-cites="feurer23">(Feurer et al. <a href="#ref-feurer23" role="doc-biblioref">2023</a>)</span>.
The results of both benchmark experiments can then be collected and jointly visualized in Figure <a href="#fig:fat">3</a> visualizing accuracy and fairness of models in our benchmark.
In addition to aggregate scores (denoted by a cross) individual iterations of the 3-fold Cross-Validation (represented by points) are shown to visualize variations in the individual results.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fat"></span>
<img src="mlr3fairness_files/figure-html5/fat-1.png" alt="Fairness-Accuracy tradeoff for 3-fold CV on the adult train set." width="1800" />
<p class="caption">
Figure 3: Fairness-Accuracy tradeoff for 3-fold CV on the adult train set.
</p>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>bmr</span><span class='op'>$</span><span class='fu'>aggregate</span><span class='op'>(</span><span class='va'>measures</span><span class='op'>)</span><span class='op'>[</span>, <span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='fl'>4</span>, <span class='fl'>7</span>, <span class='fl'>8</span><span class='op'>)</span><span class='op'>]</span></span></code></pre>
</div>
<pre><code>                             learner_id fairness.tpr classif.acc
1:                        classif.rpart     0.059767      0.8408
2:                encode.classif.glmnet     0.070781      0.8411
3: encode.reweighing_wts.classif.glmnet     0.004732      0.8351
4:                glmnet_weighted_tuned     0.009486      0.8385</code></pre>
</div>
<p>Especially when considering optimizing accuracy while still retaining a fair model, tuning can be helpful and further improve upon available trade-offs.
In this example, the <code>AutoTuner</code> improves w.r.t. the fairness metric while offering accuracy comparable with the simple <code>glmnet</code> model.
This can e.g. be observed from the fairness accuracy tradeoff shown in Figure <a href="#fig:fat">3</a>.
Whether the achieved accuracy is sufficient, needs to be determined, e.g. from a business context.
For now, we assume that the model obtained from the <code>AutoTuner</code> is the model we might want to use going forward.
Having decided for a final model, we can now train the final model on the full training data</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>at_lrn</span> <span class='op'>=</span> <span class='va'>bmr</span><span class='op'>$</span><span class='va'>learners</span><span class='op'>$</span><span class='va'>learner</span><span class='op'>[[</span><span class='fl'>4</span><span class='op'>]</span><span class='op'>]</span></span>
<span><span class='va'>at_lrn</span><span class='op'>$</span><span class='fu'>train</span><span class='op'>(</span><span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsk</a></span><span class='op'>(</span><span class='st'>"adult_train"</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>and predict on the held out <em>test</em> set available for the <code>Adult</code> dataset to obtain a final estimate.
This is important since estimating fairness metrics often incurs significant variance <span class="citation" data-cites="agrawal2020debiasing">(Agrawal et al. <a href="#ref-agrawal2020debiasing" role="doc-biblioref">2020</a><a href="#ref-agrawal2020debiasing" role="doc-biblioref">b</a>)</span> and evaluation of the test-set provides us with an unbiased estimate of model performance after the previous model selection step.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>test</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsk</a></span><span class='op'>(</span><span class='st'>"adult_test"</span><span class='op'>)</span></span>
<span><span class='va'>at_lrn</span><span class='op'>$</span><span class='fu'>predict</span><span class='op'>(</span><span class='va'>test</span><span class='op'>)</span><span class='op'>$</span><span class='fu'>score</span><span class='op'>(</span><span class='va'>measures</span>, <span class='va'>test</span><span class='op'>)</span></span></code></pre>
</div>
<pre><code>fairness.tpr  classif.acc 
    0.007342     0.834215 </code></pre>
</div>
<p>On the held-out test set, the fairness constraint is slightly violated which can happen due to the comparatively large variance in the estimation of fairness metrics.</p>
<h1 data-number="5" id="summary"><span class="header-section-number">5</span> Summary</h1>
<p>The large-scale availability and use of automated decision making systems have resulted in growing concerns for a lack of fairness in the decisions made by such systems. As a result, fairness auditing methods that allow for investigating (un-)fairness in such systems, especially ones that provide interoperability with machine learning toolkits that allows for ease of use and integration into model evaluation and tuning.
In future work we plan on implementing several tools that further support the user w.r.t. pinpointing potential fairness issues in the data, especially through the help of interpretability tools, such as the <a href="https://cran.r-project.org/package=iml">iml</a> <span class="citation" data-cites="iml">(Molnar et al. <a href="#ref-iml" role="doc-biblioref">2018</a>)</span> package. We furthermore aim to implement additional fairness metrics from the realm of `individual fairness’ <span class="citation" data-cites="dwork2012">(Dwork et al. <a href="#ref-dwork2012" role="doc-biblioref">2012</a>)</span> and `conditional demographic parity’ <span class="citation" data-cites="wachter-vlr2020">(Wachter et al. <a href="#ref-wachter-vlr2020" role="doc-biblioref">2020</a>)</span>.</p>
<div style="page-break-after: always;"></div>
<h1 data-number="6" id="appendix"><span class="header-section-number">6</span> Appendix</h1>
<h2 class="unnumbered" data-number="" id="tuning-the-ml-pipeline">Tuning the ML pipeline</h2>
<p>We include the full code to construct the <code>AutoTuner</code> with additional details
and comments below.
We first load all required packages and use <a href="#">mlr3</a>’s interaction with the <a href="https://cran.r-project.org/package=future">future</a> <span class="citation" data-cites="future">(Bengtsson <a href="#ref-future" role="doc-biblioref">2021</a>)</span> package
to automatically distribute the tuning to all available cores in parallel by setting a <code>plan</code>.
See the documentation of <a href="https://cran.r-project.org/package=future">future</a> for platform-specific hints regarding parallelization.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://mlr3misc.mlr-org.com'>mlr3misc</a></span><span class='op'>)</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://mlr3.mlr-org.com'>mlr3</a></span><span class='op'>)</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://mlr3pipelines.mlr-org.com'>mlr3pipelines</a></span><span class='op'>)</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://mlr3fairness.mlr-org.com'>mlr3fairness</a></span><span class='op'>)</span></span>
<span><span class='kw'><a href='https://rdrr.io/r/base/library.html'>library</a></span><span class='op'>(</span><span class='va'><a href='https://mlr3tuning.mlr-org.com'>mlr3tuning</a></span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># Enable parallelization utilizing all cores</span></span>
<span><span class='co'># future::plan("multicore")</span></span></code></pre>
</div>
</div>
<div class="layout-chunk" data-layout="l-body">

</div>
<p>We then instantiate an ML pipeline using <a href="#">mlr3pipelines</a>. This connects several
modelling steps, in our case <strong>categorical encoding</strong>, <strong>reweighing</strong> and a final <strong>learner</strong> using the
<code>%&gt;&gt;%</code> (double caret) operator, ultimately forming a new learner. This learner can
then subsequently be fit on a <code>Task</code>. We use the <code>po(&lt;key&gt;)</code> shorthand to construct a new
pipeline operator from a dictionary of implemented operators. We conduct <strong>categorical encoding</strong>
because <a href="#">glmnet</a> can not naturally handle categorical variables and we therefore have
to encode them (in our case using <code>one-hot</code> encoding).</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># Define the learner pipeline.</span></span>
<span><span class='va'>lrn</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/as_learner.html'>as_learner</a></span><span class='op'>(</span><span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"encode"</span><span class='op'>)</span> <span class='op'><a href='https://mlr3pipelines.mlr-org.com/reference/grapes-greater-than-greater-than-grapes.html'>%&gt;&gt;%</a></span> <span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"reweighing_wts"</span><span class='op'>)</span> <span class='op'><a href='https://mlr3pipelines.mlr-org.com/reference/grapes-greater-than-greater-than-grapes.html'>%&gt;&gt;%</a></span></span>
<span>  <span class='fu'><a href='https://mlr3pipelines.mlr-org.com/reference/po.html'>po</a></span><span class='op'>(</span><span class='st'>"learner"</span>, <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>lrn</a></span><span class='op'>(</span><span class='st'>"classif.glmnet"</span><span class='op'>)</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>We furthermore have to specify the hyperparameter space our <code>Tuner</code> should tune over.
We do this by defining a list of values with a <code>to_tune()</code> token specifying the range.
Note, that hyperparameter names are prefixed with the respective operation’s id.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='co'># Define the parameter space to optimize over</span></span>
<span><span class='va'>vals</span> <span class='op'>=</span> <span class='fu'><a href='https://rdrr.io/r/base/list.html'>list</a></span><span class='op'>(</span></span>
<span>  reweighing_wts.alpha <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/to_tune.html'>to_tune</a></span><span class='op'>(</span><span class='fl'>0.75</span>, <span class='fl'>1</span><span class='op'>)</span>,</span>
<span>  classif.glmnet.alpha <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/to_tune.html'>to_tune</a></span><span class='op'>(</span><span class='fl'>0.5</span>, <span class='fl'>1</span><span class='op'>)</span>,</span>
<span>  classif.glmnet.s <span class='op'>=</span> <span class='fu'><a href='https://paradox.mlr-org.com/reference/to_tune.html'>to_tune</a></span><span class='op'>(</span><span class='fl'>1e-4</span>, <span class='fl'>1e-2</span>, logscale <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span></span>
<span><span class='op'>)</span></span>
<span></span>
<span><span class='co'># Add search space to the learner</span></span>
<span><span class='va'>lrn</span><span class='op'>$</span><span class='va'>param_set</span><span class='op'>$</span><span class='va'>values</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3misc.mlr-org.com/reference/insert_named.html'>insert_named</a></span><span class='op'>(</span><span class='va'>lrn</span><span class='op'>$</span><span class='va'>param_set</span><span class='op'>$</span><span class='va'>values</span>, <span class='va'>vals</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>Before we now train the model, we again specify a metric we aim to satisfy,
here we would like the equalized odds difference to be smaller than <span class="math inline">\(0.1\)</span>.
In this case, we set a constraint on the <em>equalized odds difference</em> comprised of
the differences in true positive rate (TPR) and false positive rate (FPR):</p>
<p><span class="math display">\[
\Delta_{EOd} = \frac{|\textrm{TPR}_{sex = M} - \textrm{TPR}_{sex = F}| + |\textrm{FPR}_{sex = M} - \textrm{FPR}_{sex = F}|}{2}.
\]</span></p>
<p>This can be done using the <code>fairness.constraint</code> measure.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>metric</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"fairness.constraint"</span>,</span>
<span>    performance_measure <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"classif.acc"</span><span class='op'>)</span>,</span>
<span>    fairness_measure <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"fairness.eod"</span><span class='op'>)</span>,</span>
<span>    epsilon <span class='op'>=</span> <span class='fl'>0.1</span></span>
<span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>We can now instantiate a new <code>AutoTuner</code> using <code>lrn</code> defined above by additionally providing arguments specifying the tuning strategy, in our case random search, the measure to optimize for as well as the number of tuning steps.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>metric</span> <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"fairness.constraint"</span>,</span>
<span>    performance_measure <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"classif.acc"</span><span class='op'>)</span>,</span>
<span>    fairness_measure <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"fairness.eod"</span><span class='op'>)</span>,</span>
<span>    epsilon <span class='op'>=</span> <span class='fl'>0.1</span></span>
<span><span class='op'>)</span></span>
<span></span>
<span><span class='va'>at</span> <span class='op'>=</span> <span class='va'><a href='https://mlr3tuning.mlr-org.com/reference/AutoTuner.html'>AutoTuner</a></span><span class='op'>$</span><span class='fu'>new</span><span class='op'>(</span></span>
<span>  learner <span class='op'>=</span> <span class='va'>lrn</span>, <span class='co'># The learner</span></span>
<span>  resampling <span class='op'>=</span> <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>rsmp</a></span><span class='op'>(</span><span class='st'>"holdout"</span><span class='op'>)</span>, <span class='co'># inner resampling strategy</span></span>
<span>  measure <span class='op'>=</span> <span class='va'>metric</span>, <span class='co'># the metric to optimize for</span></span>
<span>  tuner <span class='op'>=</span> <span class='fu'>mlr3tuning</span><span class='fu'>::</span><span class='fu'><a href='https://mlr3tuning.mlr-org.com/reference/tnr.html'>tnr</a></span><span class='op'>(</span><span class='st'>"random_search"</span><span class='op'>)</span>, <span class='co'># tuning strategy</span></span>
<span>  terminator <span class='op'>=</span> <span class='fu'><a href='https://bbotk.mlr-org.com/reference/trm.html'>trm</a></span><span class='op'>(</span><span class='st'>"evals"</span>, n_evals <span class='op'>=</span> <span class='fl'>30</span><span class='op'>)</span><span class='op'>)</span> <span class='co'># number of tuning steps</span></span></code></pre>
</div>
</div>
<p>The so-constructed <code>AutoTuner</code> can now be used on any classification Task!
Additional information regarding the <code>AutoTuner</code> is again available in the corresponding <a href="https://mlr3book.mlr-org.com/optimization.html#autotuner">mlr3book chapter</a>.
In the following example, we will apply it to the <code>Adult</code> task and train our model.
This will perform a tuning loop for the specified number of evaluations and
automatically retrain the best found parameters on the full data.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>at</span><span class='op'>$</span><span class='fu'>train</span><span class='op'>(</span><span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsk</a></span><span class='op'>(</span><span class='st'>"adult_train"</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>After training, we can look at the best models found, here ordered by our metric.
Note, that our metric reports the negative constraint violation if the constraint is violated and the accuracy in case the constraint is satisfied.</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='fu'><a href='https://rdrr.io/r/utils/head.html'>head</a></span><span class='op'>(</span><span class='va'>at</span><span class='op'>$</span><span class='va'>archive</span><span class='op'>$</span><span class='va'>data</span><span class='op'>[</span><span class='fu'><a href='https://rdrr.io/r/base/order.html'>order</a></span><span class='op'>(</span><span class='va'>fairness.acc_equalized_odds_cstrt</span><span class='op'>)</span>, <span class='fl'>1</span><span class='op'>:</span><span class='fl'>4</span><span class='op'>]</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>We can then use the tuned model to assess our metric on the held out data:</p>
<div class="layout-chunk" data-layout="l-body">
<div class="sourceCode">
<pre class="sourceCode r"><code class="sourceCode r"><span><span class='va'>prd</span> <span class='op'>=</span> <span class='va'>at</span><span class='op'>$</span><span class='fu'>predict</span><span class='op'>(</span><span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsk</a></span><span class='op'>(</span><span class='st'>"adult_test"</span><span class='op'>)</span><span class='op'>)</span></span>
<span><span class='va'>prd</span><span class='op'>$</span><span class='fu'>score</span><span class='op'>(</span><span class='fu'><a href='https://rdrr.io/r/base/c.html'>c</a></span><span class='op'>(</span><span class='va'>metric</span>, <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"classif.acc"</span><span class='op'>)</span>, <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>msr</a></span><span class='op'>(</span><span class='st'>"fairness.eod"</span><span class='op'>)</span><span class='op'>)</span>,  <span class='fu'><a href='https://mlr3.mlr-org.com/reference/mlr_sugar.html'>tsk</a></span><span class='op'>(</span><span class='st'>"adult_test"</span><span class='op'>)</span><span class='op'>)</span></span></code></pre>
</div>
</div>
<p>So our tuned model manages to obtain an accuracy of <code>~0.84</code> while satisfying the specified constraint of <span class="math inline">\(\Delta_{EOd} &lt; 0.1\)</span>.
So to summarize, we have tuned a model to optimize accuracy with respect to a constraint on a selected fairness metric using an <code>AutoTuner</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r distill-force-highlighting-css"><code class="sourceCode r"></code></pre></div>
<h2 class="appendix unnumbered" data-number="" id="cran-packages-used">CRAN packages used</h2>
<p><a href="https://cran.r-project.org/package=mlr3fairness">mlr3fairness</a>, <a href="https://cran.r-project.org/package=caret">caret</a>, <a href="https://cran.r-project.org/package=tidymodels">tidymodels</a>, <a href="https://cran.r-project.org/package=SuperLearner">SuperLearner</a>, <a href="https://cran.r-project.org/package=mlr">mlr</a>, <a href="https://cran.r-project.org/package=mlr3">mlr3</a>, <a href="https://cran.r-project.org/package=mlr3tuning">mlr3tuning</a>, <a href="https://cran.r-project.org/package=fairness">fairness</a>, <a href="https://cran.r-project.org/package=aif360">aif360</a>, <a href="https://cran.r-project.org/package=fairmodels">fairmodels</a>, <a href="https://cran.r-project.org/package=DALEX">DALEX</a>, <a href="https://cran.r-project.org/package=mlr3pipelines">mlr3pipelines</a>, <a href="https://cran.r-project.org/package=mcboost">mcboost</a>, <a href="https://cran.r-project.org/package=mlr3benchmark">mlr3benchmark</a>, <a href="https://cran.r-project.org/package=mlr3oml">mlr3oml</a>, <a href="https://cran.r-project.org/package=mlr3viz">mlr3viz</a>, <a href="https://cran.r-project.org/package=ggplot2">ggplot2</a>, <a href="https://cran.r-project.org/package=rpart">rpart</a>, <a href="https://cran.r-project.org/package=glmnet">glmnet</a>, <a href="https://cran.r-project.org/package=iml">iml</a>, <a href="https://cran.r-project.org/package=future">future</a></p>
<h2 class="appendix unnumbered" data-number="" id="cran-task-views-implied-by-cited-packages">CRAN Task Views implied by cited packages</h2>
<p><a href="https://cran.r-project.org/view=Bayesian">Bayesian</a>, <a href="https://cran.r-project.org/view=Environmetrics">Environmetrics</a>, <a href="https://cran.r-project.org/view=HighPerformanceComputing">HighPerformanceComputing</a>, <a href="https://cran.r-project.org/view=MachineLearning">MachineLearning</a>, <a href="https://cran.r-project.org/view=Phylogenetics">Phylogenetics</a>, <a href="https://cran.r-project.org/view=Spatial">Spatial</a>, <a href="https://cran.r-project.org/view=Survival">Survival</a>, <a href="https://cran.r-project.org/view=TeachingStatistics">TeachingStatistics</a></p>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-fairnessjl">
<p><span class="smallcaps">A. Agrawal, J. Chen, S. Vollmer and A. Blaom</span>. Fairness.jl. 2020a. URL <a href="https://github.com/ashryaagr/Fairness.jl">https://github.com/ashryaagr/Fairness.jl</a>. </p>
</div>
<div id="ref-agrawal2020debiasing">
<p><span class="smallcaps">A. Agrawal, F. Pfisterer, B. Bischl, J. Chen, S. Sood, S. Shah, F. Buet-Golfouse, B. A. Mateen and S. Vollmer</span>. Debiasing classifiers: Is reality at variance with expectation? <em>arXiv:2011.02407</em>, 2020b. </p>
</div>
<div id="ref-fairwashing">
<p><span class="smallcaps">U. Aivodji, H. Arai, O. Fortineau, S. Gambs, S. Hara and A. Tapp</span>. Fairwashing: The risk of rationalization. In <em>International conference on machine learning</em>, pages. 161–170 2019. </p>
</div>
<div id="ref-compas">
<p><span class="smallcaps">J. Angwin, J. Larson, S. Mattu and L. Kichner</span>. Machine bias. 2016. URL <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</a>. </p>
</div>
<div id="ref-bao2021s">
<p><span class="smallcaps">M. Bao, A. Zhou, S. A. Zottola, B. Brubach, S. Desmarais, A. S. Horowitz, K. Lum and S. Venkatasubramanian</span>. It’s compaslicated: The messy relationship between rai datasets and algorithmic fairness benchmarks. In <em>Thirty-fifth conference on neural information processing systems datasets and benchmarks track (round 1)</em>, 2021. </p>
</div>
<div id="ref-fairmlbook">
<p><span class="smallcaps">S. Barocas, M. Hardt and A. Narayanan</span>. <em>Fairness and machine learning.</em> fairmlbook.org, 2019. URL <a href="https://fairmlbook.org/">https://fairmlbook.org/</a>. </p>
</div>
<div id="ref-aif360">
<p><span class="smallcaps">R. K. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilović, et al.</span> AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. <em>IBM Journal of Research and Development</em>, 63(4/5): 4–1, 2019. URL <a href="https://aif360.mybluemix.net/">https://aif360.mybluemix.net/</a>. </p>
</div>
<div id="ref-future">
<p><span class="smallcaps">H. Bengtsson</span>. A unifying framework for parallel and distributed processing in r using futures. <em>The R Journal</em>, 13(2): 208–227, 2021. URL <a href="https://doi.org/10.32614/RJ-2021-048">https://doi.org/10.32614/RJ-2021-048</a>. </p>
</div>
<div id="ref-richardcompas">
<p><span class="smallcaps">R. Berk, H. Heidari, S. Jabbari, M. Kearns and A. Roth</span>. Fairness in Criminal Justice Risk Assessments: The State of the Art. <em>Sociological Methods &amp; Research</em>, 2018. DOI <a href="https://doi.org/10.1177/0049124118782533">10.1177/0049124118782533</a>. </p>
</div>
<div id="ref-mlr3book">
<p><span class="smallcaps">B. Bernd, S. Raphael, K. Lars and L. Michel, eds</span>. <em>Flexible and robust machine learning using mlr3 in r.</em> 2023. URL <a href="https://mlr3book.mlr-org.com">https://mlr3book.mlr-org.com</a>. </p>
</div>
<div id="ref-dalex">
<p><span class="smallcaps">P. Biecek</span>. DALEX: Explainers for Complex Predictive Models in R. <em>Journal of Machine Learning Research</em>, 19(84): 1–5, 2018. URL <a href="https://jmlr.org/papers/v19/18-416.html">https://jmlr.org/papers/v19/18-416.html</a>. </p>
</div>
<div id="ref-mlr3pipelines">
<p><span class="smallcaps">M. Binder, F. Pfisterer, M. Lang, L. Schneider, L. Kotthoff and B. Bischl</span>. mlr3pipelines - Flexible Machine Learning Pipelines in R. <em>Journal of Machine Learning Research</em>, 22(184): 1–7, 2021. URL <a href="https://jmlr.org/papers/v22/21-0281.html">https://jmlr.org/papers/v22/21-0281.html</a>. </p>
</div>
<div id="ref-binns2020apparent">
<p><span class="smallcaps">R. Binns</span>. On the apparent conflict between individual and group fairness. In <em>Proceedings of the conference on fairness, accountability, and transparency</em>, pages. 514–524 2020. New York, NY, USA: Association for Computing Machinery. DOI <a href="https://doi.org/10.1145/3351095.3372864">10.1145/3351095.3372864</a>. </p>
</div>
<div id="ref-fairlearn">
<p><span class="smallcaps">S. Bird, M. Dudík, R. Edgar, B. Horn, R. Lutz, V. Milan, M. Sameki, H. Wallach and K. Walker</span>. Fairlearn: A toolkit for assessing and improving fairness in AI. MSR-TR-2020-32. Microsoft. 2020. URL <a href="https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/">https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/</a>. </p>
</div>
<div id="ref-mlr">
<p><span class="smallcaps">B. Bischl, M. Lang, L. Kotthoff, J. Schiffner, J. Richter, E. Studerus, G. Casalicchio and Z. M. Jones</span>. Mlr: Machine learning in R. <em>Journal of Machine Learning Research</em>, 17(170): 1–5, 2016. URL <a href="https://jmlr.org/papers/v17/15-066.html">https://jmlr.org/papers/v17/15-066.html</a>. </p>
</div>
<div id="ref-bischl2012resampling">
<p><span class="smallcaps">B. Bischl, O. Mersmann, H. Trautmann and C. Weihs</span>. Resampling methods for meta-model validation with recommendations for evolutionary computation. <em>Evolutionary Computation</em>, 20(2): 249–275, 2012. </p>
</div>
<div id="ref-gendershades">
<p><span class="smallcaps">J. Buolamwini and T. Gebru</span>. Gender shades: Intersectional accuracy disparities in commercial gender classification. In <em>Proceedings of the conference on fairness, accountability, and transparency</em>, pages. 77–91 2018. PMLR. </p>
</div>
<div id="ref-Calders2010">
<p><span class="smallcaps">T. Calders and S. Verwer</span>. Three naive Bayes approaches for discrimination-free classification. <em>Data Mining and Knowledge Discovery</em>, 21(2): 277–292, 2010. DOI <a href="https://doi.org/10.1007/s10618-010-0190-x">10.1007/s10618-010-0190-x</a>. </p>
</div>
<div id="ref-Chen2018">
<p><span class="smallcaps">J. Chen</span>. Fair lending needs explainable models for responsible recommendation. In <em>Proceedings of the 2nd fatrec workshop on responsible recommendation</em>, 2018. </p>
</div>
<div id="ref-chouldechova2017fair">
<p><span class="smallcaps">A. Chouldechova</span>. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. <em>Big Data</em>, 5(2): 153–163, 2017. DOI <a href="https://doi.org/10.1089/big.2016.0047">10.1089/big.2016.0047</a>. </p>
</div>
<div id="ref-cirillo2020sex">
<p><span class="smallcaps">D. Cirillo, S. Catuara-Solarz, C. Morey, E. Guney, L. Subirats, S. Mellino, A. Gigante, A. Valencia, M. J. Rementeria, A. S. Chadha, et al.</span> Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare. <em>NPJ Digital Medicine</em>, 3(1): 1–11, 2020. DOI <a href="https://doi.org/10.1038/s41746-020-0288-5">10.1038/s41746-020-0288-5</a>. </p>
</div>
<div id="ref-corbett2018measure">
<p><span class="smallcaps">S. Corbett-Davies and S. Goel</span>. The measure and mismeasure of fairness: A critical review of fair machine learning. <em>arXiv:1808.00023</em>, 2018. </p>
</div>
<div id="ref-corbettcompas">
<p><span class="smallcaps">S. Corbett-Davies, E. Pierson, A. Feller, S. Goel and A. Huq</span>. Algorithmic decision making and the cost of fairness. In <em>Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining</em>, pages. 797–806 2017. New York, NY, USA: Association for Computing Machinery. DOI <a href="https://doi.org/10.1145/3097983.3098095">10.1145/3097983.3098095</a>. </p>
</div>
<div id="ref-dawes1989clinical">
<p><span class="smallcaps">R. M. Dawes, D. Faust and P. E. Meehl</span>. Clinical versus actuarial judgment. <em>Science</em>, 243(4899): 1668–1674, 1989. DOI <a href="https://doi.org/10.1126/science.2648573">10.1126/science.2648573</a>. </p>
</div>
<div id="ref-uci">
<p><span class="smallcaps">D. Dua and C. Graff</span>. UCI machine learning repository. 2017. URL <a href="http://archive.ics.uci.edu/ml">http://archive.ics.uci.edu/ml</a>. </p>
</div>
<div id="ref-dwork2012">
<p><span class="smallcaps">C. Dwork, M. Hardt, T. Pitassi, O. Reingold and R. Zemel</span>. Fairness through awareness. In <em>Proceedings of the 3rd innovations in theoretical computer science conference</em>, pages. 214–226 2012. </p>
</div>
<div id="ref-eubanks2018automating">
<p><span class="smallcaps">V. Eubanks</span>. <em>Automating inequality: How high-tech tools profile, police, and punish the poor.</em> St. Martin’s Press, 2018. </p>
</div>
<div id="ref-feurer23">
<p><span class="smallcaps">M. Feurer, K. Eggensperger, E. Bergman, F. Pfisterer, B. Bischl and F. Hutter</span>. Mind the gap: Measuring generalization performance across multiple objectives. In <em>Advances in intelligent data analysis xxi</em>, pages. 130–142 2023. Springer Nature Switzerland. </p>
</div>
<div id="ref-friedler16">
<p><span class="smallcaps">S. A. Friedler, C. Scheidegger and S. Venkatasubramanian</span>. On the (im)possibility of fairness. <em>arXiv:1609.07236</em>, 2016. </p>
</div>
<div id="ref-galindo2000credit">
<p><span class="smallcaps">J. Galindo and P. Tamayo</span>. Credit risk assessment using statistical and machine learning: Basic methodology and risk modeling applications. <em>Computational Economics</em>, 15(1): 107–143, 2000. </p>
</div>
<div id="ref-datasheets">
<p><span class="smallcaps">T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. Iii and K. Crawford</span>. Datasheets for datasets. <em>Communications of the ACM</em>, 64(12): 86–92, 2021. </p>
</div>
<div id="ref-hardt2016equality">
<p><span class="smallcaps">M. Hardt, E. Price and N. Srebro</span>. Equality of opportunity in supervised learning. <em>Advances in Neural Information Processing Systems</em>, 29: 3315–3323, 2016. </p>
</div>
<div id="ref-kamiran2012data">
<p><span class="smallcaps">F. Kamiran and T. Calders</span>. Data preprocessing techniques for classification without discrimination. <em>Knowledge and Information Systems</em>, 33(1): 1–33, 2012. </p>
</div>
<div id="ref-kilbertus2017avoiding">
<p><span class="smallcaps">N. Kilbertus, M. Rojas Carulla, G. Parascandolo, M. Hardt, D. Janzing and B. Schölkopf</span>. Avoiding discrimination through causal reasoning. <em>Advances in Neural Information Processing Systems</em>, 30: 2017. </p>
</div>
<div id="ref-kim2020fact">
<p><span class="smallcaps">J. S. Kim, J. Chen and A. Talwalkar</span>. Fact: A diagnostic for group fairness trade-offs. In <em>International conference on machine learning</em>, pages. 5264–5274 2020. PMLR. </p>
</div>
<div id="ref-komiyama">
<p><span class="smallcaps">J. Komiyama, A. Takeda, J. Honda and H. Shimao</span>. Nonconvex optimization for regression with fairness constraints. In <em>International conference on machine learning</em>, pages. 2737–2746 2018. </p>
</div>
<div id="ref-kozodoi2022fairness">
<p><span class="smallcaps">N. Kozodoi, J. Jacob and S. Lessmann</span>. Fairness in credit scoring: Assessment, implementation and profit implications. <em>European Journal of Operational Research</em>, 297(3): 1083–1094, 2022. DOI <a href="https://doi.org/10.1016/j.ejor.2021.06.023">10.1016/j.ejor.2021.06.023</a>. </p>
</div>
<div id="ref-fairness">
<p><span class="smallcaps">N. Kozodoi and T. V. Varga</span>. <em>Fairness: Algorithmic fairness metrics.</em> 2021. URL <a href="https://CRAN.R-project.org/package=fairness">https://CRAN.R-project.org/package=fairness</a>. R package version 1.2.1. </p>
</div>
<div id="ref-caret">
<p><span class="smallcaps">M. Kuhn</span>. <em>Caret: Classification and regression training.</em> 2021. URL <a href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>. R package version 6.0-88. </p>
</div>
<div id="ref-tidymodels">
<p><span class="smallcaps">M. Kuhn and H. Wickham</span>. <em>Tidymodels: A collection of packages for modeling and machine learning using tidyverse principles.</em> 2020. URL <a href="https://www.tidymodels.org">https://www.tidymodels.org</a>. </p>
</div>
<div id="ref-mlr3">
<p><span class="smallcaps">M. Lang, M. Binder, J. Richter, P. Schratz, F. Pfisterer, S. Coors, Q. Au, G. Casalicchio, L. Kotthoff and B. Bischl</span>. mlr3: A modern object-oriented machine learning framework in R. <em>Journal of Open Source Software</em>, 2019. DOI <a href="https://doi.org/10.21105/joss.01903">10.21105/joss.01903</a>. </p>
</div>
<div id="ref-mehrabi">
<p><span class="smallcaps">N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman and A. Galstyan</span>. A survey on bias and fairness in machine learning. <em>ACM Computing Surveys (CSUR)</em>, 54(6): 1–35, 2021. </p>
</div>
<div id="ref-modelcards">
<p><span class="smallcaps">M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji and T. Gebru</span>. Model cards for model reporting. In <em>Proceedings of the conference on fairness, accountability, and transparency</em>, pages. 220–229 2019. New York, NY, USA: PMLR; Association for Computing Machinery. DOI <a href="https://doi.org/10.1145/3287560.3287596">10.1145/3287560.3287596</a>. </p>
</div>
<div id="ref-mitchell2021algorithmic">
<p><span class="smallcaps">S. Mitchell, E. Potash, S. Barocas, A. D’Amour and K. Lum</span>. Algorithmic fairness: Choices, assumptions, and definitions. <em>Annual Review of Statistics and Its Application</em>, 8: 141–163, 2021. DOI <a href="https://doi.org/10.1146/annurev-statistics-042720-125902">10.1146/annurev-statistics-042720-125902</a>. </p>
</div>
<div id="ref-iml">
<p><span class="smallcaps">C. Molnar, B. Bischl and G. Casalicchio</span>. Iml: An r package for interpretable machine learning. <em>JOSS</em>, 3(26): 786, 2018. URL <a href="https://joss.theoj.org/papers/10.21105/joss.00786">https://joss.theoj.org/papers/10.21105/joss.00786</a>. </p>
</div>
<div id="ref-noble2018algorithms">
<p><span class="smallcaps">S. U. Noble</span>. <em>Algorithms of oppression.</em> New York University Press, 2018. </p>
</div>
<div id="ref-o2016weapons">
<p><span class="smallcaps">C. O’neil</span>. <em>Weapons of math destruction: How big data increases inequality and threatens democracy.</em> Crown, 2016. </p>
</div>
<div id="ref-sklearn">
<p><span class="smallcaps">F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al.</span> Scikit-learn: Machine learning in python. <em>Journal of Machine Learning Research</em>, 12: 2825–2830, 2011. </p>
</div>
<div id="ref-perrone2021fair">
<p><span class="smallcaps">V. Perrone, M. Donini, M. B. Zafar, R. Schmucker, K. Kenthapadi and C. Archambeau</span>. Fair Bayesian Optimization. In <em>Proceedings of the 2021 aaai/acm conference on ai, ethics, and society</em>, pages. 854–863 2021. </p>
</div>
<div id="ref-mcboost">
<p><span class="smallcaps">F. Pfisterer, C. Kern, S. Dandl, M. Sun, M. P. Kim and B. Bischl</span>. Mcboost: Multi-calibration boosting for R. <em>Journal of Open Source Software</em>, 6(64): 3453, 2021. URL <a href="https://joss.theoj.org/papers/10.21105/joss.03453">https://joss.theoj.org/papers/10.21105/joss.03453</a>. </p>
</div>
<div id="ref-superlearner">
<p><span class="smallcaps">E. Polley, E. LeDell, C. Kennedy and M. van der Laan</span>. <em>SuperLearner: Super learner prediction.</em> 2021. URL <a href="https://CRAN.R-project.org/package=SuperLearner">https://CRAN.R-project.org/package=SuperLearner</a>. R package version 2.0-28. </p>
</div>
<div id="ref-saleiro2018aequitas">
<p><span class="smallcaps">P. Saleiro, B. Kuester, L. Hinkson, J. London, A. Stevens, A. Anisfeld, K. T. Rodolfa and R. Ghani</span>. Aequitas: A bias and fairness audit toolkit. <em>arXiv:1811.05577</em>, 2018. </p>
</div>
<div id="ref-schumann">
<p><span class="smallcaps">C. Schumann, J. S. Foster, N. Mattei and J. P. Dickerson</span>. We need fairness and explainability in algorithmic hiring. In <em>Proceedings of the 19th international conference on autonomous agents and multiagent systems</em>, pages. 1716–1720 2020. Auckland, New Zealand: International Foundation for Autonomous Agents; Multiagent Systems. </p>
</div>
<div id="ref-schwobel-facct22a">
<p><span class="smallcaps">P. Schwöbel and P. Remmers</span>. The long arc of fairness: Formalisations and ethical discourse. In <em>Proceedings of the conference on fairness, accountability, and transparency</em>, pages. 2179–2188 2022. New York, NY, USA: Association for Computing Machinery. DOI <a href="https://doi.org/10.1145/3531146.3534635">10.1145/3531146.3534635</a>. </p>
</div>
<div id="ref-scutari">
<p><span class="smallcaps">M. Scutari, F. Panero and M. Proissl</span>. Achieving fairness with a simple ridge penalty. <em>arXiv:2105.13817</em>, 2021. </p>
</div>
<div id="ref-mlr3proba">
<p><span class="smallcaps">R. Sonabend, F. J. Király, A. Bender, B. Bischl and M. Lang</span>. mlr3proba: An R Package for Machine Learning in Survival Analysis. <em>Bioinformatics</em>, 2021. DOI <a href="https://doi.org/10.1093/bioinformatics/btab039">10.1093/bioinformatics/btab039</a>. </p>
</div>
<div id="ref-Topol2019">
<p><span class="smallcaps">E. J. Topol</span>. High-performance medicine: The convergence of human and artificial intelligence. <em>Nature Medicine</em>, 25(1): 44–56, 2019. DOI <a href="https://doi.org/10.1038/s41591-018-0300-7">10.1038/s41591-018-0300-7</a>. </p>
</div>
<div id="ref-Turner2019">
<p><span class="smallcaps">M. Turner and M. McBurnett</span>. Predictive models with explanatory concepts: A general framework for explaining machine learning credit risk models that simultaneously increases predictive power. In <em>Proceedings of the 15th credit scoring and credit control conference</em>, 2019. URL <a href="https://crc.business-school.ed.ac.uk/wp-content/uploads/sites/55/2019/07/C12-Predictive-Models-with-Explanatory-Concepts-McBurnett.pdf">https://crc.business-school.ed.ac.uk/wp-content/uploads/sites/55/2019/07/C12-Predictive-Models-with-Explanatory-Concepts-McBurnett.pdf</a>. </p>
</div>
<div id="ref-Vanschoren2014">
<p><span class="smallcaps">J. Vanschoren, J. N. van Rijn, B. Bischl and L. Torgo</span>. OpenML. <em>ACM SIGKDD Explorations Newsletter</em>, 15(2): 49–60, 2014. DOI <a href="https://doi.org/10.1145/2641190.2641198">10.1145/2641190.2641198</a>. </p>
</div>
<div id="ref-wachter-vlr2020">
<p><span class="smallcaps">S. Wachter, B. Mittelstadt and C. Russell</span>. Bias preservation in machine learning: The legality of fairness metrics under EU non-discrimination law. <em>West Virginia Law Review</em>, 123: 2020. </p>
</div>
<div id="ref-chen22">
<p><span class="smallcaps">E. A. Watkins, M. McKenna and J. Chen</span>. The four-fifths rule is not disparate impact: A woeful tale of epistemic trespassing in algorithmic fairness. <em>arXiv:2202.09519</em>, 2022. </p>
</div>
<div id="ref-ggplot2">
<p><span class="smallcaps">H. Wickham</span>. <em>Ggplot2: Elegant graphics for data analysis.</em> Springer-Verlag New York, 2016. URL <a href="https://ggplot2.tidyverse.org">https://ggplot2.tidyverse.org</a>. </p>
</div>
<div id="ref-fairmodels">
<p><span class="smallcaps">J. Wiśniewski and P. Biecek</span>. Fairmodels: A flexible tool for bias detection, visualization, and mitigation in binary classification models. <em>The R Journal</em>, 14(1): 227–243, 2022. URL <a href="https://rj.urbanek.nz/articles/RJ-2022-019/">https://rj.urbanek.nz/articles/RJ-2022-019/</a>. </p>
</div>
<div id="ref-rmarkdown">
<p><span class="smallcaps">Y. Xie, C. Dervieux and E. Riederer</span>. <em>R markdown cookbook.</em> Boca Raton, Florida: Chapman; Hall/CRC, 2020. URL <a href="https://bookdown.org/yihui/rmarkdown-cookbook">https://bookdown.org/yihui/rmarkdown-cookbook</a>. </p>
</div>
<div id="ref-Zafar2017">
<p><span class="smallcaps">M. B. Zafar, I. Valera, M. Gomez Rodriguez and K. P. Gummadi</span>. Fairness beyond Disparate treatment &amp; Disparate Impact. In <em>Proceedings of the 26th international conference on world wide web</em>, pages. 1171–1180 2017. Geneva, Switzerland: International World Wide Web Conferences Steering Committee. DOI <a href="https://doi.org/10.1145/3038912.3052660">10.1145/3038912.3052660</a>. </p>
</div>
</div>
<!--radix_placeholder_article_footer-->
<!--/radix_placeholder_article_footer-->
</div>

<div class="d-appendix">
</div>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<!--radix_placeholder_site_after_body-->
<!--/radix_placeholder_site_after_body-->
<!--radix_placeholder_appendices-->
<div class="appendix-bottom">
<h3 id="references">References</h3>
<div id="references-listing"></div>
<h3 id="reuse">Reuse</h3>
<p>Text and figures are licensed under Creative Commons Attribution <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>. The figures that have been reused from other sources don't fall under this license and can be recognized by a note in their caption: "Figure from ...".</p>
<h3 id="citation">Citation</h3>
<p>For attribution, please cite this work as</p>
<pre class="citation-appendix short">Pfisterer, et al., "Fairness Audits and Debiasing Using mlr3fairness", The R Journal, 2022</pre>
<p>BibTeX citation</p>
<pre class="citation-appendix long">@article{mlr3fairness,
  author = {Pfisterer, Florian and Wei, Siyi and Vollmer, Sebastian and Lang, Michel and Bischl, Bernd},
  title = {Fairness Audits and Debiasing Using mlr3fairness},
  journal = {The R Journal},
  year = {2022},
  issn = {2073-4859},
  pages = {1}
}</pre>
</div>
<!--/radix_placeholder_appendices-->
<!--radix_placeholder_navigation_after_body-->
<!--/radix_placeholder_navigation_after_body-->

</body>

</html>
