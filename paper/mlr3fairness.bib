@Manual{R,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2021},
  note = {{ISBN} 3-900051-07-0},
  url = {http://www.R-project.org/},
}

@Article{mlr3,
    title = {{mlr3}: A modern object-oriented machine learning framework in {R}},
    author = {Michel Lang and Martin Binder and Jakob Richter and Patrick Schratz and Florian Pfisterer and Stefan Coors and Quay Au and Giuseppe Casalicchio
    and Lars Kotthoff and Bernd Bischl},
    journal = {Journal of Open Source Software},
    year = {2019},
    month = {dec},
    doi = {10.21105/joss.01903},
    url = {https://joss.theoj.org/papers/10.21105/joss.01903},
}

@Article{mlr3pipelines,
    title = {{mlr3pipelines - Flexible Machine Learning Pipelines in R}},
    author = {Martin Binder and Florian Pfisterer and Michel Lang and Lennart Schneider and Lars Kotthoff and Bernd Bischl},
    journal = {Journal of Machine Learning Research},
    year = {2021},
    volume = {22},
    number = {184},
    pages = {1-7},
    url = {https://jmlr.org/papers/v22/21-0281.html},
}

@Article{mlr3proba,
    title = {{mlr3proba: An R Package for Machine Learning in Survival Analysis}},
    author = {Raphael Sonabend and Franz J Király and Andreas Bender and Bernd Bischl and Michel Lang},
    journal = {Bioinformatics},
    month = {02},
    year = {2021},
    doi = {10.1093/bioinformatics/btab039},
    issn = {1367-4803},
}

@article{Vanschoren2014,
  doi = {10.1145/2641190.2641198},
  url = {https://doi.org/10.1145/2641190.2641198},
  year = {2014},
  month = jun,
  publisher = {Association for Computing Machinery ({ACM})},
  volume = {15},
  number = {2},
  pages = {49--60},
  author = {Joaquin Vanschoren and Jan N. van Rijn and Bernd Bischl and Luis Torgo},
  title = {{OpenML}},
  journal = {{ACM} {SIGKDD} Explorations Newsletter}
}

@article{galindo2000credit,
  title={Credit risk assessment using statistical and machine learning: basic methodology and risk modeling applications},
  author={Galindo, Jorge and Tamayo, Pablo},
  journal={Computational Economics},
  volume={15},
  number={1},
  pages={107--143},
  year={2000},
  publisher={Springer}
}


@Manual{caret,
title = {caret: Classification and Regression Training},
author = {Max Kuhn},
year = {2021},
note = {R package version 6.0-88},
url = {https://CRAN.R-project.org/package=caret},
}

@Manual{superlearner,
title = {SuperLearner: Super Learner Prediction},
author = {Eric Polley and Erin LeDell and Chris Kennedy and Mark {van der Laan}},
year = {2021},
note = {R package version 2.0-28},
url = {https://CRAN.R-project.org/package=SuperLearner},
}

@Manual{tidymodels,
title = {Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles.},
author = {Max Kuhn and Hadley Wickham},
url = {https://www.tidymodels.org},
year = {2020},
}

  @article{mcboost,
    author = {Pfisterer, Florian and Kern, Christoph and Dandl, Susanne and Sun, Matthew and
    Kim, Michael P. and Bischl, Bernd},
    title = {mcboost: Multi-Calibration Boosting for R},
    journal = {Journal of Open Source Software},
    doi = {10.21105/joss.03453},
    url = {https://doi.org/10.21105/joss.03453},
    year = {2021},
    publisher = {The Open Journal},
    volume = {6},
    number = {64},
    pages = {3453}
  }

  # Multi-Calibration
  @inproceedings{hebert-johnson2018,
    title = {Multicalibration: Calibration for the ({C}omputationally-Identifiable) Masses},
    author = {Hebert-Johnson, Ursula and Kim, Michael P. and Reingold, Omer and Rothblum, Guy},
    booktitle = {Proceedings of the 35th International Conference on Machine Learning},
    pages = {1939--1948},
    year = {2018},
    editor = {Jennifer Dy and Andreas Krause},
    volume = {80},
    series = {Proceedings of Machine Learning Research},
    address = {Stockholmsmässan, Stockholm Sweden},
    publisher = {PMLR}
  }
  # Multi-Accuracy
  @inproceedings{kim2019,
    author = {Kim, Michael P. and Ghorbani, Amirata and Zou, James},
    title = {Multiaccuracy: Black-Box Post-Processing for Fairness in Classification},
    year = {2019},
    isbn = {9781450363242},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3306618.3314287},
    doi = {10.1145/3306618.3314287},
    booktitle = {Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society},
    pages = {247--254},
    location = {Honolulu, HI, USA},
    series = {AIES '19}
  }

@misc{uci ,
    author = "Dua, Dheeru and Graff, Casey",
    year = "2017",
    title = "{UCI} Machine Learning Repository",
    url = "http://archive.ics.uci.edu/ml",
    institution = "University of California, Irvine, School of Information and Computer Sciences"
}


# Fairness
@book{fairmlbook,
  title = {Fairness and Machine Learning},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org}},
  year = {2019}
}

@article{dawes1989clinical,
  title={Clinical versus actuarial judgment},
  author={Dawes, Robyn M and Faust, David and Meehl, Paul E},
  journal={Science},
  volume={243},
  number={4899},
  pages={1668--1674},
  year={1989},
  publisher={American Association for the Advancement of Science}
}

@article{barocas2016big,
  title={Big data's disparate impact},
  author={Barocas, Solon and Selbst, Andrew D},
  journal={Calif. L. Rev.},
  volume={104},
  pages={671},
  year={2016},
  publisher={HeinOnline}
}


@article{rodolfa2020machine,
  title={Machine learning for public policy: Do we need to sacrifice accuracy to make models fair?},
  author={Rodolfa, Kit T and Lamba, Hemank and Ghani, Rayid},
  journal={arXiv preprint arXiv:2012.02972},
  year={2020}
}

@misc{compas,
author = {Julia Angwin and Jeff Larson and Surya Mattu and Lauren Kichner},
publisher = {ProPublica},
url = {https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing},
year = 2016,
month = may,
day = 23,
title = {Machine Bias},
}

@inproceedings{corbettcompas,
author = {Corbett-Davies, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
title = {Algorithmic Decision Making and the Cost of Fairness},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098095},
doi = {10.1145/3097983.3098095},
abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {797–806},
numpages = {10},
keywords = {discrimination, pretrial detention, algorithmic fairness, risk assessment, disparate impact},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@article{richardcompas,
author = {Richard Berk and Hoda Heidari and Shahin Jabbari and Michael Kearns and Aaron Roth},
title ={Fairness in Criminal Justice Risk Assessments: The State of the Art},
journal = {Sociological Methods \& Research},
numpages = {42},
year = {2018},
month = aug,
eprint = {1703.09207},
archiveprefix = {arXiv},
doi = {10.1177/0049124118782533},
abstract = { Objectives:Discussions of fairness in criminal justice risk assessments typically lack conceptual precision. Rhetoric too often substitutes for careful analysis. In this article, we seek to clarify the trade-offs between different kinds of fairness and between fairness and accuracy.Methods:We draw on the existing literatures in criminology, computer science, and statistics to provide an integrated examination of fairness and accuracy in criminal justice risk assessments. We also provide an empirical illustration using data from arraignments.Results:We show that there are at least six kinds of fairness, some of which are incompatible with one another and with accuracy.Conclusions:Except in trivial cases, it is impossible to maximize accuracy and fairness at the same time and impossible simultaneously to satisfy all kinds of fairness. In practice, a major complication is different base rates across different legally protected groups. There is a need to consider challenging trade-offs. These lessons apply to applications well beyond criminology where assessments of risk can be used by decision makers. Examples include mortgage lending, employment, college admissions, child welfare, and medical diagnoses.}
}

@article{Topol2019,
abstract = {The use of artificial intelligence, and the deep-learning subtype in particular, has been enabled by the use of labeled big data, along with markedly enhanced computing power and cloud storage, across all sectors. In medicine, this is beginning to have an impact at three levels: for clinicians, predominantly via rapid, accurate image interpretation; for health systems, by improving workflow and the potential for reducing medical errors; and for patients, by enabling them to process their own data to promote health. The current limitations, including bias, privacy and security, and lack of transparency, along with the future directions of these applications will be discussed in this article. Over time, marked improvements in accuracy, productivity, and workflow will likely be actualized, but whether that will be used to improve the patient–doctor relationship or facilitate its erosion remains to be seen.},
author = {Topol, Eric J.},
doi = {10.1038/s41591-018-0300-7},
journal = {Nature Medicine},
number = {1},
pages = {44--56},
title = {High-performance medicine: the convergence of human and artificial intelligence},
volume = {25},
year = {2019}
}


@book{eubanks2018automating,
  title={Automating inequality: How high-tech tools profile, police, and punish the poor},
  author={Eubanks, Virginia},
  year={2018},
  publisher={St. Martin's Press}
}

@book{o2016weapons,
  title={Weapons of math destruction: How big data increases inequality and threatens democracy},
  author={O'neil, Cathy},
  year={2016},
  publisher={Crown}
}

@book{noble2018algorithms,
  title={Algorithms of oppression},
  author={Noble, Safiya Umoja},
  year={2018},
  publisher={New York University Press}
}

@inproceedings{Chen2019,
abstract = {Assessing the fairness of a decision making system with respect to a protected class, such as gender or race, is challenging when class membership labels are unavailable. Probabilistic models for predicting the protected class based on observable proxies, such as surname and geolocation for race, are sometimes used to impute these missing labels for compliance assessments. Empirically, these methods are observed to exaggerate disparities, but the reason why is unknown. In this paper, we decompose the biases in estimating outcome disparity via threshold-based imputation into multiple interpretable bias sources, allowing us to explain when over- or underestimation occurs. We also propose an alternative weighted estimator that uses soft classifcation, and show that its bias arises simply from the conditional covariance of the outcome with the true class membership. Finally, we illustrate our results with numerical simulations and a public dataset of mortgage applications, using geolocation as a proxy for race. We confrm that the bias of threshold-based imputation is generally upward, but its magnitude varies strongly with the threshold chosen. Our new weighted estimator tends to have a negative bias that is much simpler to analyze and reason about.},
archivePrefix = {arXiv},
author = {Chen, Jiahao and Kallus, Nathan and Mao, Xiaojie and Svacha, Geofry and Udell, Madeleine},
booktitle = {FAT* 2019 - Proceedings of the 2019 Conference on Fairness, Accountability, and Transparency},
doi = {10.1145/3287560.3287594},
eprint = {1811.11154},
pages = {339--348},
title = {Fairness under unawareness: Assessing disparity when protected class is unobserved},
year = {2019}
}

@inproceedings{Turner2019,
author = {Turner, Matthew and McBurnett, Michael},
booktitle = {Proceedings of the 15th Credit Scoring and Credit Control Conference},
title = {Predictive models with explanatory concepts: a general framework for explaining machine learning credit risk models that simultaneously increases predictive power},
url = {https://crc.business-school.ed.ac.uk/wp-content/uploads/sites/55/2019/07/C12-Predictive-Models-with-Explanatory-Concepts-McBurnett.pdf},
year = {2019},
numpages = 11
}

@inproceedings{schumann,
author = {Schumann, Candice and Foster, Jeffrey S. and Mattei, Nicholas and Dickerson, John P.},
title = {We Need Fairness and Explainability in Algorithmic Hiring},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Algorithms and machine learning models, including the decisions made by these models,
are becoming ubiquitous in our daily life, including hiring. We make no value judgment
regarding this development; rather, we simply acknowledge that it is quickly becoming
reality that automation plays a role in hiring. Increasingly, these technologies are
used in all of the small decisions that make up the modern hiring pipeline: from which
resumes get selected for a first screen to who gets an on site interview. Thus, these
algorithms and models may potentially amplify bias and (un)fairness issues for many
historically marginalized groups. While there is a rapidly expanding literature on
algorithmic decision making and fairness, there has been limited work on fairness
specifically for online, multi-stakeholder decision making processes such as those
found in hiring. We outline broad challenges including formulating definitions for
fair treatment and fair outcomes in hiring, and incorporating these definitions into
the algorithms and processes that constitute the modern hiring pipeline. We see the
AAMAS community as uniquely positioned to address these challenges.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1716–1720},
numpages = {5},
keywords = {blue sky, bandits, algorithmic fairness},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@article{khandani2010,
  title={Consumer credit-risk models via machine-learning algorithms},
  author={Khandani, Amir E and Kim, Adlar J and Lo, Andrew W},
  journal={Journal of Banking \& Finance},
  volume={34},
  number={11},
  pages={2767--2787},
  year={2010},
  publisher={Elsevier}
}

@inproceedings{buolamwini2018gender,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77--91},
  year={2018},
  organization={PMLR}
}

@article{mehrabi,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3315--3323},
  year={2016}
}

@article{saleiro2018aequitas,
  title={Aequitas: A bias and fairness audit toolkit},
  author={Saleiro, Pedro and Kuester, Benedict and Hinkson, Loren and London, Jesse and Stevens, Abby and Anisfeld, Ari and Rodolfa, Kit T and Ghani, Rayid},
  journal={arXiv preprint arXiv:1811.05577},
  year={2018}
}

@inproceedings{kim2020fact,
  title={Fact: A diagnostic for group fairness trade-offs},
  author={Kim, Joon Sik and Chen, Jiahao and Talwalkar, Ameet},
  booktitle={International Conference on Machine Learning},
  pages={5264--5274},
  year={2020},
  organization={PMLR}
}

@article{kamiran2012data,
  title={Data preprocessing techniques for classification without discrimination},
  author={Kamiran, Faisal and Calders, Toon},
  journal={Knowledge and Information Systems},
  volume={33},
  number={1},
  pages={1--33},
  year={2012},
  publisher={Springer}
}



## Software


 @Manual{fairness,
    title = {fairness: Algorithmic Fairness Metrics},
    author = {Nikita Kozodoi and Tibor {V. Varga}},
    year = {2021},
    note = {R package version 1.2.1},
    url = {https://CRAN.R-project.org/package=fairness},
  }

  @misc{aif360,
    title = "{AI Fairness} 360:  An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
    author = {Rachel K. E. Bellamy and Kuntal Dey and Michael Hind and
    Samuel C. Hoffman and Stephanie Houde and Kalapriya Kannan and
    Pranay Lohia and Jacquelyn Martino and Sameep Mehta and
    Aleksandra Mojsilovic and Seema Nagar and Karthikeyan Natesan Ramamurthy and
    John Richards and Diptikalyan Saha and Prasanna Sattigeri and
    Moninder Singh and Kush R. Varshney and Yunfeng Zhang},
    month = oct,
    year = {2018},
    url = {https://arxiv.org/abs/1810.01943}
}

@Article{fairmodels,
    title = {fairmodels: A Flexible Tool For Bias Detection},
    author = {Jakub Wiśniewski and Przemysław Biecek},
    year = {2021},
    journal = {arxiv},
    url = {https://arxiv.org/abs/2104.00507}
}

  @Article{DALEX,
    title = {DALEX: Explainers for Complex Predictive Models in R},
    author = {Przemyslaw Biecek},
    journal = {Journal of Machine Learning Research},
    year = {2018},
    volume = {19},
    pages = {1-5},
    number = {84},
    url = {https://jmlr.org/papers/v19/18-416.html},
  }


@book{molnar2019, title = {Interpretable Machine Learning},
author = {Christoph Molnar},
year = {2019},
subtitle = {A Guide for Making Black Box Models Explainable} }

@techreport{fairlearn,
    author = {Bird, Sarah and Dud{\'i}k, Miro and Edgar, Richard and Horn, Brandon and Lutz, Roman and Milan, Vanessa and Sameki, Mehrnoosh and Wallach, Hanna and Walker, Kathleen},
    title = {Fairlearn: A toolkit for assessing and improving fairness in {AI}},
    institution = {Microsoft},
    year = {2020},
    month = {May},
    url = "https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/",
    number = {MSR-TR-2020-32},
}

@article{sklearn,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}

@misc{fairnessjl,
  author       = {Ashrya Agrawal and Jiahao Chen and Sebastian Vollmer and Anthony Blaom},
  title        = {Fairness.jl},
  year         = {2020},
  publisher    = {Zenodo},
  version      = {v0.1.2},
  doi          = {10.5281/zenodo.3977197}
}


@misc{pfisterer2019multiobjective,
    title={Multi-Objective Automatic Machine Learning with AutoxgboostMC},
    author={Florian Pfisterer and Stefan Coors and Janek Thomas and Bernd Bischl},
    year={2019},
    eprint={1908.10796},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@inproceedings{feldman2015certifying,
  title={Certifying and removing disparate impact},
  author={Feldman, Michael and Friedler, Sorelle A and Moeller, John and Scheidegger, Carlos and Venkatasubramanian, Suresh},
  booktitle={proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={259--268},
  year={2015}
}

@article{chouldechova2017fair,
  archivePrefix = {arXiv},
  author = {Chouldechova, Alexandra},
  doi = {10.1089/big.2016.0047},
  eprint = {1703.00056},
  journal = {Big Data},
  month = jun,
  number = {2},
  pages = {153--163},
  title = {Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments},
  volume = {5},
  year = {2017}
}

@article{mlr,
    title = {mlr: Machine Learning in R},
    author = {Bernd Bischl and Michel Lang and Lars Kotthoff and Julia Schiffner and Jakob Richter and Erich Studerus
      and Giuseppe Casalicchio and Zachary M. Jones},
    journal = {Journal of Machine Learning Research},
    year = {2016},
    volume = {17},
    number = {170},
    pages = {1-5},
    url = {https://jmlr.org/papers/v17/15-066.html}
}

@inproceedings{Chen2018,
  archivePrefix = {arXiv},
  author = {Chen, Jiahao},
  eprint = {1809.04684},
  booktitle = {Proceedings of the 2nd FATREC Workshop on Responsible Recommendation},
  month = sep,
  numpages = {4},
  title = {Fair lending needs explainable models for responsible recommendation},
  year = {2018}
}

 @Book{rmarkdown,
    title = {R Markdown Cookbook},
    author = {Yihui Xie and Christophe Dervieux and Emily Riederer},
    publisher = {Chapman and Hall/CRC},
    address = {Boca Raton, Florida},
    year = {2020},
    note = {ISBN 9780367563837},
    url = {https://bookdown.org/yihui/rmarkdown-cookbook},
  }



@inproceedings{modelcards,
author = {Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
title = {Model Cards for Model Reporting},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287596},
doi = {10.1145/3287560.3287596},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {220–229},
numpages = {10},
keywords = {ethical considerations, fairness evaluation, ML model evaluation, disaggregated evaluation, documentation, model cards, datasheets},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@article{datasheets,
  title={Datasheets for datasets},
  author={Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daum{\'e} III, Hal and Crawford, Kate},
  journal={arXiv preprint arXiv:1803.09010},
  year={2018}
}

@inproceedings{gendershades,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77--91},
  year={2018},
  organization={PMLR}
}

@article{Calders2010,
abstract = {In this paper, we investigate how to modify the naive Bayes classifier in order to perform classification that is restricted to be independent with respect to a given sensitive attribute. Such independency restrictions occur naturally when the decision process leading to the labels in the data-set was biased; e.g., due to gender or racial discrimination. This setting is motivated by many cases in which there exist laws that disallow a decision that is partly based on discrimination. Naive application of machine learning techniques would result in huge fines for companies. We present three approaches for making the naive Bayes classifier discrimination-free: (i) modifying the probability of the decision being positive, (ii) training one model for every sensitive attribute value and balancing them, and (iii) adding a latent variable to the Bayesian model that represents the unbiased label and optimizing the model parameters for likelihood using expectation maximization. We present experiments for the three approaches on both artificial and real-life data. {\textcopyright} The Author(s) 2010.},
author = {Calders, Toon and Verwer, Sicco},
doi = {10.1007/s10618-010-0190-x},
journal = {Data Mining and Knowledge Discovery},
number = {2},
pages = {277--292},
title = {Three naive Bayes approaches for discrimination-free classification},
volume = {21},
year = {2010}
}

@inproceedings{perrone2021fair,
  title={Fair bayesian optimization},
  author={Perrone, Valerio and Donini, Michele and Zafar, Muhammad Bilal and Schmucker, Robin and Kenthapadi, Krishnaram and Archambeau, C{\'e}dric},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={854--863},
  year={2021}
}


@inproceedings{Zafar2017,
abstract = {As the use of automated decision making systems becomes wide-spread, there is a growing concern about their potential unfairness towards people with certain traits. Anti-discrimination laws in various countries prohibit unfair treatment of individuals based on specific traits, also called sensitive attributes (e.g., gender, race). In many learning scenarios, the trained algorithms (classifiers) make decisions with certain inaccuracy (misclassification rate). As learning mechanisms target minimizing the error rate for all decisions, it is quite possible that the optimally trained algorithm makes decisions for users belonging to different sensitive attribute groups with different error rates (e.g., decision errors for females are higher than for males). To account for and avoid such unfairness when learning, in this paper, we introduce a new notion of unfairness, disparate mistreatment, which is defined in terms of misclassification rates. We then propose an intuitive measure of disparate mistreatment for decision boundary-based classifiers, which can be easily incorporated into their formulation as a convex-concave constraint. Experiments on synthetic as well as real world datasets show that our methodology is effective at avoiding disparate mistreatment, often at a small cost in terms of accuracy.},
address = {Geneva, Switzerland},
author = {Zafar, Muhammad Bilal and Valera, Isabel and {Gomez Rodriguez}, Manuel and Gummadi, Krishna P.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web},
doi = {10.1145/3038912.3052660},
month = apr,
pages = {1171--1180},
publisher = {International World Wide Web Conferences Steering Committee},
title = {Fairness Beyond Disparate Treatment {\&} Disparate Impact},
year = {2017}
}