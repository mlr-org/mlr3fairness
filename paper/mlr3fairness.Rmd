---
title: Fairness Audits And Debiasing Using mlr3fairness
author:
  # see ?rjournal_article for more information
  - name: Florian Pfisterer
    affiliation: LMU Munich
    orcid: 0000-0001-8867-762X
    email:  author1@work
  - name: Michel Lang
    url: https://journal.r-project.org
    email: Michel.Lang@stat.uni-muenchen.de
    orcid: 0000-0001-9754-0393
    affiliation: LMU Munich
    affiliation2: TU Dortmund University
abstract: >
  Given an increase in data-driven automated decision-making based on machine learning models, it is
  imparative that along with tools to develop and improve such models there are sufficient capabilities
  to analyze and assess models with respect to potential biases. We present the package \CRANpkg{mlr3fairness},
  a collection of metrics and methods that allow for the assessment of bias in machine learning models.
  Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases along with a set of visualizations that can help to provide additional insights into such biases. \CRANpkg{mlr3fairness} furthermore integrates debiasing methods that can help aleviate biases in ML models through data preprocessing or post-processing of predictions.
preamble: |
  \usepackage{longtable}
  \usepackage{bbm}

# per R journal requirement, the bib filename should be the same as the output
# tex file. Don't forget to rename the bib file and change this example value.
bibliography: mlr3fairness.bib

output: rticles::rjournal_article
---

```{r setup, include = FALSE}
options(tinytex.verbose = TRUE)
set.seed(4444L)
library("mlr3misc")
library("ggplot2")
library("mlr3verse")
library("bbotk")
library("mlr3tuning")
library("patchwork")

lgr::get_logger("bbotk")$set_threshold("warn")
lgr::get_logger("mlr3")$set_threshold("warn")
```


# Introduction

Humans are increasingly subject to data-driven automated decision-making.
Those automated procedures such as credit risk assessments are often done using statistical or machine learning (ML) models [@galindo2000credit].
It is imperative that along with tools to develop and improve such models, we also develop sufficient capabilities to analyze and assess models with respect to their robustness and predictive performance, but also address potential biases.
While a plethora of performance metrics are readily implemented in statistical modeling frameworks in R [@R]  (e.g., \CRANpkg{caret} [@caret], \CRANpkg{tidymodels} [@tidymodels], \CRANpkg{SuperLearner} [@superlearner], or \CRANpkg{mlr} [@mlr]), fairness metrics are widely missing.
This lack of availability can be detrimental to obtaining fair and unbiased models if the result is to forgo bias audits due to the considerable complexity those metrics often pose.
Consequently, there exists considerable necessity for R packages (a) implementing such metrics and (b) connecting these metrics to existing ML frameworks.
If biases are detected in a model and need to be mitigated, we furthermore require debiasing techniques that tightly integrate with the fitting and evaluation of the resulting models in order to obtain trade-offs between the fairness and utility (e.g., predictive accuracy) of a model.

In this article, we present the \CRANpkg{mlr3fairness} package tightly integrated into the \CRANpkg{mlr3} [@mlr3] ecosystem of packages for ML in R.
It brings fairness metrics, visualizations, and model-agnostic pre- and postprocessing operators to the table, allowing
to conveniently perform fairness audits and build unbiased models.

<!--
TODO: move somewhere else?
The remainder of this manuscript is structured as follows:
We provide a brief introduction to the concepts of fairness in ML and discuss related software implementations.
We furthermore give a brief introduction to \CRANpkg{mlr3} before introducing the functionality shipped with our package.
We conclude with a short case study demonstrating \CRANpkg{mlr3fairness}'s capabilities on the **Adult Income** data set.
-->


## Fairness in Machine Learning

### Fairness Definitions

Studies have found that data-driven automated decision-making systems often improve over human expertise (c.f. @dawes1989clinical) and high stakes decisions can therefore be improved using data-driven systems.
This often does not only improve predictions but can also make decisions more efficient.
Such systems, often without human oversight, are now ubiquitous in everyday life (@o2016weapons, @eubanks2018automating, @noble2018algorithms).
To provide further examples, ML driven systems are now used for highly influential decisions such as loan accommodations (@Chen2018, @Turner2019), job applications [@schumann], healthcare [@Topol2019], and criminal sentencing (@compas, @corbettcompas, @richardcompas).
With this proliferation, such decisions have become subject to scrutiny as a result of prominent inadequacies or failures of such methods for example in the case of the COMPAS recidivism prediction system [@compas].

Without proper auditing, those models can unintentionally result in negative consequences for individuals, often from underprivileged groups [@fairmlbook].
Several sources of such biases are worth mentioning in this context: Data often contains **historical biases** such as gender or racial stereotypes, that -- if picked up by the model -- will be replicated into the future.
Similarly, unprivileged populations are often not represented in data due to **sampling biases** leading to models that perform well in groups sufficiently represented in the data but worse on others [@buolamwini2018gender].
Other biases include biases in the *label generating process* and **feedback** loops where repeated decisions affect the population subject to such decisions.
For an in-depth discussion and further sources of biases the interested reader is referred to @mehrabi.

We now turn to the question of how such biases can be measured in a model.
The answer to this question depends on a society's ethical values and whether we take a neutral or a normative position.
Depending on the answer to those questions, different metrics have been proposed.
We focus on a subgroup of those, so-called *statistical group fairness* metrics that are usually based on differences in some summary statistics between two groups.
The observations are grouped by a protected attribute $A$ ($A = 0$ vs.\ $A = 1$) which, e.g., is an identifier for a person's race or a person's gender.
While the discussed scenarios consider a *binary classification* scenario and a *binary protected attribute*, the concepts discussed in the following often extend naturally to other scenarios including multiclass-classification, regression, or survival analysis.
We now provide and discuss two examples from [@hardt2016equality] to provide further intuition about the purpose of such metrics.

#### Equalized Odds

A predictor $\hat{Y}$ satisfies *equalized odds* with respect to a protected attribute $A$ and observed outcome $Y$, if $\hat{Y}$ and $A$ are conditionally independent given $Y$:

\begin{equation} \label{eq:eod}
\mathbb{P}\left(\hat{Y} = 1 \mid A = 0, Y = y\right) = \mathbb{P}\left(\hat{Y} = 1 \mid A = 1, Y = y\right), \quad y \in \{0,1\}.
\end{equation}
In short, we require the same true positive rates (TPR) and false positive rates (FPR) across both groups $A = 0$ and $A = 1$.

#### Equality of Opportunity

A predictor $\hat{Y}$ satisfies *equality of opportunity* with respect to a protected attribute $A$ and observed outcome $Y$, if $\hat{Y}$ and $A$ are conditionally independent given $Y = 1$ (where $Y = 1$ is the favoured outcome). This is a relaxation of the aforementioned *equalized odds* essentially only requiring equal TPRs:

\begin{equation} \label{eq:eop}
\mathbb{P}\left(\hat{Y} = 1 \mid A = 0, Y = 1\right) = \mathbb{P}\left(\hat{Y} = 1 \mid A = 1, Y = 1\right).
\end{equation}


While both metrics are conceptually similar, they encode a different belief of what constitutes *fair* in a given scenario.
A discussion of different metrics and their applicability can be found in the Aequitas Fairness Toolkit [@saleiro2018aequitas].
In order to encode the requirements in Equations (\ref{eq:eod}) and (\ref{eq:eop}) into a fairness metric, we often encode differences between measured quantities in two groups, (denoting $\mathbb{P}\left(\hat{Y} = 1 \mid A = \star, Y = 1\right)$ with $\mathrm{TPR}_{A=\star}$):

\[
\Delta_{\mathrm{TPR}} = \mathrm{TPR}_{A=0} - \mathrm{TPR}_{A=1}.
\]

We now might consider a model fair if $|\Delta_{\mathrm{TPR}}| < \epsilon$ for a given threshold $\epsilon$, e.g., $\epsilon = 0.05$, to allow for small deviations from perfect fairness due to variance in the estimation of $\mathrm{TPR}_{A=\star}$ or additional sources of bias.
A collection of such metrics can be found e.g. in (@saleiro2018aequitas, @kim2020fact, @mehrabi).
The goal of **debiasing techniques** is now to reduce such gaps, either by using data pre-processing, model inprocessing or post-processing techniques on the predictions.
<!-- TODO: hier ein bisschen mehr -->
<!--
\CRANpkg{mlr3fairness} implements a wide variety of fairness metrics along with the possibility to implement and use custom fairness metrics based on this paradigm.
It furthermore implements debiasing techniques described in @kamiran2012data and @hardt2016equality to allow for obtaining fair(er) models.
-->


#### A short remark on mathematical fairness definitions

We want to briefly restate a remark similarly stated in @pfisterer2019multiobjective:
It is important to note that fairness can not be achieved solely through a reduction into mathematical criteria, e.g. statistical and individual notions of fairness such as disparate treatment or disparate impact [@feldman2015certifying].
Many problems with such metrics still persist and require additional research.
Furthermore, practitioners need not only take into account the model itself, but also the data used to train the algorithm and the process behind collecting and labeling of such data, including possible feedback loops emerging from potentially biased models.
While metrics and visualizations proposed in this article can help investigate biases in ML models, they do by no means guarantee that a model does not contain any undetected bias.
Consider, for example, fairness metrics based on the TPR from above.
If the labels $Y$ arise from a biased *label generating process*, there is little hope in obtaining fair outcomes even if fairness metrics indicate this.

## Related work {#related}

We now describe other software implemented in R or other programming languages and briefly describe their functionality.
Additionally, we further describe the \CRANpkg{mlr3} to provide an overview over the ecosystem \CRANpkg{mlr3fairness} is integrated in.

<!-- ## Bias Auditing and Debiasing Software {#othersoft} -->

Several R packages provide similar capabilities to our software, mostly focussing on fairness metrics and visualization.
The \CRANpkg{fairness} package [@fairness] allows for the calculation of a variety of fairness metrics, while \CRANpkg{aif360} [@aif360] wraps the Python \pkg{aif360} module allowing for the computation of fairness metrics and several debiasing techniques but has only limited interoparability with R objects such as \code{data.frame}s.
The \CRANpkg{fairmodels} package again allows for the computation of fairness metrics for classification and regression settings as well as several debiasing techniques.
It tightly integrates with \CRANpkg{DALEX} [@dalex] to gain further insight using interpretability techniques [@molnar2019].

Outside R, in Python, the \pkg{fairlearn} module [@fairlearn] provides ample functionality to study a wide variety of metrics, debias with respect to a variety of pre-, in- and postprocessing methods as well as to visualize differences.
It furthermore provides a *fairlearn dashboard* providing a comprehensive fairness report.
The \pkg{aif360} [@aif360]  module similarly provides metrics as well as debiasing techniques while the \pkg{aequitas} fairness toolkit [@saleiro2018aequitas] provides similar capabilities.
Interoperability with the \pkg{scikit-learn} [@sklearn] machine learning framework allow for debiasing a wide variety of machine learning models in all aforementioned systems.
Similar capabilities are also available in Julia's \pkg{Fairness.jl} [@fairnessjl] library.



# mlr3fairness {#main}

We first bla mlr3, then bla mlr3fairness ...

## The mlr3 ecosystem {#mlr3}

\CRANpkg{mlr3fairness} is tightly integrated into the ecosystem of packages around \CRANpkg{mlr3}.
Besides the infrastructure implemented in \CRANpkg{mlr3} to fit, resample, or evaluate over 100 ML algorithms using a unified API, extension packages bring numerous additional advantages.
In the context of fairness, the following extension packages deserve special mention:

* \CRANpkg{mlr3pipelines} [@mlr3pipelines] for pre- and postprocessing.
  \CRANpkg{mlr3fairness} implements pre- and postprocessing operators as pipeline operators for \CRANpkg{mlr3pipelines}.
  This allows merging debiasing with arbitrary ML algorithms implemented in \CRANpkg{mlr3} as well as comparison of different models through joint resampling and tuning.
  It furthermore integrates with \CRANpkg{mcboost} [@mcboost], which implements further debiasing methods.
* \CRANpkg{mlr3tuning} for its extensive tuning capabilities.
  Fusing debiasing techniques with ML algorithms as well as other often necessary
  preprocessing steps such as imputation of missing values or class balancing allows for joint tuning of hyperparameters with respect to
  arbitrary performance and fairness metrics.
* \CRANpkg{mlr3proba} [@mlr3proba] for survival analysis.
* \CRANpkg{mlr3benchmark} for post-hoc analysis of benchmarked approaches.
* \CRANpkg{mlr3oml} as a connector to OpenML [@Vanschoren2014], an online scientific platform for collaborative ML.

In order to provide the required understanding for \CRANpkg{mlr3}, we briefly introduce some terminology and syntax.
A full introduction can be found in the mlr3 book \footnote{https://mlr3book.mlr-org.com}.

A `Task` is the basic building block holding the data, storing covariates and the target variable along with some meta-information.
It provides a useful abstraction that allows for seamlessly switching between different data formats such as `data.table`s, sparse matrices or (remote) databases.
The shorthand constructor function `tsk()` can be used to quickly load example tasks shipped with \CRANpkg{mlr3} or \CRANpkg{mlr3fairness}.

```{r}
library("mlr3verse")
library("mlr3fairness")

# get the adult training data set
task = tsk("adult_train")
```

A `Learner` is a wrapper around an ML algorithm, e.g., around an implementation of a logistic regression or a decision tree.
It can be trained on a `Task` and used for obtaining a `Prediction` which can subsequently be scored using a `Measure` to get an estimate for the predictive performance on new data.
The shorthand constructors `lrn()` and `msr()` allow for the instantiation of implemented `Learner`s and `Measure`s, respectively.

```{r}
# get a classification tree from package rpart
learner = lrn("classif.rpart")

# performance measure to use - classification accuracy
measure = msr("classif.acc")

# split into training and test set
idx = partition(task)

# fit model on training set
learner$train(task, idx$train)

# predict on observations of test set
prediction = learner$predict(task, idx$test)

# score with classification accuracy
prediction$score(measure)
```

As the split into training set and test set is stochastic, the procedure should be repeated multiple times and the resulting performance values should be aggregated.
This process is called resampling, and can easily be performed with the `resample()` function, yielding a `ResampleResult` object:
```{r}
# use a 3-fold cross-validation strategy
resampling = rsmp("cv", folds = 3)

# perform resampling
rr = resample(task, learner, resampling)

# mean classification accuracy across 3 folds:
rr$aggregate(measure)
```

To properly compare competing modeling approaches, candidates can be benchmarked against each other using the `benchmark()` function.
```{r, warning = FALSE}
# try a logistic regression
learner2 = lrn("classif.log_reg")

# build an exhaustive grid design
grid = benchmark_grid(task, list(learner, learner2), resampling)

# perform benchmark
bmr = benchmark(grid)

# aggregate results
bmr$aggregate(measure)
```



We split the exposition of \CRANpkg{mlr3fairness}'s capabilities in two sections, *detecting bias* and *debiasing*.
A full example is included in the later use case and we therefore omit details discussed there in the following sections.


### Detecting bias

For brevity, we start our exposition from a fitted ML model, e.g. obtained through \CRANpkg{mlr3}.
The ML model is trained on the widely used COMPAS data set [@compas] with the goal to predict whether a parolee will re-offend within a span of 2 years.
The data set is included in \CRANpkg{mlr3fairness} and can be loaded with `tsk("compas")`.
For simplicity of exposition, we here use a simplified version of the data set with a binary protected attribute *race* ('Caucasian' / 'African American').
As an ML algorithm, we use a Decision Tree from \CRANpkg{rpart} trained on the first $4000$ observations in our data set.

```{r, echo = FALSE, message = FALSE}
# TODO: include in package: compas_small ?
library("mlr3")
library("mlr3fairness")

task = tsk("compas")
dt = task$data()
rows = which(dt$race %in% c("Caucasian", "African-American"))
task$filter(rows = rows)
task$select(
  c("age", "age_cat", "c_charge_degree", "days_b_screening_arrest",
    "decile_score", "length_of_stay", "priors_count", "score_text",
    "sex", "race"
  )
)
task$set_col_roles("race", add_to = "pta")
task$set_col_roles("sex", remove_from = "pta")
task$droplevels()

model = lrn("classif.rpart", predict_type = "prob", cp = 0.001, maxdepth = 12)
task
```

```{r}
idx = partition(task)
model$train(task$clone(), row_ids = idx$train)
prediction = model$predict(task$clone(), row_ids = idx$test)
```

We can now instantiate measures using the `msr()` function, where measure are prefixed by their purpose (e.g., fairness measures have the prefix *fairness*).
Simply calling `msr()` without any arguments will return a list of all measures.
In this case we compute the binary accuracy measure `"classif.acc"` and the equalized odds metric from above using `"fairness.eod"`.

```{r}
eod = msr("fairness.eod")
acc = msr("classif.acc")
prediction$score(list(acc, eod), task = task)
```

We can clearly see a comparatively large difference in equalized odds indicating that our model might be biased.
Looking at the individual components of the fairness this becomes clearer:


```{r}
fairness_tensor(prediction, task)
```

```{r}
p1 = fairness_prediction_density(prediction, task)
p2 = compare_metrics(prediction, msrs(c("fairness.fpr", "fairness.tpr", "fairness.eod")), task)
```

```{r, echo = FALSE, fig.height =3, fig.align='center'}
(p1 + xlab("") + p2) * theme_bw() + theme(axis.text.x = element_text(angle = 30, hjust = .7))
```

We provide a list of implemented fairness metrics below. Note, that new fairness metrics can be implemented in few lines of code based on performance metrics or by composition of existing fairness and performance metrics.

Table: (\#tab:metrics) List of implemented fairness metrics:

|key                   |description                                                                                 |
|:---------------------|:-------------------------------------------------------------------------------------------|
|fairness.acc          | accuracy equality \citet{gendershades}                                                     |
|fairness.fpr          | false positive rate equality/predictive equality \citet{chouldechova2017fair}              |
|fairness.tpr          | true positive rate equality /equality of opportunity \citet{hardt2016equality}             |
|fairness.fnr          | false negative / false omission rate equality \citet{richardcompas}                        |
|fairness.tnr          | true negative rate equality                                                                |
|fairness.eod          | equalized odds: fairness.fpr + fairness.tpr \citet{hardt2016equality}                      |
|fairness.ppv          | demographic parity, equalized positive rates \citet{Calders2010}                           |
|fairness.npv          | equalized negative rates                                                                   |
|fairness.acc\_eod=.05 |Accuracy under equalized odds constraint \citet{perrone2021fair}                            |
|fairness.acc\_ppv=.05 |Accuracy under equalized odds constraint \citet{perrone2021fair}                            |


### Debiasing

Table @ref{tab:debiasing} provides an overview over implemented debiasing techniques.
Debiasing techniques are implemented as `PipeOps` from \pkg{mlr3pipelines} and can be
combined with arbitrary learners to form a pipeline.

```{r, eval = FALSE}
# Automatically reweigh data before training a learner:
po("reweighing_wts") %>>% po("learner", lrn("classif.glmnet"))
# Post-process predictions for equalized odds.
po("learner_cv", lrn("classif.glmnet")) %>>% po("EOd")
```

Table: (\#tab:debiasing) Overview over available debiasing techniques.

|Key                | Description                      | Type              | Reference          |
|-------------------|----------------------------------|-------------------|--------------------|
|"EOd"              |Equalized-Odds Debiasing          |Postprocessing     | \citet{hardt2016equality} |
|"reweighing_os"    |Reweighing (Oversampling)         |Preprocessing      | \citet{kamiran2012data}   |
|"reweighing_wts"   |Reweighing (Instance Weights)     |Preprocessing      | \citet{kamiran2012data}   |

It is simple for users or package developers to extend \pkg{mlr3fairness} with additional
debiasing methods -- as an example, \pkg{mcboost} package adds further postprocessing methods
that can further improve fairness.

<!-- FIXME:
multi-crit tuning valid?
-->

###  Reports

Because fairness aspects can not always be investigated based on the fairness definitions
above (e.g. due to biased sampling or labelling procedures), it is important to document data collection and the resulting data as well as the models resulting from this data.
Informing auditors about those aspects of a deployed model can lead to better assessments of a model's fairness. Questionaires for ML models [@modelcards] and data sets [@datasheets] have been proposed in literature. We further add a fairness report that creates an automated report using large variety of fairness metrics and visualization. Functions generate a R Markdown file [@rmarkdown] that can and should be further customized by the user to
generate final report.


| Report             |  Description             | Reference                   |
|--------------------|--------------------------|-----------------------------|
| `report_modelcard` | Modelcard for ML models  | \citet{modelcards}          |
| `report_datasheet` | Datasheet for data sets  | \citet{datasheets}          |
| `report_fairness`  | Fairness Report          | --                          |


# Case Study - Adult Income {#usecase}

In order to demonstrate a full workflow, we conduct full bias assessment and debiasing on the popular adult data set [@uci].
The goal is to predict whether an individual's income is larger than \$$50.000$ with the protected attribute being *gender*.
The data set ships with \CRANpkg{mlr3fairness}, separated into a *train* and *test* task and can be instantiated using `tsk("adult_train")` and `tsk("adult_test")`,  respectively.
As a fairness metric we consider *predictive parity* [@chouldechova2017fair] which calls for equality in true positive rates between groups.
We furthermore are interested in the model's utility, e.g. its accuracy.

```{r}
library("mlr3verse")
library("mlr3fairness")

task = tsk("adult_train")
metrics = msrs(c("fairness.tpr", "classif.acc"))
print(task)
```


In order to get an initial perspective, we benchmark three models using 3-fold cross-validation:

* a classification tree from the \CRANpkg{rpart} package,
* a penalized logistic regression from the \CRANpkg{glmnet} package, and
* a penalized logistic regression from the \CRANpkg{glmnet} package, but with reweighs preprocessing.

The logistic regression in the latter two approaches do not support operating on factor features natively, therefore we pre-process the data with a feature encoder from \CRANpkg{mlr3pipelines}:

```{r, message = FALSE, warning=FALSE, error=FALSE}
learners = list(
    lrn("classif.rpart"),
    po("encode") %>>% lrn("classif.glmnet"),
    po("encode") %>>% po("reweighing_wts") %>>% po("learner", lrn("classif.glmnet"))
)
grid = benchmark_grid(
  tasks = tsks("adult_train"),
  learners = learners,
  resamplings = rsmp("cv", folds = 3)
)

bmr1 = benchmark(grid)
bmr1$aggregate(metrics)[, c(4, 7, 8)]
```

The preprocessing step of reweighing already improved the fairness while sacrificing only a tiny bit of performance. <!-- TODO CHECK with seed -->

To see if we can further improve, we use \CRANpkg{mlr3tuning} to jointly tune all hyperparameters of the *glmnet* model as well as our reweighing hyperparameter.
In order to do this, we use a `AutoTuner` from \CRANpkg{mlr3tuning}; a model that tunes its own hyperparameters during training.
The full code for setting up this model can be found in the appendix and is omitted here for brevity.
A `AutoTuner` requires a specific metric to tune for. Here, we define a fairness-thresholded accuracy metric. We set $\epsilon = 0.01$ as a threshold.

\[
  if \; \Delta_{EOd} \leq \epsilon: accuracy \;\; else: \; 1 + \Delta_{EOd}
\]

```{r}
metric = msr("fairness.constraint", msr("classif.acc"), msr("fairness.eod"))
```


```{r, echo = FALSE, message = FALSE}
library(mlr3misc)
library(mlr3)
library(mlr3pipelines)
library(mlr3fairness)
library(mlr3tuning)

# Enable paralellization over all cores
# future::plan("multisession")

lrn = as_learner(po("encode") %>>% po("reweighing_wts") %>>% po("learner", lrn("classif.glmnet")))
# Define the parameter space to optimize over
vals = list(
  reweighing_wts.alpha = to_tune(0.8, 1),
  classif.glmnet.alpha = to_tune(0.8, 1),
  classif.glmnet.s = to_tune(1e-4, 1e-2, logscale = TRUE)
)
# Add search space to the learner
lrn$param_set$values = insert_named(lrn$param_set$values, vals)
```

```{r}
at = AutoTuner$new(lrn, rsmp("holdout"),
    metric,
    tuner = mlr3tuning::tnr("random_search"),
    terminator = trm("evals", n_evals = 3)
)

grd = benchmark_grid(
  tasks = tsks("adult_train"),
  learners = list(at),
  resamplings = rsmp("cv", folds = 3)
)

bmr2 = benchmark(grd, store_models = TRUE)
bmr2$aggregate(metrics)[, c(4, 7, 8)]
```

The result improves w.r.t.\ accuracy while only slightly decreasing the measured fairness.
Note, that the generalization error is estimated using a holdout strategy during training
and slight violations of the desired threshold $\epsilon$ are therefore normal.
The results of both benchmark experiments can then be collected and jointly visualized.
In addition to aggregate scores, individual iterations of the 3 fold Cross-Validation are depicted
to visualize variations in the individual results.

```{r}
bmr = c(bmr1, bmr2)
fairness_accuracy_tradeoff(bmr, fairness_measure = metrics[[1]])
```
Especially when considering optimizing accuracy while still retaining a fair model,
tuning can be helpful and further improve


# Summary

The large-scale availability and use of automated decision making systems have resulted in growing concerns for a lack of fairness in the decisions made by such systems.
As a result, fairness metrics and debiasing methods that allow for investigating un-fairness and hopefully alleviating biases in existing systems have been proposed.
Widely available implementations of such methods are still not widely available, especially considering the required interoperability with machine learning toolkits that allows for ease of use and integration into model evaluation and tuning.

We have presented \CRANpkg{mlr3fairness}, a package for fairness auditing and bias correction on mlr3 models.
Integrated fairness metrics allow for auditing ML models with respect to a multiplicity of fairness criteria and models identified as biased can subsequently be retrained / debiased using implemented debiasing methods.
We hope that this package can constitute a first step towards more equitable outcomes in automated decision making.


# Appendix

## Tuning the ML Pipeline

We include the full code to construct the `AutoTuner` with additional details
and comments below.
We first load all required packages and use \pkg{mlr3}'s interaction with \CRANpkg{future}
to automatically distribute the tuning to all available cores in parallel by setting a `plan`.

```{r, eval = FALSE}
library(mlr3misc)
library(mlr3)
library(mlr3pipelines)
library(mlr3fairness)
library(mlr3tuning)

# Enable paralellization over all cores
# future::plan("multisession")
```

We then instantiate an ML pipeline using \pkg{mlr3pipelines}. This connects several
modeling steps, in our case **categorical encoding**, **reweighing** and a final **learner** using the
`%>>%` (double caret) operator, ultimately forming a new learner. This learner can
then subsequently be fit on a `Task`. We use the `po(<key>)` shorthand to construct a new
pipeline operator from a dictionary of implemented operators. We conduct **categorical encoding**
because \pkg{glmnet} can not naturally handle categorical variables and we therefore have
to encode them (in our case using `one-hot` encoding).

```{r, eval = FALSE}
# Define the learner pipeline.
lrn = as_learner(po("encode") %>>% po("reweighing_wts") %>>% po("learner", lrn("classif.glmnet")))
```

We furthermore have to specify the hyperparameter space our `Tuner` should tune over.
We do this by defining a list of values with a `to_tune()` token specifying the range.
Note, that hyperparameter names are prefixed with the respective operation's id.

```{r, eval = FALSE}
# Define the parameter space to optimize over
vals = list(
  reweighing_wts.alpha = to_tune(0.75, 1),
  classif.glmnet.alpha = to_tune(0.5, 1),
  classif.glmnet.s = to_tune(1e-4, 1e-2, logscale = TRUE)
)
# Add search space to the learner
lrn$param_set$values = insert_named(lrn$param_set$values, vals)
```

We can now instantiate a new `AutoTuner` using `lrn` defined above by
additionally providing arguments specifying the tuning strategy, in our
case random search, the measure to optimize for as well as the number of
tuning steps.

```{r, eval = FALSE}
at = AutoTuner$new(
  learner = lrn, # The learner
  resampling = rsmp("holdout"), # inner resampling strategy
  measure = metric, # the metric to optimize for
  tuner = mlr3tuning::tnr("random_search"), # tuning strategy
  terminator = trm("evals", n_evals = 3)) # number of tuning steps
```

The so-constructed `AutoTuner` can now be used on any classification Task!


