---
title: Fairness Audits And Debiasing Using mlr3fairness
author:
  - name: Florian Pfisterer
    affiliation: LMU Munich
    orcid: 0000-0001-8867-762X
    email:  florian.pfisterer@stat.uni-muenchen.de
  - name: Siyi Wei
    email: weisiyi2@gmail.com
    affiliation: University of Toronto
  - name: Sebastian Vollmer
    email: svollmer@stat.uni-muenchen.de
    affiliation: DFKI
    affiliation2: University of Kaiserslautern
    affiliation3: Univeristy of Warwick
  - name: Michel Lang
    email: michel.lang@stat.uni-muenchen.de
    orcid: 0000-0001-9754-0393
    affiliation: LMU Munich
    affiliation2: TU Dortmund University
  - name: Bernd Bischl
    email: bernd.bischl@stat.uni-muenchen.de
    orcid: 0000-0001-6002-6980
    affiliation: LMU Munich


abstract: >
  Given an increase in data-driven automated decision-making based on machine learning models, it is
  imperative that along with tools to develop and improve such models there are sufficient capabilities
  to analyze and assess models with respect to potential biases. We present the package \CRANpkg{mlr3fairness},
  a collection of metrics and methods that allow for the assessment of bias in machine learning models.
  Our package implements a variety of widely used fairness metrics that can be used to audit models for potential biases along with a set of visualizations that can help to provide additional insights into such biases. \CRANpkg{mlr3fairness} furthermore integrates debiasing methods that can help allevaite biases in ML models through data preprocessing or post-processing of predictions. These allow practicioners to trade off performance and fairness metric that are appropriate for their use case.
preamble: |
  \usepackage{longtable}
  \usepackage{bbm}

# per R journal requirement, the bib filename should be the same as the output
# tex file. Don't forget to rename the bib file and change this example value.
bibliography: mlr3fairness.bib

output: rticles::rjournal_article
---

```{r setup, include = FALSE}
options(tinytex.verbose = TRUE)
set.seed(4444L)
library("mlr3misc")
library("ggplot2")
library("mlr3verse")
library("bbotk")
library("mlr3tuning")
library("patchwork")

lgr::get_logger("bbotk")$set_threshold("warn")
lgr::get_logger("mlr3")$set_threshold("warn")
```


# Introduction

Humans are increasingly subject to data-driven automated decision-making.
Those automated procedures such as credit risk assessments are often applied using predictive models [@galindo2000credit].
It is imperative that along with tools to develop and improve such models, we also develop sufficient capabilities to analyze and assess models with respect to their robustness and predictive performance, but also address potential biases. This is highlighted by the GDPR requirement to process data fairly.
Popular R [@R] modeling frameworks such as \CRANpkg{caret} [@caret], \CRANpkg{tidymodels} [@tidymodels], \CRANpkg{SuperLearner} [@superlearner], or \CRANpkg{mlr} [@mlr] implement a plethora of metrics to measure performance, but fairness metrics are widely missing.
This lack of availability can be detrimental to obtaining fair and unbiased models if the result is to forgo bias audits due to the considerable complexity of implementing such metrics.
Consequently, there exists considerable necessity for R packages (a) implementing such metrics, and (b) connecting these metrics to existing ML frameworks.
If biases are detected and need to be mitigated, we furthermore require debiasing techniques that tightly integrate with the fitting and evaluation of the resulting models in order to obtain trade-offs between a model's fairness and utility (e.g., predictive accuracy).

In this article, we present the \CRANpkg{mlr3fairness} package which builds upon the ML framework \CRANpkg{mlr3} [@mlr3].
Our extension contains fairness metrics, fairness visualizations, and model-agnostic pre- and postprocessing operators that aim to reduce biases in ML models.
Additionally, \CRANpkg{mlr3fairness} comes with reporting functionality to allow for fairness audits and conveniently building unbiased models.

In the remainder of the article, we first provide an introduction to fairness in ML with the goal to raise awareness for biases that can arise due to the use of ML models.
Next, we introduce the \CRANpkg{mlr3fairness} package, followed by an extensive case study, showcasing the capabilities of \CRANpkg{mlr3fairness}.
We conclude with a short summary.


# Fairness in Machine Learning

Studies have found that data-driven automated decision-making systems often improve over human expertise (c.f. @dawes1989clinical) and high stakes decisions can therefore be improved using data-driven systems.
This often does not only improve predictions but can also make decisions more efficient through automation.
Such systems, often without human oversight, are now ubiquitous in everyday life (@o2016weapons, @eubanks2018automating, @noble2018algorithms).
To provide further examples, ML-driven systems are used for highly influential decisions such as loan accommodations (@Chen2018, @Turner2019), job applications [@schumann], healthcare [@Topol2019], and criminal sentencing (@compas, @corbettcompas, @richardcompas).
With this proliferation, such decisions have become subject to scrutiny as a result of prominent inadequacies or failures, for example in the case of the COMPAS recidivism prediction system [@compas].

Without proper auditing, those models can unintentionally result in negative consequences for individuals, often from underprivileged groups [@fairmlbook].
Several sources of such biases are worth mentioning in this context: 
Data often contains **historical biases** such as gender or racial stereotypes, that -- if picked up by the model -- will be replicated into the future.
Similarly, unprivileged populations are often not represented in data due to **sampling biases** leading to models that perform well in groups sufficiently represented in the data but worse on others [@gendershades] - this includes a higher rate of missing data.
Other biases include biases in the *label generating process* and **feedback** loops where repeated decisions affect the population subject to such decisions. 
For an in-depth discussion and further sources of biases, the interested reader is referred to [@mehrabi, @mitchell2021algorithmic].

## Quantifying Fairness

We now turn to the question of how we can detect whether disparities exist in a model and if so, how they can be quantified.
Note, that fairness definitions in this case serve a dual purpose [@wachter-vlr2020]:
First, as a *diagnostic tool* with the goal to detect disparities.
This e.g. allows assessing whether a model has inherited biases, e.g. from historical disparities reflected in the data.
The second purpose is as a basis for *model selection* and making fair decisions in practice.
In this setting, fairness notions can help to understand existing disparities or learn more about sources of biases.
Fairness metrics should however not be used as the sole basis for making decisions about whether to employ a given ML model or to assess whether a given system is fair.
We therefore explicitly encourage using the metrics found below in an explorative manner.
What constitutes a fair models depends on a society's ethical values and whether we take a normative position, resulting in different metrics that are applied to a problem at hand.
In this article, we focus on a subgroup of these, so-called *statistical group fairness* metrics.
First, the observations are grouped by a protected attribute $A$ ($A = 0$ vs.\ $A = 1$) which, e.g., is an identifier for a person's race or a person's gender.
For the sake of simplicity, we consider a *binary classification* scenario and a *binary protected attribute*.
Each observation has an associated label $Y, Y \in \{0, 1\}$ we aim to predict, e.g. whether a defendant was caught re-offending. 
A system then makes a prediction $\hat{Y}, \hat{Y} \in \{0,1\}$ with the the goal to predict whether an individual might re-offend. 
We assume that $Y = 1$ is the favored outcome in the following exposition.
However, the concepts discussed in the following often extend naturally to more complex scenarios including multi-class classification, regression or survival analysis and similarly to settings with multiple protected attributes.
We now provide and discuss two metrics from @hardt2016equality to provide further intuition about the purpose of such metrics.
We now provide and discuss several metrics grouped into metrics that require *Separation* and *Independence* [@fairmlbook] to provide further intuition regarding core concepts and possible applications.

### Separation

One group of widely used fairness notions requires **Separation**: $\hat{Y} \perp A | Y$. 
This essentially requires, that some notion of model error, e.g. accuracy or false positive rates is equal across groups $A$.
From this notion, we can derive several metrics that come with different implications.
It is important to note, that those metrics can only meaningfully identify biases under the assumption that no disparities exist in the data, or that they are legally justified. 
For this reason, @wachter-vlr2020 refer to those metrics as *bias-preserving* metrics since underlying disparities are not addressed.
We now provide and discuss several metrics to provide further intuition regarding core concepts and possible applications.

#### Equalized Odds

A predictor $\hat{Y}$ satisfies *equalized odds* with respect to a protected attribute $A$ and observed outcome $Y$, if $\hat{Y}$ and $A$ are conditionally independent given $Y$:
\begin{equation} \label{eq:eod}
\mathbb{P}\left(\hat{Y} = 1 \mid A = 0, Y = y\right) = \mathbb{P}\left(\hat{Y} = 1 \mid A = 1, Y = y\right), \quad y \in \{0,1\}.
\end{equation}
In short, we require the same true positive rates (TPR) and false positive rates (FPR) across both groups $A = 0$ and $A = 1$.
<<<<<<< HEAD
This intuitively requires, that independent of the protected attribute, qualified individuals have the same chance to be accepted and unqualified individuals are rejected, e.g. in the case of university admission.
Similar arguments have been made for equalized false positive rates [@chouldechova2017fair] and false omission rates [@richardcompas] depending on the exact scenario.

#### Equality of Opportunity

A predictor $\hat{Y}$ satisfies *equality of opportunity* with respect to a protected attribute $A$ and observed outcome $Y$, if $\hat{Y}$ and $A$ are conditionally independent given $Y = 1$.
This is a relaxation of the aforementioned *equalized odds* essentially only requiring equal TPRs:
\begin{equation} \label{eq:eop}
\mathbb{P}\left(\hat{Y} = 1 \mid A = 0, Y = 1\right) = \mathbb{P}\left(\hat{Y} = 1 \mid A = 1, Y = 1\right).
\end{equation}
Intuitively, this only requires that independent of the protected attribute, qualified individuals have the same chance of being accepted.

#### Performance Parity

A more general formulation can be applied when we require parity of some performance metric across groups. 
To provide an example, @gendershades compare accuracy across intersectional subgroups, essentially arguing that model performance should be equal across groups:

\begin{equation} \label{eq:accp}
\mathbb{P}\left(\hat{Y} = Y \mid A = 0\right) = \mathbb{P}\left(\hat{Y} = Y \mid A = 1\right) \quad Y \in \{0,1\}.
\end{equation}
This intuitively requires, that the model should work equally well for all groups, i.e. individuals are correctly accepted or denied at the same rate, independent of the predicted attribute.
This notion can be extended across supervised learning settings and performance metrics, leading to considerations of equal mean squared error, e.g. in a regression setting.


### Independence (Demographic Parity)

A second group of fairness metrics is given by so-called *bias-transforming* metrics [ @wachter-vlr2020]. 
They require, that prediction rates, such as the positive rate are equal across groups

A predictor $\hat{Y}$ satisfies *demographic parity* [@{Calders2010] with respect to a protected attribute $A$ and observed outcome $Y$, if $\hat{Y}$ and $A$ are conditionally independent.
\begin{equation} \label{eq:dp}
\mathbb{P}\left(\hat{Y} = 1 \mid A = 0\right) = \mathbb{P}\left(\hat{Y} = 1 \mid A = 1\right).
\end{equation}
In contrast to the previous definitions, this now requires that the chance of acceptance is equal across groups.
This notion can identify biases e.g. arising from societal biases that manifest in different base rates across groups.
At the same time, employing such notions poses a considerable risk, as blindly optimizing for demographic parity might result in predictors that e.g. jail innocent people from an advantaged group in order to achieve parity across both groups [@dwork2012, @richardcompas].

#### Fairness metrics

In order to encode the requirements in equations \ref{eq:eod}) - \ref{eq:dp} into a fairness metric, we often encode differences between measured quantities in two groups.
I.e., with $\mathbb{P}\left(\hat{Y} = 1 \mid A = \star, Y = 1\right)$ denoted as $\mathrm{TPR}_{A=\star}$, we calculate the difference in TPR between the two groups as
\[
\Delta_{\mathrm{TPR}} = \mathrm{TPR}_{A=0} - \mathrm{TPR}_{A=1}.
\]

To provide a binary conclusion, a model could be considered fair, if $|\Delta_{\mathrm{TPR}}| < \epsilon$ for a given threshold $\epsilon > 0$, e.g., $\epsilon = 0.05$ to allow for small deviations from perfect fairness due to variance in the estimation of $\mathrm{TPR}_{A=\star}$ or additional sources of bias. 
It is important to note, that such thresholds are arbitrary and do not translate to legal doctrines, such as e.g. disparate impact [@chen22].
A collection of this and similarly constructed metrics can be found in (@saleiro2018aequitas, @kim2020fact, @mehrabi).


#### Selecting fairness metrics

While the three metrics are conceptually similar, they encode a different belief of what constitutes *fair* in a given scenario.
@wachter-vlr2020 differentiate between *bias-preserving* and *bias transforming* metrics: 
Bias-preserving metrics such as equalized odds and equality of opportunity require that errors made by a model are equal across groups.
This can help to detect biases, stemming e.g from bias during data acquisition, but might be problematic in cases where e.g. labels are biased.
To provide an example, police enforcement and subsequent arrests of violent re-offenders might be different across ZIP code areas, a proxy for race.
This might lead to situations where labels $Y$ suffer from differential bias depending on the protected attribute [@bao2021s].
Bias-transforming methods, in contrast do not depend on the labels and might therefore not suffer from this problem.
They can help detecting biases arising from different base-rates across populations, arising e.g. from aforementioned biases in the labeling or as a consequence of structural discrimination.
Deciding which metrics to use constitutes a value judgement and requires careful assessment of the societal context a decision making system is deployed in. 
A discussion of different metrics and their applicability can be found in the Aequitas Fairness Toolkit [@saleiro2018aequitas] which also provides guidance towards selecting a metric via the [Aequitas Fairness Tree](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/).
@wachter-vlr2020 recommend using *bias-transforming* metrics and provide a checklist that can guide the choice of fairness metric.
@corbett2018measure on the other hand point out several limitations of available metrics and argue for grounding decisions in real world quantities in addition to abstract fairness metrics.
Similarly, @friedler16 emphasize the need to differentiate between constructs we aim to measure (e.g. job-related knowledge) and the observed quantity that can be measured in practice (e.g. years in a job) when trying to automate decision.
To provide an example, individuals with similar ability might exhibit different measured quantities (grades) due to structural bias, e.g. worse access to after-school tutoring programs.

#### The dangers of fairness metrics

We want to stress, that overly trusting in metrics can be dangerous and that fairness metrics can and should not be used to *prove* or *guarantee* fairness.
Whether a selected fairness notion is actually fair depends on the societal context in which a decision is made and which action should be derived from a given prediction.
Therefore, selecting the correct fairness metric requires a thorough understanding of the societal context decisions are made in as well as possible implications of such decisions.
Fairness metrics merely provide a reduction of the aforementioned fairness notions into mathematical objectives.
As such, they require a variety of abstraction steps that might invalidate the metric [@chen22], as they e.g. require that the data is a large enough and representative sample of the entire population that we aim to investigate.
Furthermore, practitioners need to look beyond the model, but also the data used to train it and the process of acquiring data and labels.
If the data e.g. exhibits disparate measurement errors in the features or labels, valid fairness assessments can become impossible.
Similarly, feedback loops might arise from a prediction leading to changes in the data collected in the future.
Even an *initially fair* model might then lead to adverse effects in the long term [@schwobel-facct22a].

### Fairness Constraints 

It is important, that in practice we might not be able to perfectly satisfy a selected metric, e.g. due to stochasticity in data and labels.
One approach to take this into account might therefore be to impose constraints, e.g. $|\Delta M| \leq \epsilon$ requiring that the absolute difference in a metric $M$ between groups 
is smaller than a chosen value $\epsilon$. 
The goal of finding a model now reduces to e.g. maximizing some notion of model performance subject to a constraint on unfairness. 
In the following, we denote the fairness metric with $\Delta M$ and the performance metric with $\rho$. 
\[
  \rho_{|\Delta| M \leq \epsilon} = \left\{
\begin{array}{ll}
\rho         & |\Delta M| \leq \epsilon      \\
- |\Delta M| &  \textrm{else}                \\
\end{array}
\right.
\]

This approach has e.g. been employed by [@perrone2021fair] as the goal of an AutoML system.

## Debiasing Models

If biases are detected in a model, we might now be interested in improving models in order to potentially mitigate such biases.
Bias in models might arise from a variety of sources, so a careful understanding of the data, data quality and distribution might lead to approaches that can help in decreasing biases, e.g. through the collection of better or additional data or a better balancing of protected groups. 
Similarly, biases might arise from the model, e.g. through under- or overfitting and more careful tuning of model hyperparameters might help with improving fairness. 
Especially in the case of *bias-transforming* metrics, a better solution might often be to address fairness problems in the real world instead of relying on algorithmic interventions to solve fairness not only momentarily.
In addition, a variety of algorithmic **debiasing techniques**, that might help with obtaining fairer models have been proposed. 
Their goal is to reduce measured gaps in fairness, either via data pre-processing, employing models that incorporate fairness or by applying post-processing techniques on a model's predictions.
Popular examples for such techniques include computing instance weights before training [@kamiran2012data], directly learning fair models that incorporate fairness constraints [@Zafar2017] or subsequent adaption of model predictions [@hardt2016equality].
While perfect fairness is often an elusive goal, a more practical goal is often to optimize performance under a constraint for unfairness or studying a variety of models that offer different trade-offs between fairness and performance.
Since debiasing techniques are often tailored towards a particular fairness metric, the optimal choice of debiasing technique is often not trivial and a combination of algorithms and debiasing techniques, e.g. determined via tuning might result in an optimal model.

Bias mitigation techniques, as proposed above have the goal to mitigate fairness issues, as e.g. measured by fairness metrics. 
In practice, this usually comes with several drawbacks:
First, bias mitigation strategies often lead to a decrease in a classifier's predictive performance [@corbettcompas]. 
In addition, processing schemes can worsen interpretability or introduce stochasticity during prediction (see e.g. [@hardt2016equality]).
Furthermore, we want to caution against favoring bias mitigation techniques over policy interventions that tackle biases at their root cause.
If biases are only addressed at a given moment and without regard for downstream effects, they might simultaneously lead to a decrease in predictive performance in the near term and to negative consequences for the protected group in the long term [@schwobel-facct22a].

# mlr3fairness {#main}

In this section, we first give an overview of related software.
Next, we give a very briefly introduce to the \CRANpkg{mlr3} ecosystem of packages.
Finally, the implemented extensions for fairness are presented.


## Related Software {#related}

Several R packages provide similar capabilities to our software, but mostly focus on fairness metrics and visualization.
The \CRANpkg{fairness} package [@fairness] allows for the calculation of a variety of fairness metrics, while \CRANpkg{aif360} [@aif360] wraps the Python \pkg{aif360} module allowing for the computation of fairness metrics and several debiasing techniques but has only limited interoperability with R objects such as \code{data.frame}s.
The \CRANpkg{fairmodels}[@fairmodels] package again allows for the computation of fairness metrics for classification and regression settings as well as several debiasing techniques.
It tightly integrates with \CRANpkg{DALEX} [@dalex] to gain further insight using interpretability techniques.

Outside R, in Python, the \pkg{fairlearn} module [@fairlearn] provides ample functionality to study a wide variety of metrics, debias with respect to a variety of pre-, in- and postprocessing methods as well as to visualize differences.
It furthermore provides a *fairlearn dashboard* providing a comprehensive fairness report.
The \pkg{aif360} [@aif360]  module similarly provides metrics as well as debiasing techniques while the \pkg{aequitas} fairness toolkit [@saleiro2018aequitas] provides similar capabilities.
Interoperability with the \pkg{scikit-learn} [@sklearn] ML framework allows for debiasing a wide variety of ML models in all aforementioned systems.
Similar capabilities are also available in Julia's \pkg{Fairness.jl} [@fairnessjl] library.


## The mlr3 Ecosystem {#mlr3}

\CRANpkg{mlr3fairness} is tightly integrated into the ecosystem of packages around the ML framework \CRANpkg{mlr3} [@mlr3].
\CRANpkg{mlr3} provides the infrastructure to fit, resample, and evaluate over 100 ML algorithms using a unified API.
Multiple extension packages bring numerous additional advantages and extra functionality.
In the context of fairness, the following extension packages deserve special mention:

* \CRANpkg{mlr3pipelines} [@mlr3pipelines] for pre- and postprocessing via pipelining.
  This allows merging debiasing with arbitrary ML algorithms shipped with \CRANpkg{mlr3} as well as comparison of different models through joint resampling and tuning.
  It furthermore integrates with \CRANpkg{mcboost} [@mcboost], which implements additional debiasing methods. 
  We present an example in the supplementary material.
* \CRANpkg{mlr3tuning} for its extensive tuning capabilities.
  Fusing debiasing techniques with ML algorithms as well as other often necessary
  preprocessing steps such as imputation of missing values or class balancing allows for joint tuning of hyperparameters with respect to
  arbitrary performance and fairness metrics.
* \CRANpkg{mlr3proba} [@mlr3proba] for survival analysis.
* \CRANpkg{mlr3benchmark} for post-hoc analysis of benchmarked approaches.
* \CRANpkg{mlr3oml} as a connector to OpenML [@Vanschoren2014], an online scientific platform for collaborative ML.

In order to provide the required understanding for \CRANpkg{mlr3}, we briefly introduce some terminology and syntax.
A full introduction can be found in the mlr3 book [@mlr3book].

A `Task` in \CRANpkg{mlr3} is a basic building block holding the data, storing covariates and the target variable along with some meta-information.
The shorthand constructor function `tsk()` can be used to quickly access example tasks shipped with \CRANpkg{mlr3} or \CRANpkg{mlr3fairness}.
In the following chunk, we retrieve the binary classification task with id `"compas_race_binary"` from the package.
It contains a simplified version of the COMPAS data set [@compas].
The task is to predict whether a parolee will re-offend within a span of 2 years.
The column `"race"` is set as a binary protected attribute with levels `"Caucasian"` and `"African American"`.

```{r}
library("mlr3verse")
library("mlr3fairness")

# get a simplified compas example data set
task = tsk("compas_race_binary")
print(task)
```

The protected attribute(s) are identified by a `col_role` named `pta` and can be set accordingly, e.g. via
`task$col_roles$pta = c("gender, "race")`. 
If more than one protected attribute is specified, metrics will be computed based on intersecting groups formed by the columns. 

The second building block is the `Learner`.
It is a wrapper around an ML algorithm, e.g., an implementation of logistic regression or a decision tree.
It can be trained on a `Task` and used for obtaining a `Prediction` on an independent test set which can subsequently be scored using a `Measure` to get an estimate for the predictive performance on new data.
The shorthand constructors `lrn()` and `msr()` allow for the instantiation of implemented `Learner`s and `Measure`s, respectively.
In the following example, we will instantiate a learner, train it on the train set of the dataset and evaluate predictions on held-out test data.
The train-test split in this case is given by row indices, here stored in the `idx` variable.

```{r}
# initialize a classification tree from package rpart, predicting probabilities
learner = lrn("classif.rpart", predict_type = "prob")
# split into a list with train and test set
idx = partition(task)
# fit model on train set
learner$train(task, idx$train)
# predict on observations of test set
prediction = learner$predict(task, idx$test)
```

We then employ the `classif.acc` measure which measures the accuracy of a prediction compared to the true label:

```{r}
measure = msr("classif.acc")
prediction$score(measure)
```

In the example above, we obtain an accuracy score of `r round(prediction$score(measure), 4)`, meaning our ML model correctly classifies roughly `r round(100 * prediction$score(measure))` \% of the samples in the test data. 
As the split into training set and test set is stochastic, the procedure should be repeated multiple times for smaller datasets [@bischl2012resampling] and the resulting performance values should be aggregated.
This process is called resampling, and can easily be performed with the `resample()` function, yielding a `ResampleResult` object.
In the following, we employ 10-fold cross-validation as a resampling strategy.

```{r}
resampling = rsmp("cv", folds = 10)
rr = resample(task, learner, resampling)
rr$aggregate(measure)
```

We can call the `aggregate` method on the `ResampleResult` to obtain the accuracy aggregated across all $10$ replications.
Here, we again obtain an accuracy of `r round(rr$aggregate(measure), 4)`, so slightly higher than previous scores, due to using a larger fraction of the data.
Furthermore, this estimate has a lower variance (as it is an aggregate) at the cost of additional computation time. 
To properly compare competing modeling approaches, candidates can be benchmarked against each other using the `benchmark()` function (yielding a `BenchmarkResult`).
In the following, we compare the decision tree from above to a logistic regression model.
To do this, we use the `benchmark_grid` function to compare the two `Learners` across the same `Task` and resampling procedure.

```{r, warning = FALSE}
learner2 = lrn("classif.log_reg", predict_type = "prob")

# build an exhaustive grid design and run benchmark
grid = benchmark_grid(task, list(learner, learner2), resampling)
bmr = benchmark(grid)
bmr$aggregate(measure)[, .(learner_id, classif.acc)]
```
After running the benchmark, we can again call `.$aggregate` to obtain aggregated scores. 
The \CRANpkg{mlr3viz} package comes with several ready-made visualizations for objects from `mlr3` via the `autoplot` function.
For a `BenchmarkResult`, the `autoplot` function provides a Box-plot comparison of performances across the cross-validation folds for each `Learner`.
Figure @ref(fig:bmrbox) contains the box-plot comparison.
We can see, that `log_reg` has a higher accuracy and lower inter-quartile range across the 10 folds and we might therefore want to prefer the `log_reg` model. 

```{r, warning = FALSE, include = FALSE}
library(mlr3viz)
autoplot(bmr, measure = measure)
```

```{r bmrbox, echo = FALSE, warning = FALSE, fig.height = 3.5, fig.width = 3.5, fig.cap = "Model comparison for decision trees (rpart) and logistic regression (log\\_reg).", fig.align = 'center'}
library(mlr3viz)
autoplot(bmr, measure = measure) + 
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank(),
    panel.border = element_blank(),
    axis.line = element_blank()
  ) +
  theme(axis.text.x = element_text(angle = 30, hjust = .7))
```

### Selecting the protected attribute

For a given task, we can select one or multiple protected attributes.
In \CRANpkg{mlr3}, the protected attribute is identified by the column role `pta` and can be set as follows:

```{r, eval = FALSE}
task$col_roles$pta = "race"
```

This information is then automatically passed on when the task is used, e.g. when computing fairness metrics.

### Quantifying Fairness

With the \CRANpkg{mlr3fairness} package loaded, fairness measures can be constructed via `msr()` like any other measure in \CRANpkg{mlr3}.
They are listed with prefix *fairness*, and simply calling `msr()` without any arguments will return a list of all available measures.
Table \ref{tab:metrics} provides a brief overview over some popular fairness measures which are readily available.
The full list can be obtained from `mlr_measures_fairness`.

Table: Selection of implemented fairness metrics.
\label{tab:metrics}

|key                   |description                                                                                  |
|:---------------------|:--------------------------------------------------------------------------------------------|
|fairness.acc          | Accuracy equality \citep{gendershades}                                                      |
|fairness.mse          | Mean squared error equality (regression)                                                    |
|fairness.fpr          | False positive rate equality / Pedictive equality \citep{chouldechova2017fair}              |
|fairness.tpr          | True positive rate equality / Equality of opportunity \citep{hardt2016equality}             |
|fairness.fnr          | False negative / False omission rate equality \citep{richardcompas}                         |
|fairness.eod          | Equalized odds \citep{hardt2016equality}                                                    |
|fairness.cv           | Demographic parity / Equalized positive rates \citep{Calders2010}                           |
|fairness.acc\_eod=.05 | Accuracy under equalized odds constraint \citep{perrone2021fair}                            |
|fairness.acc\_ppv=.05 | Accuracy under ppv constraint \citep{perrone2021fair}                                       |

Furthermore, new custom fairness measures can be easily implemented, either by implementing them directly or by composing them from existing metrics.
This process is extensively documented in an accompanying [vignette](https://mlr3fairness.mlr-org.com/articles/measures-vignette.html).

Here we choose the binary accuracy measure `"classif.acc"` and the equalized odds metric from above using `"fairness.eod"`:
The constructed list of measures can then be used to score a `Prediction`, a `ResampleResult` or `BenchmarkResult`, e.g.

```{r}
measures = list(msr("classif.acc"), msr("fairness.eod"))
rr$aggregate(measures)
```

```{r, echo = FALSE, results = 'hide'}
eodi = round(rr$aggregate(measures)[2], 2)
```
We can clearly see a comparatively large difference in equalized odds at around `r eodi`.
This means, that in total, the false positive rates (FPR) and true positive rates (TPR) on average differ by ~`r eodi`, indicating that our model might exhibit a bias.
Looking at the individual components, yields a clearer picture.
Here, we are looking at the confusion matrices of the combined predictions of the 10 folds, grouped by protected attribute:

```{r}
fairness_tensor(rr)
```

Plotting the prediction density or comparing measures graphically often provides additional insights:
We can e.g. see, that African-American defendants are more often assigned low probabilities of not re-offending (predicted class `0`).
Similarly, we can see that both equality in FPR and TPR differ considerably.

```{r, eval = FALSE}
fairness_prediction_density(prediction, task)
compare_metrics(prediction, msrs(c("fairness.fpr", "fairness.tpr", "fairness.eod")), task)
```

```{r, echo = FALSE, fig.height=3, fig.width=6, fig.align='center', fig.cap = "Left: Prediction densities for the negative class for races Caucasian and African-American. Right: Fairness metrics comparison for FPR, TPR, EOd fairness metrics."}
library(patchwork)
p1 = fairness_prediction_density(prediction, task) + theme(legend.position="bottom")
p2 = compare_metrics(prediction, msrs(c("fairness.fpr", "fairness.tpr", "fairness.eod")), task)
(p1 + xlab("") + p2) * theme_bw() + theme(axis.text.x = element_text(angle = 30, hjust = .7))
```


### Debiasing

As mentioned above, several ways to improve a model's fairness exist. 
While non-technical interventions, such as e.g. collecting more data should be prefered, 
\CRANpkg{mlr3fairnes} provides several debiasing techniques that can be used together with a `Learner` to obtain fairer models.
Table \ref{tab:debiasing} provides an overview over implemented debiasing techniques.
They are implemented as `PipeOps` from the \CRANpkg{mlr3pipelines} package and can be
combined with arbitrary learners to build a pipeline. 
An introduction to \CRANpkg{mlr3pipelines} is available in the corresponding [mlr3book chapter](https://mlr3book.mlr-org.com/pipelines.html).

```{r, eval = FALSE}
# Automatically reweigh data before training a learner:
po("reweighing_wts") %>>% po("learner", lrn("classif.glmnet"))

# Post-process predictions for equalized odds.
po("learner_cv", lrn("classif.glmnet")) %>>% po("EOd")
```

Table: Overview over available debiasing techniques.\label{tab:debiasing}

|Key                | Description                      | Type              | Reference          |
|-------------------|----------------------------------|-------------------|--------------------|
| EOd               |Equalized-Odds Debiasing          |Postprocessing     | \citet{hardt2016equality} |
| reweighing_os     |Reweighing (Oversampling)         |Preprocessing      | \citet{kamiran2012data}   |
| reweighing_wts    |Reweighing (Instance Weights)     |Preprocessing      | \citet{kamiran2012data}   |

It is simple for users or package developers to extend \CRANpkg{mlr3fairness} with additional
debiasing methods -- as an example, the \CRANpkg{mcboost} package adds further postprocessing methods
that can improve fairness.<br>
Along with pipeline operators, \CRANpkg{mlr3fairness} contains several algorithms that can directly incorporate
fairness constraints. They can similarly be constructed using `lrn()`.

Table: Overview over fair ML algorihtms.\label{tab:fairlearns}

| key               | package | reference              |
| :---------------- | :------ | :--------------------  |
| regr.fairfrrm     | fairml  | \citet{scutari}        |
| classif.fairfgrrm | fairml  | \citet{scutari}        |
| regr.fairzlm      | fairml  | \citet{Zafar2017}      |
| classif.fairzlrm  | fairml  | \citet{Zafar2017}      |
| regr.fairnclm     | fairml  | \citet{komiyama}       |

###  Reports

Because fairness aspects can not always be investigated based on the fairness definitions above (e.g., due to biased sampling or labelling procedures), it is important to document data collection and the resulting data as well as the models resulting from this data.
Informing auditors about those aspects of a deployed model can lead to better assessments of a model's fairness.
Questionnaires for ML models [@modelcards] and data sets [@datasheets] have been proposed in literature.
We further add an automated report template using R markdown [@rmarkdown] which includes many fairness metrics and visualizations to provide a good starting point in order to generate a full fairness report inspired by similar reports offered in the *Aequitas Toolkit* [@saleiro2018aequitas].

| Report             |  Description             | Reference                   |
|--------------------|--------------------------|-----------------------------|
| `report_modelcard` | Modelcard for ML models  | \citet{modelcards}          |
| `report_datasheet` | Datasheet for data sets  | \citet{datasheets}          |
| `report_fairness`  | Fairness Report          | --                          |


# Case Study

In order to demonstrate a full workflow, we conduct full bias assessment and debiasing on the popular adult data set [@uci].
The goal is to predict whether an individual's income is larger than \$$50.000$ with the protected attribute being *gender*.
The data set ships with \CRANpkg{mlr3fairness}, separated into a *train* and *test* task and can be instantiated using `tsk("adult_train")` and `tsk("adult_test")`,  respectively.
As a fairness metric, we consider *predictive parity* [@chouldechova2017fair] which calls for equality in true positive rates between groups.
We furthermore are interested in the model's utility, here measured with its classification accuracy.

```{r}
library("mlr3verse")
library("mlr3fairness")

task = tsk("adult_train")
print(task)

measures = msrs(c("fairness.tpr", "classif.acc"))
```


In order to get an initial perspective, we benchmark three models using 3-fold cross-validation each:

* a classification tree from the \CRANpkg{rpart} package,
* a penalized logistic regression from the \CRANpkg{glmnet} package and
* a penalized logistic regression from the \CRANpkg{glmnet} package, but with reweighing preprocessing.

The logistic regression in the latter two approaches do not support operating on factor features natively, therefore we pre-process the data with a feature encoder from \CRANpkg{mlr3pipelines}:

```{r, message = FALSE, warning=FALSE, error=FALSE}
set.seed(4321)
learners = list(
    lrn("classif.rpart"),
    po("encode") %>>% lrn("classif.glmnet"),
    po("encode") %>>% po("reweighing_wts") %>>% po("learner", lrn("classif.glmnet"))
)
grid = benchmark_grid(
  tasks = tsks("adult_train"),
  learners = learners,
  resamplings = rsmp("cv", folds = 3)
)
bmr1 = benchmark(grid)
bmr1$aggregate(measures)[, c(4, 7, 8)]
```

The preprocessing step of reweighing already improved the fairness while sacrificing only a tiny bit of performance.
To see if we can further improve, we use \CRANpkg{mlr3tuning} to jointly tune all hyperparameters of the *glmnet* model as well as our reweighing hyperparameter.
In order to do this, we use an `AutoTuner` from \CRANpkg{mlr3tuning}; a model that tunes its own hyperparameters during training.
The full code for setting up this model can be found in the appendix.
An `AutoTuner` requires a specific metric to tune for.
Here, we define a fairness-thresholded accuracy metric. We set $\epsilon = 0.01$ as a threshold.

\[
  if \; \Delta_{EOd} \leq \epsilon: accuracy \;\; else: \; 1 + \Delta_{EOd}
\]

```{r}
metric = msr("fairness.constraint",
    performance_measure = msr("classif.acc"),
    fairness_measure = msr("fairness.eod"),
    epsilon = 0.01
)
```

We then design the pipeline and the hyperparameters we want to  tune over. 
```{r, echo = FALSE, message = FALSE}
# Enable paralellization utilizing all cores
# future::plan("multisession")

lrn = as_learner(po("encode") %>>% po("reweighing_wts") %>>% po("learner", lrn("classif.glmnet")))

# Define the parameter space to optimize over
vals = list(
  reweighing_wts.alpha = to_tune(0.8, 1),
  classif.glmnet.alpha = to_tune(0.8, 1),
  classif.glmnet.s = to_tune(1e-4, 1e-2, logscale = TRUE)
)
# Add search space to the learner
lrn$param_set$values = insert_named(lrn$param_set$values, vals)
```

```{r}
at = AutoTuner$new(lrn, rsmp("holdout"),
    metric,
    tuner = mlr3tuning::tnr("random_search"),
    terminator = trm("evals", n_evals = 3)
)

grd = benchmark_grid(
  tasks = tsks("adult_train"),
  learners = list(at),
  resamplings = rsmp("cv", folds = 3)
)

bmr2 = benchmark(grd, store_models = TRUE)
bmr2$aggregate(measures)[, c(4, 7, 8)]
```

The result improves w.r.t.\ accuracy while only slightly decreasing the measured fairness.
Note, that the generalization error is estimated using a holdout strategy during training
and slight violations of the desired threshold $\epsilon$ can therefore happen.
The results of both benchmark experiments can then be collected and jointly visualized in Figure @ref(fig:fat).
In addition to aggregate scores (denoted by a cross) individual iterations of the 3 fold Cross-Validation (denoted by points) are shown 
to visualize variations in the individual results.


```{r, results = 'hide', include = FALSE, fig.width = 6, fig.height = 4, fig.align='center', fig.cap = "Fairness-Accuracy tradeoff for 3-fold CV on the adult train set."}
bmr = c(bmr1, bmr2)
fairness_accuracy_tradeoff(bmr, fairness_measure = measures[[1]])
```

```{r fat, echo = FALSE, fig.width = 6, fig.height = 4, fig.align='center', fig.cap = "Fairness-Accuracy tradeoff for 3-fold CV on the adult train set.", dpi=300}
bmr = c(bmr1, bmr2)
fairness_accuracy_tradeoff(bmr, fairness_measure = measures[[1]]) +
  guides(shape = "none") + 
  theme_minimal()
```

and print the aggregated scores:

```{r}
bmr$aggregate(measures)[, c(4, 7, 8)]
```

Especially when considering optimizing accuracy while still retaining a fair model,
tuning can be helpful and further improve upon available trade-offs.
In this example, the `AutoTuner` improves w.r.t. the fairness metric while offering accuracy comparable with the simple `glmnet` model.
We therefore assume that the model obtained from the `AutoTuner` is the model we might want to use going forward.
Having decided for a final model, we can now train the final model:

```{r}
at_lrn = bmr$learners$learner[[4]]
at_lrn$train(tsk("adult_train"))
```

and predict on the held out test set available for the `Adult` dataset to obtain a final estimate.

```{r, warning = FALSE}
test = tsk("adult_test")
at_lrn$predict(test)$score(measures, test)
```

On the held-out test set, the fairness constraint is slightly violated which can happen due to the comparatively large variance in the estimation of fairness metrics.

# Summary

The large-scale availability and use of automated decision making systems have resulted in growing concerns for a lack of fairness in the decisions made by such systems. As a result, fairness auditing methods that allow for investigating (un-)fairness in such systems are required.
Implementations of such methods are still not widely available, especially considering the required interoperability with machine learning toolkits that allows for ease of use and integration into model evaluation and tuning.

We have presented \CRANpkg{mlr3fairness}, a package for fairness auditing and bias correction on \CRANpkg{mlr3} models.
Integrated fairness metrics allow for auditing ML models with respect to a multiplicity of fairness criteria and models identified as biased can subsequently be retrained / debiased using implemented debiasing methods.
We hope that this package can constitute a first step to support users in detecting fairness problems in machine learning models as well as with locating the root problems for such biases.

\pagebreak 

# Appendix

## Tuning the ML Pipeline

We include the full code to construct the `AutoTuner` with additional details
and comments below.
We first load all required packages and use \pkg{mlr3}'s interaction with \CRANpkg{future}
to automatically distribute the tuning to all available cores in parallel by setting a `plan`.
See the documentation of \CRANpkg{future} for platform-specific hints regarding parallelization.

```{r, eval = FALSE}
library(mlr3misc)
library(mlr3)
library(mlr3pipelines)
library(mlr3fairness)
library(mlr3tuning)

# Enable paralellization utilizing all cores
# future::plan("multicore")
```

```{r, eval=FALSE, echo=FALSE}
set.seed(1234L)
```

We then instantiate an ML pipeline using \pkg{mlr3pipelines}. This connects several
modeling steps, in our case **categorical encoding**, **reweighing** and a final **learner** using the
`%>>%` (double caret) operator, ultimately forming a new learner. This learner can
then subsequently be fit on a `Task`. We use the `po(<key>)` shorthand to construct a new
pipeline operator from a dictionary of implemented operators. We conduct **categorical encoding**
because \pkg{glmnet} can not naturally handle categorical variables and we therefore have
to encode them (in our case using `one-hot` encoding).

```{r, eval = FALSE}
# Define the learner pipeline.
lrn = as_learner(po("encode") %>>% po("reweighing_wts") %>>% po("learner", lrn("classif.glmnet")))
```

We furthermore have to specify the hyperparameter space our `Tuner` should tune over.
We do this by defining a list of values with a `to_tune()` token specifying the range.
Note, that hyperparameter names are prefixed with the respective operation's id.

```{r, eval = FALSE}
# Define the parameter space to optimize over
vals = list(
  reweighing_wts.alpha = to_tune(0.75, 1),
  classif.glmnet.alpha = to_tune(0.5, 1),
  classif.glmnet.s = to_tune(1e-4, 1e-2, logscale = TRUE)
)
# Add search space to the learner
lrn$param_set$values = insert_named(lrn$param_set$values, vals)
```

Before we now train the model, we again specify a metric we aim to satisfy, 
here we would like the equalized odds difference to be smaller than $0.1$.
In this case, we set a constraint on the *equalized odds difference* comprised of
the differences in true positive rate (TPR) and false positive rate (FPR):

$$
\Delta_EOd = |TPR_{sex = M} - TPR_{sex = F}| + |FPR_{sex = M} - FPR_{sex = F}|
$$

This can be done using the `fairness.constraint` measure. 

```{r, eval = FALSE}
metric = msr("fairness.constraint",
    performance_measure = msr("classif.acc"),
    fairness_measure = msr("fairness.eod"),
    epsilon = 0.1
)
``` 

We can now instantiate a new `AutoTuner` using `lrn` defined above by
additionally providing arguments specifying the tuning strategy, in our
case random search, the measure to optimize for as well as the number of
tuning steps.



```{r, eval = FALSE}
metric = msr("fairness.constraint",
    performance_measure = msr("classif.acc"),
    fairness_measure = msr("fairness.eod"),
    epsilon = 0.1
)
at = AutoTuner$new(
  learner = lrn, # The learner
  resampling = rsmp("holdout"), # inner resampling strategy
  measure = metric, # the metric to optimize for
  tuner = mlr3tuning::tnr("random_search"), # tuning strategy
  terminator = trm("evals", n_evals = 30)) # number of tuning steps
```

The so-constructed `AutoTuner` can now be used on any classification Task!
Additional information regarding the `AutoTuner` is again available in the corresponding [mlr3book chapter](https://mlr3book.mlr-org.com/optimization.html#autotuner).
In the following example, we will apply it to the `Adult` task and train our model.
This will perform a tuning loop for the specified number of evaluations and 
automatically retrain the best found parameters on the full data. 

```{r, eval = FALSE}
at$train(tsk("adult_train"))
```

After training, we can look at the best models found, here ordered by our metric. 
Note, that our metric reports the negative constraint violation if the constraint is violated and the accuracy in case the constraint is satisfied.

```{r, eval=FALSE}
head(at$archive$data[order(fairness.acc_equalized_odds_cstrt), 1:4])
```

We can then use the tuned model to assess our metric on the held out data:

```{r, eval=FALSE}
prd = at$predict(tsk("adult_test"))
prd$score(c(metric, msr("classif.acc"), msr("fairness.eod")),  tsk("adult_test"))
```

So our tuned model manages to obtain an accuracy of `~0.84` while satisfying the specified constraint of $\Delta_{EOd} < 0.1$. 
So to summarize, we have tuned a model with the goal to optimize accuracy with respect to a constraint on a selected fairness metric using an `AutoTuner`.
